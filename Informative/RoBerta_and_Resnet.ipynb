{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBerta and Resnet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "d96c3ff8-da7d-4cc2-e283-56389cfbf5db"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzejvNI6kk3A"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "e8035f0a-2116-40b1-92aa-dc6b8ccf3cc1"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"informative\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/ColabNotebooks/train.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'tweet_text', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['tweet_text'] = df_train['tweet_text'].apply(clean_text)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/ColabNotebooks/val.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'tweet_text', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['tweet_text'] = df_val['tweet_text'].apply(clean_text)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/ColabNotebooks/test.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'tweet_text', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(clean_text)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, targets, paths, tokenizer, max_len):\n",
        "    self.tweets = tweets\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.paths = paths\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    tweet = str(self.tweets[item])\n",
        "    target = self.targets[item]\n",
        "    path = str(self.paths[item])\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "      truncation = True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'tweet_text': tweet,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long),\n",
        "      'tweet_image': img\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    tweets=df.tweet_text.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "    paths=df.image.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "    for param in self.roberta.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.resnet = torchvision.models.resnet18(pretrained=True)\n",
        "    for param in self.resnet.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(self.roberta.config.hidden_size + 1000)\n",
        "\n",
        "    self.linear1 = nn.Linear(self.roberta.config.hidden_size + 1000, 1000)\n",
        "    self.relu1    = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(p=0.4)\n",
        "\n",
        "    self.linear2 = nn.Linear(1000, 500)\n",
        "    self.relu2    = nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "    self.linear3 = nn.Linear(500, 250)\n",
        "    self.relu3    = nn.ReLU()\n",
        "    self.dropout3 = nn.Dropout(p=0.1)\n",
        "\n",
        "    self.linear4 = nn.Linear(250, 125)\n",
        "    self.relu4    = nn.ReLU()\n",
        "    self.dropout4 = nn.Dropout(p=0.02)\n",
        "\n",
        "    self.linear5 = nn.Linear(125, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask, tweet_img):\n",
        "    output = self.roberta(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False\n",
        "    )\n",
        "    output = output[0]\n",
        "    text_output = torch.mean(output, 1)\n",
        "\n",
        "    image_output = self.resnet(tweet_img)\n",
        "    merged_output = torch.cat((text_output, image_output), dim=1)\n",
        "    bn_output = self.bn(merged_output)\n",
        "\n",
        "    linear1_output = self.linear1(bn_output)\n",
        "    relu1_output = self.relu1(linear1_output)\n",
        "    dropout1_output = self.dropout1(relu1_output)\n",
        "\n",
        "    linear2_output = self.linear2(dropout1_output)\n",
        "    relu2_output = self.relu2(linear2_output)\n",
        "    dropout2_output = self.dropout2(relu2_output)\n",
        "\n",
        "    linear3_output = self.linear3(dropout2_output)\n",
        "    relu3_output = self.relu3(linear3_output)\n",
        "    dropout3_output = self.dropout3(relu3_output)\n",
        "\n",
        "    linear4_output = self.linear4(dropout3_output)\n",
        "    relu4_output = self.relu4(linear4_output)\n",
        "    dropout4_output = self.dropout4(relu4_output)\n",
        "\n",
        "    linear5_output = self.linear5(dropout4_output)\n",
        "\n",
        "\n",
        "    probas = self.sigmoid(linear5_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "MAX_LEN = 150\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.BCELoss().to(device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "ed8ee974-04ee-49da-af97-fb71da631efe"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/Models/RobertaResnet/checkpoint.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "# best_accuracy = checkpoint['best_accuracy']\n",
        "\n",
        "# print(start_epoch)\n",
        "# print(best_accuracy)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {start_epoch + epoch + 1}/{start_epoch + EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/Models/RobertaResnet/checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6353140316511455 accuracy 0.6450369753150713\n",
            "Val   loss 0.5338080823421478 accuracy 0.7666878575969485\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.43062655863009003 accuracy 0.8242891365482762\n",
            "Val   loss 0.40450168401002884 accuracy 0.8264462809917356\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.32168271510224594 accuracy 0.8655348401208207\n",
            "Val   loss 0.34296662360429764 accuracy 0.8531468531468531\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2937098082743193 accuracy 0.8794917196125404\n",
            "Val   loss 0.332816444337368 accuracy 0.8658614113159567\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.276086153952699 accuracy 0.8856369128215811\n",
            "Val   loss 0.32787927240133286 accuracy 0.8690400508582327\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2605864852666855 accuracy 0.892927820018748\n",
            "Val   loss 0.3270678445696831 accuracy 0.8696757787666879\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.25211467476267563 accuracy 0.8994896364961983\n",
            "Val   loss 0.3239610269665718 accuracy 0.8696757787666879\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24010311537667325 accuracy 0.9057389855223414\n",
            "Val   loss 0.3232276290655136 accuracy 0.866497139224412\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23243775101084457 accuracy 0.90480158316842\n",
            "Val   loss 0.32031404972076416 accuracy 0.8734901462174189\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2266697750279778 accuracy 0.9082387251327987\n",
            "Val   loss 0.3198064789175987 accuracy 0.8722186904005086\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.22044223390127482 accuracy 0.9094885949380273\n",
            "Val   loss 0.3161826357245445 accuracy 0.8773045136681501\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20729040864266848 accuracy 0.9171961254036037\n",
            "Val   loss 0.3164157047867775 accuracy 0.8785759694850604\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20360263319391952 accuracy 0.9190709301114467\n",
            "Val   loss 0.31610287725925446 accuracy 0.8766687857596949\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19803974895100845 accuracy 0.9213623580876992\n",
            "Val   loss 0.31805650889873505 accuracy 0.8766687857596949\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.18737279349251798 accuracy 0.9283407978335589\n",
            "Val   loss 0.32324177771806717 accuracy 0.8760330578512396\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.18241065740585327 accuracy 0.9294865118216852\n",
            "Val   loss 0.32715389132499695 accuracy 0.8773045136681501\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.18079298107247604 accuracy 0.9296948234558899\n",
            "Val   loss 0.3308350369334221 accuracy 0.8753973299427845\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1710340341455058 accuracy 0.9324028747005519\n",
            "Val   loss 0.32503893226385117 accuracy 0.8792116973935156\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.17097504829105578 accuracy 0.9336527445057806\n",
            "Val   loss 0.328211672604084 accuracy 0.8817546090273363\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.16857987250152387 accuracy 0.935110925945214\n",
            "Val   loss 0.3308943435549736 accuracy 0.8836617927527018\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.16292155651669754 accuracy 0.9381314446411831\n",
            "Val   loss 0.3331202119588852 accuracy 0.8811188811188811\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.15636886577857168 accuracy 0.9417768982397666\n",
            "Val   loss 0.3325972929596901 accuracy 0.8798474253019708\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.16005061253121025 accuracy 0.9379231330069784\n",
            "Val   loss 0.33744387328624725 accuracy 0.8804831532104259\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.15231133605304517 accuracy 0.9425059889594833\n",
            "Val   loss 0.33648625016212463 accuracy 0.8811188811188811\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14943859843831314 accuracy 0.9450057285699406\n",
            "Val   loss 0.34533190727233887 accuracy 0.8779402415766052\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1477040530819642 accuracy 0.9444849494844286\n",
            "Val   loss 0.341494545340538 accuracy 0.8779402415766052\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14508583279032455 accuracy 0.9443807936673263\n",
            "Val   loss 0.34359245747327805 accuracy 0.8773045136681501\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14334977653465772 accuracy 0.9467763774606811\n",
            "Val   loss 0.34412481635808945 accuracy 0.8785759694850604\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14520563028360667 accuracy 0.9454223518383501\n",
            "Val   loss 0.3492042198777199 accuracy 0.8785759694850604\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14245768675678655 accuracy 0.9458389751067596\n",
            "Val   loss 0.34950166940689087 accuracy 0.8773045136681501\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13419236164343984 accuracy 0.9500052077908551\n",
            "Val   loss 0.3500268906354904 accuracy 0.8760330578512396\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13798390093602633 accuracy 0.9474013123632955\n",
            "Val   loss 0.35548894107341766 accuracy 0.8766687857596949\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13245150486105367 accuracy 0.9528174148526195\n",
            "Val   loss 0.3523405119776726 accuracy 0.8779402415766052\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12788357428814234 accuracy 0.9535465055723362\n",
            "Val   loss 0.3555808663368225 accuracy 0.8766687857596949\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.127068498024815 accuracy 0.951880012498698\n",
            "Val   loss 0.35687315464019775 accuracy 0.8773045136681501\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12727702644310498 accuracy 0.9534423497552338\n",
            "Val   loss 0.3574373126029968 accuracy 0.8741258741258742\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1303869756428819 accuracy 0.9504218310592646\n",
            "Val   loss 0.35729093849658966 accuracy 0.8760330578512396\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1251710112157621 accuracy 0.956462868451203\n",
            "Val   loss 0.3586648851633072 accuracy 0.8779402415766052\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12181850365902248 accuracy 0.9554213102801791\n",
            "Val   loss 0.3580900654196739 accuracy 0.8773045136681501\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12508444723330045 accuracy 0.9543797521091553\n",
            "Val   loss 0.36015498638153076 accuracy 0.8779402415766052\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12358816909162622 accuracy 0.9543797521091553\n",
            "Val   loss 0.3609066978096962 accuracy 0.8766687857596949\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12286767089053204 accuracy 0.9539631288407457\n",
            "Val   loss 0.36090368777513504 accuracy 0.8760330578512396\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12233573825735795 accuracy 0.9534423497552338\n",
            "Val   loss 0.3621545433998108 accuracy 0.8760330578512396\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11844136761991601 accuracy 0.9543797521091553\n",
            "Val   loss 0.3626067340373993 accuracy 0.8753973299427845\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12335592665170368 accuracy 0.9542755962920528\n",
            "Val   loss 0.36238018423318863 accuracy 0.8753973299427845\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11989251641850722 accuracy 0.9562545568169982\n",
            "Val   loss 0.3620775565505028 accuracy 0.8760330578512396\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11813460290431976 accuracy 0.9582335173419435\n",
            "Val   loss 0.3623308390378952 accuracy 0.8753973299427845\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12027037888765335 accuracy 0.9555254660972815\n",
            "Val   loss 0.3630760461091995 accuracy 0.8747616020343293\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12270111237701617 accuracy 0.95458806374336\n",
            "Val   loss 0.3625488132238388 accuracy 0.8747616020343293\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11640871865184683 accuracy 0.957816894073534\n",
            "Val   loss 0.3635126128792763 accuracy 0.8747616020343293\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFmTSaxYMw-"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/RobertaResnet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScOj15BovCww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "174c479a-6d88-4ac1-c2bb-381c571d1101"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxcZZn3/89VW6/pJZ0OJJ2VRcOahKwIIsrDYwAJKmJAGAZeKCM/2R7RGcafCyr8fj7oMEw0LuCgMINABh8QfFBGMuGJKGASlhDClkBiVtLpdCe9d1fV9fxxTneqO91JJenqTrq+79erXnW2Oue6q6vPdZ/7nHMfc3dERCR/RYY6ABERGVpKBCIieU6JQEQkzykRiIjkOSUCEZE8p0QgIpLnlAhkWDOz35nZ3w70sgcYw9lmtmkf839qZt8Y6O2KZMt0H4EcbsysKWO0GGgHUuH437n7g4Mf1cEzs7OBf3f3cYe4nvXA5939mYGIS6RLbKgDEOnN3Uu7hve18zOzmLsnBzO2I5W+K9kXNQ3JEaOricXM/sHMtgG/MLNKM/utmdWaWX04PC7jM8+a2efD4avM7Dkz+0G47Htmdt5BLjvZzJaZWaOZPWNmi8zs3/cT/y1mtt3MtprZ1RnTf2lmt4fDo8IyNJjZTjP7o5lFzOzfgAnAk2bWZGZ/Hy4/38xeD5d/1sxOyFjv+vC7WgU0m9lXzezXvWJaaGb/cjB/Dxk+lAjkSHM0MBKYCFxL8Bv+RTg+AWgFfrSPz88B3gJGAXcC/2pmdhDL/gr4C1AF3Ab8TRZxlwM1wDXAIjOr7GO5W4BNQDVwFPA1wN39b4C/Ahe6e6m732lmHwAeAm4Ol3+KIFEkMtZ3GXABUAH8OzDPzCogOEoALgUe2E/sMswpEciRJg18y93b3b3V3evc/dfu3uLujcAdwEf28fkN7n6vu6eA+4ExBDvcrJc1swnALOCb7t7h7s8BT+wn7k7gO+7e6e5PAU3AB/tZbgwwMVz2j97/ibwFwP929z+4eyfwA6AI+FDGMgvdfWP4XW0FlgGXhPPmATvcfeV+YpdhTolAjjS17t7WNWJmxWb2MzPbYGa7CXZ0FWYW7efz27oG3L0lHCw9wGXHAjszpgFs3E/cdb3a6Fv62e73gbXAf5rZu2Z26z7WORbYkBFjOoyjZh9x3Q9cEQ5fAfzbfuKWPKBEIEea3rXjWwhq1nPcvQw4K5zeX3PPQNgKjDSz4oxp4wdixe7e6O63uPsxwHzgy2Z2TtfsXotvIWgSAyBsthoPbM5cZa/PPA6camYnA58AjqgrsCQ3lAjkSDeC4LxAg5mNBL6V6w26+wZgBXCbmSXM7HTgwoFYt5l9wsyOC3fquwgum02Hs98HjslYfDFwgZmdY2ZxgqTYDvx5H7G3AY8SnuNw978ORNxyZFMikCPd3QTt4juAF4DfD9J2LwdOB+qA24FHCHbCh+p44BmCcwjPAz9296XhvP8f+Hp4hdBX3P0tguadHxKU/0KCk8kd+9nG/cApqFlIQrqhTGQAmNkjwJvunvMjkkMVnux+Ezja3XcPdTwy9HREIHIQzGyWmR0bXuM/D7iIoP39sGZmEeDLwMNKAtIlZ4nAzO4Lb55Z3c98C29mWWtmq8zstFzFIpIDRwPPEjThLASuc/eXhzSi/TCzEmA3cC6DcC5Fjhw5axoys7MI/kkecPeT+5h/PnADcD7BjTv/4u5zchKMiIj0K2dHBO6+DNi5j0UuIkgS7u4vEFz7PSZX8YiISN+GstO5Gnre7LIpnLa194Jmdi1BdwKUlJTMmDJlyqAEKCIyXKxcuXKHu1f3Ne+I6H3U3e8B7gGYOXOmr1ixYogjEhE5spjZhv7mDeVVQ5vpeTfmOHreESkiIoNgKBPBE8CV4dVDc4FdYadYIiIyiHLWNGRmDwFnA6MseEzft4A4gLv/lKDL3PMJOthqAa7ue00iIpJLOUsE7n7ZfuY78KVcbV9ERLKjO4tFRPKcEoGISJ47Ii4fFRE5EqTTTmNbkobWDhpaOtnV2okZVI8oYPSIQiqK4kQiB/aojHTa2dHcztaGNsZUFDJ6ROGAx61EIJLnmtuTbG9spzOVpiOZpqPrPZnGgdKCGGWFMUYUxhlRGKM4EcXMcHdaO1M0tSVpbE/S3J6kqT1JeVGco8sKGVmSoPfjoFNp553tjby6sYFXNu7i1Y0NbG9soyAWpTAeoTAepSgepTC+Z3zPtD3j/T1lOmJGLGLBe9SIRoLxaCRCLBJMi2UMmxltnSnak+nu9/bOFB2p4BEQhnVvq2uTXTv6Xa1JGlo62NUa7PAbWjrZ3dbJvnrtiUWMUaUFVI8oYGRJgsJ4pLvsXe8RM97f3caWXW1s3dXKtl1tdKaCld7+yZO5Yu7E/jdwkJQIRIaB+uYO3qtrZv2OZjbubKWkIMqo0oLgNSJBdWkBlcUJdjS38/qW3azZsps1W4P39XXN+9x59RYxKE7EaOlIkt7H5xLRCKPLCji6rJCjygupbWxn9eZdtHSkACgrjDF1fAXTJlTQ3pmmLZmirSNFWzJFa2eKnc0dGdPStIbzhrrn/IhBeVGciuIE5UVxKosTTKoqobI4TnlRnPLiBBVFcSrC8VTaqW1qp7YxeG0P3xtaOmjrTNOe7JmIkmln9IgCxpYXcdqESsaUFzG2opAx5UWcUlOekzIpEYhkqTOVpr65g9qmduqaOtjR1M6u1s6w1hkJa5tGLBohHumqSQbvBt2148a2TnY2d1DX3EFdU3v3cDLlFCeilBQEte7SghjFiRiJWIS0O8mUk0oHO4q0Oy0dKTbUtbC+rpmGls79xh8xeuy4x48s4qQx5Xxqeg3jRxaRiEZJxCLEo0YiFiERjWAW1ID3vDppbEvS0pEKYiyMUVoQY0T4XpSIsru1k2272ti6u433d7WxbXcba7bspqwozmdnjmfa+Aqmjq9gUlXxXkcM++PudKTS/SYDd0im06TSHnxPaaez6z0VfHfJlJNMp8Natoc18SgFsQgF4VFHIhrpXh+A493DRfHoATfvHO6UCGTYcnd2tXaycWcrG+tb2FTfwsadrWxpaKWsKM6EkcVMGFnMxKpiJlQVU11aQCrtbNjZwtrtTT1em+pbqM9iZ3sg4lFjZEmCqpICqkoTxCJGc0eK7Y1ttLSnaO5I0tIe1BK7mziiRtSCJo9ELMKEkcVccMoYJo8qYVJVCZOrSxhfWUxrZ4odTe3saGynNnzf0dRBZUmCk8aWccKYMsqL4gNansFgZhTEovtZan/zpTclAjliuDttnWmaMtqjm9uT7Grt5P3dQc1z2672jOE2mtqTPdZRVhhjbEURb25r5PFXNveoWRbFoxk1xcCY8kKOG13K1PFj9jS1lCaoKi2gqiRoGkh70PbdmeqqiQbrcO9Zk+waH1EYZ2RJgrLC2AHXiLOViEUoL4pzbHVpTtYvw4sSgeSEu9OZ8qBNNw1p9/AVzEu5s7O5Y69209qmdna3dtLakaKlI0VLR9AM0dqRoqUzRWofjdLRiDF6RAFHlRVyXHUpZx43inGVRYyrLGb8yOA9sxbcnkyxub6VDTtb2LizhQ11LSRiEY6rLuW40aUcO7qU0gL9i8jwp1+5HBR3Z11tEy++t5MX393J2+83BjvszuDk3v522n0pLYhRPaKAsqI4JYkoFcVxihIxShJRihJRShIxSgpilBYE7ejBcNA+fXRZIVWlBUQPoO22IBblmOpSjlGtWfKcEoHsUzKVZmdLB/XNndQ1t/PWtkb+8t5O/vLeTuqaOwAYPaKAU8eVd58sLIrHKEpEui8DjJgRMYhErPukacSMyuI4o8sKqC4tZNSIBMUJ/RxFhoL+8/LUloZW/vLezu4rX7peu8P3+pbgypZdrXufIK2pKOIjH6xm7uQqZk8eycSDuPpDRA4fSgR5orGtkxfe3clz79Tyx7U7eLe2uXueGZQVhtdAF8UpKwpOqI4sSYRXtSSoDIcnVpVQU1E0hCURkYGmRHAES6bSbNvdxub6VrbsaqW2sb37xGpzxknW7Y3tvLqxgWTaKYxHmDO5is/NnsCHjh3FuJFFlCZiw+66aBHJnhLBYaIzleatbY28/Nd6Xv5rA69uaqCtM73XLegF8QjN7Um2NASXSPZ1QrYwHqE4EaMoHqWkIEp5UZxrzzqGM48fxYyJlVlchy0i+USJYIjsaGpn5YZ6XtoQ7PhXbQ52/BB0UDVtfAUjCmPdfZ8E72l2NndQGI8yZ/JIaiqLGFtRRE1FETWVRYweUUBxInZAV86IiCgRDAJ3Z+32JlZsqGfF+npWbtjJ+roWIOiP5eSaMi6fM5HpEyqYNr6CmooinXwVkUGjRJAD7s67O5p5fl0dz6+r44V367ovtRxZkmDGxEoumz2BmZMqObmmXE01IjKklAgGQDKV5s1tjbyysYHl63fy/Lo6tje2A0EXBR/5QDVzj6li5qRKJo8qUW1/oLhDZwsk26GwAiJ6zpLIwVAiOAh1Te28+N5OXtnYwMt/ree1zbu62/dHlRZw+rFVnH5MFacfW3VQPSzmLXdoqYPdW6C5Nng1bYfm7dBUG8xra4DWhj3v6fA+B4tA8SgoHQ0l1cF78SgoGAGJYogXQ6I0HC4BHFIdkOoMXx3BurqGu6anw/FIHMrGQvm44L2sBgrL6bdjfJEjiBLBAdjS0MpP/886Hl6+kY5kmkQswkljy7hs9gSmja9g+vhKxo88DNr3O1qg9g14/3V4fw3UrYX2Ruhsho7mYH5n+EqUBLXpooqe77GCvtdtEYjGIZoIdo5dw2bBejuagvV2hNtKtgc7497rLywLdvL163u+Opr23ma0INyxVwWfL6vpub5YITTvCBJG845gvTvXQXNdUOZDFU1AOgme7jk9XhLEFemnaS8S6+O7igcJpuv76Qy/s46WPUmtN4vuSWCJ4uBvFi8J3hNhgouH0xMlEC8KYs1Mcl3JLdma8RvI+D3gvdZRHKw7WhAmw+Se9XR9F6Wjg79FWc2eJDni6GDZ1vpeSbv3eAO07Qrek23BdxTN+D1F40G5032UIZ2EWFHGd5GR5GOF4feeyFhnov+/Ebb3drv+VvT1f+wZFYRelYbu/43e60r0nNYdU6yfbexDaTUUVR7YZ7KgRJCFjTtb+PGza3l05Sbc4TMzxnHp7AmcOKaMRGw/zRHp1J5/nr5YJPgh7yt5tDbAX1+ADc/Bhj8HNePMHUHXcLI12PHvfBcILyuNF8Oo44Paa1Flz8/EC4OdQOY/6PY3eta09ypPek8tua8yxQp7/mNGE7Dj7WDdbbv23pnGCqFyUvCadCZUTITyGigZvad2XzDi4Gve6fSexJS548v2nzYSC7adSkLTtuBoZdem4H33Fmh6f893nck9+H4yjypSSehsDdZdXAUV43v+HaOJfsqQ3HvH3dEMLTugoVfZUh17f75rxxaJB0ki8zdQWB7sxLHwe2oJytU1nGrv9b107SSBzS8FyfdAxIp6JvHyccHvsMeRWfg9pZN7thsPv59oLPibdLYFZW7bDbu3huVvyUgYHXv/1oaDC+6CWdcM+GqVCPbhvR3NLFq6lsde3kzUjEtnTeCLZx9LTaQetr0IL3ftEDYHr12boXVnz9pTXzuJ3uIlYY2qZk8Nq3R0UJPf8CfYtjpYTzQBNTNh3OyeNcmm2uAfIRKDo06CUz8bvI8+ESon567tPJ0Od3bhP12iZB81r3D5jsawdrg7KGPpUbltXolEoKA0eB2KaCzYaZWPg/GzBya2XEglg9+GRTJq1jn8fpMd0BgmxV2boXFrcDTZ11Fm0T6ONHMhnQqTS3v/ScG9/6bB/vRXeeg+CuvoVQHoffSQsa0DNXb6gX8mC0oE/Xjn/UYu/NFzuMOVp0/k7846lqMbV8MfroM1T4AHj9vDojBiTLAjH3NqUNOLFgQ7jq7D00hsT82yt3Qy2JHvDpPKuqVBzdPTQe1p/Cw4+x9h0hlQMyOo0R0uIhGIJCDWT022r+ULy4OX5EY0BtGywdteLLHniO5wE4kGr/jAP+x9uFEi6ENHMs3/WPwKxYkYv/3SXMZuXQKP/g/Y+CIUlMOHrocpFwY1+NKj9l0LPhipZHDIXTwq+52siMhBUiLow8Il77B68y6eOv0txj7wZWj4a1DjOe9OmHb5oTcz7E80FrbbiojknhJBLys31PPTZ9/m4TGPcOLLT8L4OfDx/w8+eP7A1/xFRA4DSgQZmtuT/P0jy/lZ0U+YW/8n+PAt8LFv6FpxERnWlAgy3Pnbl/lG0+2cHXkVzv0unHHjUIckIpJzSgShZavWccGrX2JW5G24cCHM+NuhDklEZFAoEQD12zcz+n9dzLGRjSQ//a/ET714qEMSERk0ed9LlzfX0X7vx5nom9ly3n1KAiKSd/I+Ebz/9A8Y3bGJ309fxMQ5Fw11OCIigy6/E0FrAyPXPMBT6dnMOfvCoY5GRGRI5HciWP5zEskmfmGfYky5bkMXkfyU00RgZvPM7C0zW2tmt/Yxf4KZLTWzl81slZmdn8t4euhohhd+zCuFs+moPmXou44WERkiOUsEZhYFFgHnAScCl5nZib0W+zqw2N2nA5cCP85VPHt56QFoqeMnqYs4bnSOu4wQETmM5fKIYDaw1t3fdfcO4GGg99lYB7q6SiwHtuQwnj2SHfCnhaQmnMHTjZOVCEQkr+UyEdQAGzPGN4XTMt0GXGFmm4CngBv6WpGZXWtmK8xsRW1t7aFH9upD0LiFDSd+EYBjq5UIRCR/DfXJ4suAX7r7OOB84N/MbK+Y3P0ed5/p7jOrq6sPbYupJDz3zzB2Oq/Eg4c8HDe65NDWKSJyBMtlItgMjM8YHxdOy3QNsBjA3Z8HCoFROYwJ1jwO9e/Bh29hbW0zsYgxsUqJQETyVy4TwXLgeDObbGYJgpPBT/Ra5q/AOQBmdgJBIhiAtp9+pNPwx3+C6inwwQtYV9vExKpi4tGhPjASERk6OdsDunsSuB54GniD4Oqg183sO2Y2P1zsFuALZvYq8BBwlbtn8ZDfg/TO07B9DZz5ZYhEWLu9SSeKRSTv5bTTOXd/iuAkcOa0b2YMrwHOyGUMGRuGZT+Aiolw8sV0ptJsqGth3slHD8rmRUQOV/nTJvLeMti8As68GaIxNtQ1k0y7rhgSkbyXP4mgYQNUHQdTPwfA2u1NAGoaEpG8lz/PIzjtyuDB8+Fzh9fVNgO6h0BEJH+OCKDHw+fXbm9ibHkhJQX5kwtFRPqSX4kgw9rtTRyrZiERkfxMBOm0s662Sc1CIiLkaSLYuruNlo6UThSLiJCniWCdrhgSEemWl4lAl46KiOyRn4mgtomK4jhVJYmhDkVEZMjlZyLY3sRx1aV6PKWICHmaCNZt1xVDIiJd8i4R1Dd3UNfcofMDIiKhvEsE62p1olhEJFPeJQJdMSQi0lNeJoKCWISaiqKhDkVE5LCQf4mgtoljqkuJRHTFkIgI5GMi0OMpRUR6yKtE0NqRYnNDK8fp0lERkW55lQje3dGEu04Ui4hkyqtEoCuGRET2lleJYN32JiIGk0YVD3UoIiKHjbxKBGtrm5gwspiCWHT/C4uI5In8SgS6YkhEZC95kwiSqTTrd7ToOcUiIr3kTSLYWN9KRyqtS0dFRHrJm0TQdcWQjghERHrKu0SgcwQiIj3FhjqAwTJ/2liOqS6hrDA+1KGIiBxW8iYR1FQUqcdREZE+5E3TkIiI9E2JQEQkzykRiIjkOSUCEZE8l9NEYGbzzOwtM1trZrf2s8xnzWyNmb1uZr/KZTwiIrK3nF01ZGZRYBFwLrAJWG5mT7j7moxljgf+ETjD3evNbHSu4hERkb7l8ohgNrDW3d919w7gYeCiXst8AVjk7vUA7r49h/GIiEgfcpkIaoCNGeObwmmZPgB8wMz+ZGYvmNm8vlZkZtea2QozW1FbW5ujcEVE8tNQnyyOAccDZwOXAfeaWUXvhdz9Hnef6e4zq6urBzlEEZHhbb+JwMwuNLODSRibgfEZ4+PCaZk2AU+4e6e7vwe8TZAYRERkkGSzg18AvGNmd5rZlANY93LgeDObbGYJ4FLgiV7LPE5wNICZjSJoKnr3ALYhIiKHaL+JwN2vAKYD64BfmtnzYZv9iP18LglcDzwNvAEsdvfXzew7ZjY/XOxpoM7M1gBLga+6e90hlEdERA6QuXt2C5pVAX8D3EywYz8OWOjuP8xdeHubOXOmr1ixYjA3KSJyxDOzle4+s6952ZwjmG9mjwHPAnFgtrufB0wFbhnIQEVEZPBlc0PZxcA/u/uyzInu3mJm1+QmLBERGSzZJILbgK1dI2ZWBBzl7uvdfUmuAhMRkcGRzVVD/wGkM8ZT4TQRERkGskkEsbCLCADC4UTuQhIRkcGUTSKozbjcEzO7CNiRu5BERGQwZXOO4IvAg2b2I8AI+g+6MqdRiYjIoNlvInD3dcBcMysNx5tyHpWIiAyarJ5HYGYXACcBhWYGgLt/J4dxiYjIIMnmhrKfEvQ3dANB09AlwMQcxyUiIoMkm5PFH3L3K4F6d/82cDpB53AiIjIMZJMI2sL3FjMbC3QCY3IXkoiIDKZszhE8GT4s5vvAS4AD9+Y0KhERGTT7TAThA2mWuHsD8Gsz+y1Q6O67BiU6ERHJuX02Dbl7GliUMd6uJCAiMrxkc45giZldbF3XjYqIyLCSTSL4O4JO5trNbLeZNZrZ7hzHJSIigySbO4v3+UhKERE5su03EZjZWX1N7/2gGhEROTJlc/noVzOGC4HZwErgYzmJSEREBlU2TUMXZo6b2Xjg7pxFJCIigyqbk8W9bQJOGOhARERkaGRzjuCHBHcTQ5A4phHcYSwiIsNANucIVmQMJ4GH3P1POYpHREQGWTaJ4FGgzd1TAGYWNbNid2/JbWgiIjIYsrqzGCjKGC8CnslNOCIiMtiySQSFmY+nDIeLcxeSiIgMpmwSQbOZndY1YmYzgNbchSQiIoMpm3MENwP/YWZbCB5VeTTBoytFRGQYyOaGsuVmNgX4YDjpLXfvzG1YIiIyWLJ5eP2XgBJ3X+3uq4FSM/t/ch+aiIgMhmzOEXwhfEIZAO5eD3whdyGJiMhgyiYRRDMfSmNmUSCRu5BERGQwZXOy+PfAI2b2s3D874Df5S4kEREZTNkkgn8ArgW+GI6vIrhySEREhoH9Ng2FD7B/EVhP8CyCjwFvZLNyM5tnZm+Z2Vozu3Ufy11sZm5mM7MLW0REBkq/RwRm9gHgsvC1A3gEwN0/ms2Kw3MJi4BzCbquXm5mT7j7ml7LjQBuIkg2IiIyyPZ1RPAmQe3/E+5+prv/EEgdwLpnA2vd/V137wAeBi7qY7nvAv8TaDuAdYuIyADZVyL4NLAVWGpm95rZOQR3FmerBtiYMb4pnNYt7LpivLv/732tyMyuNbMVZraitrb2AEIQEZH96TcRuPvj7n4pMAVYStDVxGgz+4mZ/fdD3bCZRYC7gFv2t6y73+PuM919ZnV19aFuWkREMmRzsrjZ3X8VPrt4HPAywZVE+7MZGJ8xPi6c1mUEcDLwrJmtB+YCT+iEsYjI4DqgZxa7e31YOz8ni8WXA8eb2WQzSwCXAk9krGuXu49y90nuPgl4AZjv7iv6Xp2IiOTCwTy8PivungSuB54muNx0sbu/bmbfMbP5udquiIgcmGxuKDto7v4U8FSvad/sZ9mzcxmLiIj0LWdHBCIicmRQIhARyXNKBCIieU6JQEQkzykRiIjkOSUCEZE8p0QgIpLnlAhERPKcEoGISJ5TIhARyXNKBCIieU6JQEQkzykRiIjkOSUCEZE8p0QgIpLnlAhERPKcEoGISJ5TIhARyXNKBCIieU6JQEQkzykRiIjkOSUCEZE8p0QgIpLnlAhERPKcEoGISJ5TIhARyXNKBCIieU6JQEQkzykRiIjkOSUCEZE8p0QgIpLnlAhERPKcEoGISJ5TIhARyXM5TQRmNs/M3jKztWZ2ax/zv2xma8xslZktMbOJuYxHRET2lrNEYGZRYBFwHnAicJmZndhrsZeBme5+KvAocGeu4hERkb7l8ohgNrDW3d919w7gYeCizAXcfam7t4SjLwDjchiPiIj0IZeJoAbYmDG+KZzWn2uA3/U1w8yuNbMVZraitrZ2AEMUEZHD4mSxmV0BzAS+39d8d7/H3We6+8zq6urBDU5EZJiL5XDdm4HxGePjwmk9mNl/A/5f4CPu3p7DeEREpA+5PCJYDhxvZpPNLAFcCjyRuYCZTQd+Bsx39+05jEVERPqRs0Tg7kngeuBp4A1gsbu/bmbfMbP54WLfB0qB/zCzV8zsiX5WJyIiOZLLpiHc/SngqV7Tvpkx/N9yuX0REdm/nCaCwdLZ2cmmTZtoa2sb6lDkMFFYWMi4ceOIx+NDHYrIYW9YJIJNmzYxYsQIJk2ahJkNdTgyxNyduro6Nm3axOTJk4c6HJHD3mFx+eihamtro6qqSklAADAzqqqqdIQokqVhkQgAJQHpQb8HkewNm0QgIiIHR4lgADQ0NPDjH//4oD57/vnn09DQMMARiYhkT4lgAOwrESSTyX1+9qmnnqKioiIXYR0SdyedTg91GCIyCIbFVUOZvv3k66zZsntA13ni2DK+deFJ/c6/9dZbWbduHdOmTePcc8/lggsu4Bvf+AaVlZW8+eabvP3223zyk59k48aNtLW1cdNNN3HttdcCMGnSJFasWEFTUxPnnXceZ555Jn/+85+pqanhN7/5DUVFRT229eSTT3L77bfT0dFBVVUVDz74IEcddRRNTU3ccMMNrFixAjPjW9/6FhdffDG///3v+drXvkYqlWLUqFEsWbKE2267jdLSUr7yla8AcPLJJ/Pb3/4WgI9//OPMmTOHlStX8tRTT/G9732P5cuX09raymc+8xm+/e1vA7B8+XJuuukmmpubKSgoYMmSJVxwwQUsXLiQadOmAXDmmWeyaNEipk6dOqB/DxEZWMMuEQyF733ve6xevZpXXnkFgGeffZaXXnqJ1atXd1++eN999zFy5EhaW1uZNWsWF198MVVVVT3W88477/DQQw9x77338tnPfpZf//rXXHHFFT2WOfPMM3nhhRcwM37+8+XCgOwAAAx9SURBVJ9z55138k//9E9897vfpby8nNdeew2A+vp6amtr+cIXvsCyZcuYPHkyO3fu3G9Z3nnnHe6//37mzp0LwB133MHIkSNJpVKcc845rFq1iilTprBgwQIeeeQRZs2axe7duykqKuKaa67hl7/8JXfffTdvv/02bW1tSgIiR4Bhlwj2VXMfTLNnz+5xDfvChQt57LHHANi4cSPvvPPOXolg8uTJ3bXpGTNmsH79+r3Wu2nTJhYsWMDWrVvp6Ojo3sYzzzzDww8/3L1cZWUlTz75JGeddVb3MiNHjtxv3BMnTuxOAgCLFy/mnnvuIZlMsnXrVtasWYOZMWbMGGbNmgVAWVkZAJdccgnf/e53+f73v899993HVVddtd/ticjQ0zmCHCkpKekefvbZZ3nmmWd4/vnnefXVV5k+fXqf17gXFBR0D0ej0T7PL9xwww1cf/31vPbaa/zsZz87qGvlY7FYj/b/zHVkxv3ee+/xgx/8gCVLlrBq1SouuOCCfW6vuLiYc889l9/85jcsXryYyy+//IBjE5HBp0QwAEaMGEFjY2O/83ft2kVlZSXFxcW8+eabvPDCCwe9rV27dlFTEzzf5/777++efu6557Jo0aLu8fr6eubOncuyZct47733ALqbhiZNmsRLL70EwEsvvdQ9v7fdu3dTUlJCeXk577//Pr/7XfDcoA9+8INs3bqV5cuXA9DY2NidtD7/+c9z4403MmvWLCorKw+6nCIyeJQIBkBVVRVnnHEGJ598Ml/96lf3mj9v3jySySQnnHACt956a4+mlwN12223cckllzBjxgxGjRrVPf3rX/869fX1nHzyyUydOpWlS5dSXV3NPffcw6c//WmmTp3KggULALj44ovZuXMnJ510Ej/60Y/4wAc+0Oe2pk6dyvTp05kyZQqf+9znOOOMMwBIJBI88sgj3HDDDUydOpVzzz23+0hhxowZlJWVcfXVVx90GUVkcJm7D3UMB2TmzJm+YsWKHtPeeOMNTjjhhCGKSDJt2bKFs88+mzfffJNIZGjrGfpdiOxhZivdfWZf83REIAPmgQceYM6cOdxxxx1DngREJHvD7qohGTpXXnklV1555VCHISIHSNU2EZE8p0QgIpLnlAhERPKcEoGISJ5TIhgipaWlQHC55Wc+85k+lzn77LPpfalsb3fffTctLS3d4+rWWkQOlBLBEBs7diyPPvroQX++dyI4XLu17o+6uxYZesPv8tHf3QrbXhvYdR59Cpz3vX5n33rrrYwfP54vfelLAN3dPH/xi1/koosuor6+ns7OTm6//XYuuuiiHp9dv349n/jEJ1i9ejWtra1cffXVvPrqq0yZMoXW1tbu5a677rq9uoNeuHAhW7Zs4aMf/SijRo1i6dKl3d1ajxo1irvuuov77rsPCLp+uPnmm1m/fr26uxaRHoZfIhgCCxYs4Oabb+5OBIsXL+bpp5+msLCQxx57jLKyMnbs2MHcuXOZP39+v8/T/clPfkJxcTFvvPEGq1at4rTTTuue11d30DfeeCN33XUXS5cu7dHdBMDKlSv5xS9+wYsvvoi7M2fOHD7ykY9QWVmp7q5FpIfhlwj2UXPPlenTp7N9+3a2bNlCbW0tlZWVjB8/ns7OTr72ta+xbNkyIpEImzdv5v333+foo4/ucz3Lli3jxhtvBODUU0/l1FNP7Z7XV3fQmfN7e+655/jUpz7V3Zvopz/9af74xz8yf/58dXctIj0Mv0QwRC655BIeffRRtm3b1t2524MPPkhtbS0rV64kHo8zadKkg+o2uqs76OXLl1NZWclVV111UOvp0ru768wmqC433HADX/7yl5k/fz7PPvsst9122wFv50C7u862fL27u165cuUBxyYie+hk8QBZsGABDz/8MI8++iiXXHIJEHQZPXr0aOLxOEuXLmXDhg37XMdZZ53Fr371KwBWr17NqlWrgP67g4b+u8D+8Ic/zOOPP05LSwvNzc089thjfPjDH866POruWiR/KBEMkJNOOonGxkZqamoYM2YMAJdffjkrVqzglFNO4YEHHmDKlCn7XMd1111HU1MTJ5xwAt/85jeZMWMG0H930ADXXnst8+bN46Mf/WiPdZ122mlcddVVzJ49mzlz5vD5z3+e6dOnZ10edXctkj/UDbUckbLp7lq/C5E91A21DCvq7lpkYOlksRxx1N21yMAaNtWpI62JS3JLvweR7A2LRFBYWEhdXZ3++QUIkkBdXR2FhYVDHYrIEWFYNA2NGzeOTZs2UVtbO9ShyGGisLCQcePGDXUYIkeEYZEI4vF4912tIiJyYHLaNGRm88zsLTNba2a39jG/wMweCee/aGaTchmPiIjsLWeJwMyiwCLgPOBE4DIzO7HXYtcA9e5+HPDPwP/MVTwiItK3XB4RzAbWuvu77t4BPAxc1GuZi4Cu/gseBc6x/rrmFBGRnMjlOYIaYGPG+CZgTn/LuHvSzHYBVcCOzIXM7Frg2nC0yczeOsiYRvVed57I13JD/pZd5c4v2ZR7Yn8zjoiTxe5+D3DPoa7HzFb0d4v1cJav5Yb8LbvKnV8Otdy5bBraDIzPGB8XTutzGTOLAeVAXQ5jEhGRXnKZCJYDx5vZZDNLAJcCT/Ra5gngb8PhzwD/5borTERkUOWsaShs878eeBqIAve5++tm9h1ghbs/Afwr8G9mthbYSZAscumQm5eOUPlabsjfsqvc+eWQyn3EdUMtIiIDa1j0NSQiIgdPiUBEJM/lTSLYX3cXw4WZ3Wdm281sdca0kWb2BzN7J3wfdg/5NbPxZrbUzNaY2etmdlM4fViX3cwKzewvZvZqWO5vh9Mnh922rA27cUkMday5YGZRM3vZzH4bjg/7cpvZejN7zcxeMbMV4bRD+p3nRSLIsruL4eKXwLxe024Flrj78cCScHy4SQK3uPuJwFzgS+HfeLiXvR34mLtPBaYB88xsLkF3Lf8cdt9ST9Cdy3B0E/BGxni+lPuj7j4t496BQ/qd50UiILvuLoYFd19GcAVWpsyuPO4HPjmoQQ0Cd9/q7i+Fw40EO4cahnnZPdAUjsbDlwMfI+i2BYZhuQHMbBxwAfDzcNzIg3L345B+5/mSCPrq7qJmiGIZCke5+9ZweBtw1FAGk2thL7bTgRfJg7KHzSOvANuBPwDrgAZ3T4aLDNff+93A3wPpcLyK/Ci3A/9pZivD7nfgEH/nR0QXEzJw3N3NbNheM2xmpcCvgZvdfXdmH4bDtezungKmmVkF8BgwZYhDyjkz+wSw3d1XmtnZQx3PIDvT3Teb2WjgD2b2ZubMg/md58sRQTbdXQxn75vZGIDwffsQx5MTZhYnSAIPuvv/CifnRdkB3L0BWAqcDlSE3bbA8Py9nwHMN7P1BE29HwP+heFfbtx9c/i+nSDxz+YQf+f5kgiy6e5iOMvsyuNvgd8MYSw5EbYP/yvwhrvflTFrWJfdzKrDIwHMrAg4l+D8yFKCbltgGJbb3f/R3ce5+ySC/+f/cvfLGeblNrMSMxvRNQz8d2A1h/g7z5s7i83sfII2xa7uLu4Y4pBywsweAs4m6Jb2feBbwOPAYmACsAH4rLv3PqF8RDOzM4E/Aq+xp834awTnCYZt2c3sVIKTg1GCit1id/+OmR1DUFMeCbwMXOHu7UMXae6ETUNfcfdPDPdyh+V7LByNAb9y9zvMrIpD+J3nTSIQEZG+5UvTkIiI9EOJQEQkzykRiIjkOSUCEZE8p0QgIpLnlAhEejGzVNizY9drwDqqM7NJmT3DihwO1MWEyN5a3X3aUAchMlh0RCCSpbAf+DvDvuD/YmbHhdMnmdl/mdkqM1tiZhPC6UeZ2WPhswJeNbMPhauKmtm94fMD/jO8I1hkyCgRiOytqFfT0IKMebvc/RTgRwR3qgP8ELjf3U8FHgQWhtMXAv8nfFbAacDr4fTjgUXufhLQAFyc4/KI7JPuLBbpxcya3L20j+nrCR4C827Ywd02d68ysx3AGHfvDKdvdfdRZlYLjMvs4iDsIvsP4QNEMLN/AOLufnvuSybSNx0RiBwY72f4QGT2fZNC5+pkiCkRiByYBRnvz4fDfyboARPgcoLO7yB4ZOB10P3wmPLBClLkQKgmIrK3ovCJX11+7+5dl5BWmtkqglr9ZeG0G4BfmNlXgVrg6nD6TcA9ZnYNQc3/OmArIocZnSMQyVJ4jmCmu+8Y6lhEBpKahkRE8pyOCERE8pyOCERE8pwSgYhInlMiEBHJc0oEIiJ5TolARCTP/V/tHKEY9cD/fQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTghsXN8vEpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c23f14-a28c-4f4f-bbd9-72b4f4891a51"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "      preds = torch.round(outputs)\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Informative', 'Informative'], digits=4))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not Informative     0.8949    0.7599    0.8219       504\n",
            "    Informative     0.8906    0.9563    0.9223      1030\n",
            "\n",
            "       accuracy                         0.8918      1534\n",
            "      macro avg     0.8927    0.8581    0.8721      1534\n",
            "   weighted avg     0.8920    0.8918    0.8893      1534\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}