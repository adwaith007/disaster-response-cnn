{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBerta_and_Resnext.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "88e1cccc-ff8c-4a75-81c3-29dcd75e3628"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzejvNI6kk3A"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "28ea408b-d867-4685-96a3-eabe7335d59e"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"informative\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/ColabNotebooks/train.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'tweet_text', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['tweet_text'] = df_train['tweet_text'].apply(clean_text)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/ColabNotebooks/val.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'tweet_text', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['tweet_text'] = df_val['tweet_text'].apply(clean_text)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/ColabNotebooks/test.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'tweet_text', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(clean_text)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, targets, paths, tokenizer, max_len):\n",
        "    self.tweets = tweets\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.paths = paths\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    tweet = str(self.tweets[item])\n",
        "    target = self.targets[item]\n",
        "    path = str(self.paths[item])\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "      truncation = True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'tweet_text': tweet,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long),\n",
        "      'tweet_image': img\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    tweets=df.tweet_text.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "    paths=df.image.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "    for param in self.roberta.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.resnext = torchvision.models.resnext50_32x4d(pretrained=True)\n",
        "    for param in self.resnext.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(self.roberta.config.hidden_size + 1000)\n",
        "\n",
        "    self.linear1 = nn.Linear(self.roberta.config.hidden_size + 1000, 1000)\n",
        "    self.relu1    = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(p=0.4)\n",
        "\n",
        "    self.linear2 = nn.Linear(1000, 500)\n",
        "    self.relu2    = nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "    self.linear3 = nn.Linear(500, 250)\n",
        "    self.relu3    = nn.ReLU()\n",
        "    self.dropout3 = nn.Dropout(p=0.1)\n",
        "\n",
        "    self.linear4 = nn.Linear(250, 125)\n",
        "    self.relu4    = nn.ReLU()\n",
        "    self.dropout4 = nn.Dropout(p=0.02)\n",
        "\n",
        "    self.linear5 = nn.Linear(125, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask, tweet_img):\n",
        "    output = self.roberta(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False\n",
        "    )\n",
        "    output = output[0]\n",
        "    text_output = torch.mean(output, 1)\n",
        "\n",
        "    image_output = self.resnext(tweet_img)\n",
        "    merged_output = torch.cat((text_output, image_output), dim=1)\n",
        "    bn_output = self.bn(merged_output)\n",
        "\n",
        "    linear1_output = self.linear1(bn_output)\n",
        "    relu1_output = self.relu1(linear1_output)\n",
        "    dropout1_output = self.dropout1(relu1_output)\n",
        "\n",
        "    linear2_output = self.linear2(dropout1_output)\n",
        "    relu2_output = self.relu2(linear2_output)\n",
        "    dropout2_output = self.dropout2(relu2_output)\n",
        "\n",
        "    linear3_output = self.linear3(dropout2_output)\n",
        "    relu3_output = self.relu3(linear3_output)\n",
        "    dropout3_output = self.dropout3(relu3_output)\n",
        "\n",
        "    linear4_output = self.linear4(dropout3_output)\n",
        "    relu4_output = self.relu4(linear4_output)\n",
        "    dropout4_output = self.dropout4(relu4_output)\n",
        "\n",
        "    linear5_output = self.linear5(dropout4_output)\n",
        "\n",
        "\n",
        "    probas = self.sigmoid(linear5_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "MAX_LEN = 150\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.BCELoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "599af9bc-55c6-4bab-e3d8-53c56f9a430a"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/Models/RobertaResnext/checkpoint.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "# best_accuracy = checkpoint['best_accuracy']\n",
        "\n",
        "# print(start_epoch)\n",
        "# print(best_accuracy)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {start_epoch + epoch + 1}/{start_epoch + EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/Models/RobertaResnext/checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6280071296189961 accuracy 0.6673263201749817\n",
            "Val   loss 0.5288203358650208 accuracy 0.7476160203432931\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4343449928258595 accuracy 0.8182480991563379\n",
            "Val   loss 0.39538925141096115 accuracy 0.8207247298156389\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3229427651355141 accuracy 0.8659514633892302\n",
            "Val   loss 0.33257097750902176 accuracy 0.856325492689129\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.287426485042823 accuracy 0.8786584730757212\n",
            "Val   loss 0.3301216661930084 accuracy 0.8556897647806739\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2681947852435865 accuracy 0.887511717529424\n",
            "Val   loss 0.327856220304966 accuracy 0.8626827717736809\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2497420624682778 accuracy 0.8982397666909696\n",
            "Val   loss 0.3280169889330864 accuracy 0.8626827717736809\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.237962638076983 accuracy 0.901572752838246\n",
            "Val   loss 0.3283008113503456 accuracy 0.8677685950413223\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2301978986514242 accuracy 0.906468076242058\n",
            "Val   loss 0.328991562128067 accuracy 0.8671328671328672\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2226677866358506 accuracy 0.9109467763774607\n",
            "Val   loss 0.3260311782360077 accuracy 0.8696757787666879\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.21702118688508085 accuracy 0.9146963857931465\n",
            "Val   loss 0.32704176753759384 accuracy 0.870311506675143\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20731469831968607 accuracy 0.9174044370378085\n",
            "Val   loss 0.3284630700945854 accuracy 0.8690400508582327\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19990585116963638 accuracy 0.9242787209665659\n",
            "Val   loss 0.3327144384384155 accuracy 0.8715829624920534\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19578229518313156 accuracy 0.9228205395271326\n",
            "Val   loss 0.3338182643055916 accuracy 0.8722186904005086\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.18802210453309512 accuracy 0.925111967503385\n",
            "Val   loss 0.33380505442619324 accuracy 0.8773045136681501\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.18501294051346026 accuracy 0.9261535256744089\n",
            "Val   loss 0.33755894005298615 accuracy 0.8753973299427845\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1740478802668421 accuracy 0.9329236537860639\n",
            "Val   loss 0.34298175573349 accuracy 0.8741258741258742\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1714645443778289 accuracy 0.9350067701281116\n",
            "Val   loss 0.3435495048761368 accuracy 0.8773045136681501\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.16597015214593788 accuracy 0.9341735235912926\n",
            "Val   loss 0.3391142189502716 accuracy 0.8811188811188811\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.16427057354073776 accuracy 0.9364649515675451\n",
            "Val   loss 0.34507307410240173 accuracy 0.8773045136681501\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.15954658624372983 accuracy 0.9409436517029476\n",
            "Val   loss 0.34818198531866074 accuracy 0.8779402415766052\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.15564615161795364 accuracy 0.9395896260806166\n",
            "Val   loss 0.3499988690018654 accuracy 0.8785759694850604\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.15301840791576787 accuracy 0.9413602749713571\n",
            "Val   loss 0.3524986729025841 accuracy 0.8785759694850604\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14841750028886294 accuracy 0.943755858764712\n",
            "Val   loss 0.3573068305850029 accuracy 0.8773045136681501\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14292243828898982 accuracy 0.9488594938027288\n",
            "Val   loss 0.3630445823073387 accuracy 0.8779402415766052\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14184062104476125 accuracy 0.947817935631705\n",
            "Val   loss 0.36167050153017044 accuracy 0.8773045136681501\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14058648128258555 accuracy 0.9474013123632955\n",
            "Val   loss 0.3680962324142456 accuracy 0.8753973299427845\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13824564336161865 accuracy 0.947817935631705\n",
            "Val   loss 0.37395837903022766 accuracy 0.8741258741258742\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13263365391053653 accuracy 0.9509426101447765\n",
            "Val   loss 0.3736078515648842 accuracy 0.8760330578512396\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13276180821029762 accuracy 0.9519841683158005\n",
            "Val   loss 0.3743208199739456 accuracy 0.8773045136681501\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13073237985372543 accuracy 0.9500052077908551\n",
            "Val   loss 0.37585942447185516 accuracy 0.8753973299427845\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12371444310012616 accuracy 0.9550046870117695\n",
            "Val   loss 0.38090160489082336 accuracy 0.8741258741258742\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1253750202687163 accuracy 0.9538589730236433\n",
            "Val   loss 0.38610774278640747 accuracy 0.8722186904005086\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12597218587210304 accuracy 0.9542755962920528\n",
            "Val   loss 0.3862464725971222 accuracy 0.8722186904005086\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1182980278604909 accuracy 0.9546922195604625\n",
            "Val   loss 0.38888130336999893 accuracy 0.8728544183089638\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12004757123558145 accuracy 0.9560462451827935\n",
            "Val   loss 0.3886600434780121 accuracy 0.8728544183089638\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11920268480714999 accuracy 0.9576085824393292\n",
            "Val   loss 0.3900262489914894 accuracy 0.8709472345835982\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12138321760453676 accuracy 0.9538589730236433\n",
            "Val   loss 0.3934808373451233 accuracy 0.8715829624920534\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11928746731657731 accuracy 0.9542755962920528\n",
            "Val   loss 0.39796996861696243 accuracy 0.8734901462174189\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11560276542839251 accuracy 0.9583376731590459\n",
            "Val   loss 0.3905828967690468 accuracy 0.8741258741258742\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11495071845619302 accuracy 0.9580252057077387\n",
            "Val   loss 0.39686527103185654 accuracy 0.8715829624920534\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11485142574498527 accuracy 0.9565670242683053\n",
            "Val   loss 0.4013626500964165 accuracy 0.8722186904005086\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11027583283813376 accuracy 0.9601083220497865\n",
            "Val   loss 0.40274444967508316 accuracy 0.8734901462174189\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11236905581072758 accuracy 0.9594833871471722\n",
            "Val   loss 0.3999132812023163 accuracy 0.8734901462174189\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11368371428627717 accuracy 0.9599000104155817\n",
            "Val   loss 0.3989999294281006 accuracy 0.8734901462174189\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11317870961992364 accuracy 0.9604207895010936\n",
            "Val   loss 0.4003702700138092 accuracy 0.8734901462174189\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10752399697115547 accuracy 0.962399750026039\n",
            "Val   loss 0.40220097452402115 accuracy 0.8734901462174189\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10909018234202736 accuracy 0.9617748151234247\n",
            "Val   loss 0.40467624366283417 accuracy 0.8741258741258742\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11025385950741015 accuracy 0.9601083220497865\n",
            "Val   loss 0.4044693186879158 accuracy 0.8734901462174189\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10993690435823641 accuracy 0.9597958545984793\n",
            "Val   loss 0.40351833403110504 accuracy 0.8741258741258742\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10541149777801413 accuracy 0.9625039058431413\n",
            "Val   loss 0.40327971428632736 accuracy 0.8734901462174189\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFmTSaxYMw-"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/RobertaResnext/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScOj15BovCww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4812373c-7fef-4277-c3e1-6d7de5a75066"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xddZnv8c+zd3buSZuk6TWFVm4tVEqhF5SLINMz5WJRECsDg3BQRo5yOaIzHI4KCryOgw7DoDgjOgg43DowCDgoCpapjqK9IKUtBQptbdq0Tdukud/2fs4fayXdSZN2t83ObrK/79drv/a6Za1nJSu/Z63fb63fMndHRESyVyTTAYiISGYpEYiIZDklAhGRLKdEICKS5ZQIRESynBKBiEiWUyKQEc3Mfm5mnxnsZQ8yhnPMrHo/8//FzL422NsVSZXpOQI50phZU9JoIdAOxMPxv3H3x4Y+qkNnZucA/+buVYe5no3AZ9395cGIS6RbTqYDEOnL3Yu7h/dX+JlZjrt3DWVsw5V+V7I/qhqSYaO7isXM/s7MtgE/NrMyM/uZmdWaWV04XJX0M6+a2WfD4avN7Ldm9p1w2Q1mdv4hLjvVzJaaWaOZvWxmD5jZvx0g/lvMbIeZ1ZjZNUnTHzazu8LhMeE+1JvZbjP7jZlFzOwnwFHAC2bWZGZ/Gy6/0MzWhMu/ambTk9a7MfxdrQKazewrZvZMn5juN7N/OpS/h4wcSgQy3IwHyoGjgesIjuEfh+NHAa3A9/bz8/OAt4ExwD3Av5qZHcKyjwN/BCqAO4C/TiHuUcAk4FrgATMr62e5W4BqoBIYB9wGuLv/NfBn4GPuXuzu95jZ8cATwM3h8i8SJIrcpPVdDlwIjAb+DVhgZqMhuEoAPg08eoDYZYRTIpDhJgHc7u7t7t7q7rvc/Rl3b3H3RuBu4CP7+flN7v5Dd48DjwATCArclJc1s6OAOcDX3b3D3X8LPH+AuDuBb7p7p7u/CDQBJwyw3ATg6HDZ3/jADXmLgP9091+5eyfwHaAA+HDSMve7++bwd1UDLAUuC+ctAHa6+4oDxC4jnBKBDDe17t7WPWJmhWb2AzPbZGYNBAXdaDOLDvDz27oH3L0lHCw+yGUnAruTpgFsPkDcu/rU0bcMsN1vA+uBX5rZ+2Z2637WORHYlBRjIoxj0n7iegS4Mhy+EvjJAeKWLKBEIMNN37PjWwjOrOe5eylwdjh9oOqewVADlJtZYdK0yYOxYndvdPdb3P0DwELgS2Z2XvfsPotvJagSAyCstpoMbEleZZ+f+SlwspnNAC4ChtUdWJIeSgQy3JUQtAvUm1k5cHu6N+jum4DlwB1mlmtmHwI+NhjrNrOLzOzYsFDfQ3DbbCKcvR34QNLii4ELzew8M4sRJMV24Hf7ib0NeJqwjcPd/zwYccvwpkQgw919BPXiO4HXgF8M0XavAD4E7ALuAp4iKIQP13HAywRtCL8Hvu/uS8J5/w/4aniH0Jfd/W2C6p3vEuz/xwgakzsOsI1HgA+iaiEJ6YEykUFgZk8B69w97Vckhyts7F4HjHf3hkzHI5mnKwKRQ2Bmc8zsmPAe/wXAxQT170c0M4sAXwKeVBKQbmlLBGb2UPjwzOoB5lv4MMt6M1tlZqemKxaRNBgPvEpQhXM/cL27v57RiA7AzIqABmA+Q9CWIsNH2qqGzOxsgn+SR919Rj/zLwBuAC4geHDnn9x9XlqCERGRAaXtisDdlwK797PIxQRJwt39NYJ7vyekKx4REelfJjudm0Tvh12qw2k1fRc0s+sIuhOgqKjotGnTpg1JgCIiI8WKFSt2untlf/OGRe+j7v4g8CDA7Nmzffny5RmOSERkeDGzTQPNy+RdQ1vo/TRmFb2fiBQRkSGQyUTwPHBVePfQ6cCesFMsEREZQmmrGjKzJ4BzgDEWvKbvdiAG4O7/QtBl7gUEHWy1ANf0vyYREUmntCUCd7/8APMd+EK6ti8iIqnRk8UiIlluWNw1JCJypHF3Gtu72NPSyZ7WThLuxKIRYlELvyPkRI2CWJSi3BwikUPrGb21I05tYzu1TW1UlRUyrjR/kPdEiUBEDlMi4TR3dNHU3kVzexflRXmUFcYY+A2gA3N3djd3sHFXC9sb2jAgEjGiZkQjRiRi5ESMgtwoxXk5FOXlUJybQ1FelJxohKb2LrbUtVJd18KW+laqw+HWjnhP4RyLGjnhcMQgnvCeT1fCibsTjztdiQSd8WB6ZzxBV/jd2NZFfUsHDW1dxBOp9cxgBsV5OZTmxyjJz6EkP4fC3JxgnwwiSfvn7uxs6mBnYzs7Gttpat/7PqO7Pj6DK08/ej9bOjRKBCIjQH1LB+u2NfL2tkbeq23CgPzcKIWxHApyIxTEohTk5mCwt8BLJMLvpEKwz3hnPEFLRxfN7fFe30GhH6epvatXQdWtOC+HqrICjiov5KjyQiaXF1KUl0OiV2EbbH9Paycbd7WwcWczG3c109i27/pSkZsToaMrsc+0qtEFFOXl0BlPhB+nK56gI+4k3MmJBIVwNEwye4f3Jo1oxMiPRSjOy+HoiiJGF8QYVRBjdGHwPaogRjRidIbr7exK0JVI0NGVoLUzTmNbV9KnsyeZJDz4eyTce74BKorzOHFiKR8pyaOyJI/K4jzGluYzfXzJIf1uDkSJQGQ/OuMJNuxsZmdjO3mxCPmxKPmxKAXhd15OpFcB0t9ZsLuTcOhKJOiKB2e8NXvaqNnTyrY9bdTsaWPbnjbqWzto60zQ1hkPPwnauuIkEs6owhijC3J7Cp7RhTFi0QjrdzTx9rZGdjTufRVCSX5wptnSEd+nYExVNDwLj0WNwrwcinKjFIZn3qMLc5lUVkBxXg7FeTGK83MoycuhOD+HwtwotY3tVNe18ufdLWzY2cx/vVNL+37iiEaMqrICjq4o4tSjRnN0RRFTxhQyYVQBsG9B2RV3WjriPVcg3UmppaOL0YW5VJUVMKmsgKqyAsYU5R1ylUw2USKQrOceFCy7mzvYsLOZddsaWFfTyLptjazf0URHPPXCtLsAjUToOds7UPVBUW6UCaMLKCsMqg0qS/LCRBOcyQPsae2kvjWoi95S10p9aydtnXGmjinizOPGMG18CSeML2Xa+BLGluT1JKR4wmnrjNPSEae1Ix7EGE068zUjGg2+c6J7q2AOpVpnIO5ObVM7bR2Jnm31JM+wDj0W1X0rmaREIMNOIuHUNLSxfkcT7+1oYntjGwWxoM64+6y1MDeH/FiE5vY4DW2dNLR20tDWFX53Utfcwa7mDnY1dbCruZ22zt6F/bjSPKaNL+Ws48cwfXwp40fl096VfLYeFKztXYl+q1QS7mG9L0QjkV5VDuWFuYwflc+EUfmMH5VPSX4sbb+raMQoCuvSM8XMGFsy+A2cMniUCGTItHXGqW/ppK6lgz2tnUTMyMuJkJsT6fnOzYnQ1pFgd0sHu5vb2d3c2fO9tb6V92qbeL+2mdbOeM96c6ORlM/aS/KDBrvyolzKi3I5dmwxFUW5VBTnUV4UVCtMH19KWVFuun4NIkccJQIZVO5OdV0ryzbuZtnG3ayq3sPu5g7qWjr2Oes+GLGoMa40n2Mqi5k3tYJjxxZzTGURx4QFecKhpaOLlo44zWGdcWtnnKK8KKX5MUoLYhTnBXXnItKbEoGkJJFwGtu6aGjr7NWQ2doRVJNsb2jjjxvrWLZhN9sa2oDg7PuUyaOZPqGUssIYowuDxs6ywlxGFcRwh454nPbOBB3xBO2dCdrjCfJzIj1n7N2f4ryc/dZbRw1K8mNprWYRGamUCLJIVzzBWzWN/GHDLpZt3M0bm/dgRk8dckleUL9elJdDW2fQeBp8guqcAzV6jivNY86UcuZOLWfOlHJOGFeiOzZEhgElghHG3Wlo7aK2qY0dje3UNrbz510tLNtUx8pNdT33fB9VXsjpHygnJxpJugWvi9rwAZb8WHBWPnVMEacdnUd5UYzyojxK83P63EIZ3FJZVpTLxFH5g3q3iYgMDSWCYSaRcHY0trO5roU/72oJvne3UL27lS31rdQ2tfd77/gJ40r4xKxJzJlaztwp5Ywfpbs4RCSgRHCEauuMs2FnM+/VNvHejvC7nztmzGB8aT6TywuZN7WcytLgKcTK8InEsSX5jCvNU925iAxIiSDDOuMJNu1q5u1tTby9vZF3tjXyzvZGNu5qprtK3gwmjS7gmMpiTv9ABVMqCjmqoojJ4ROUeTnRzO6EiAxrSgRDoLm9i2Ubd7OlvpWt9a1srW/rGd62p42usMSPGEypKOL4cSVcdPIEjhtXwjGVxUwdU0RBrgp7EUkPJYI0SSScP27czdMrqnnxzRpawsf7cyLG+FH5TBxdwJwp5Uwcnc+xY4s5Piz082Mq8EVkaCkRDLLNu1t4ZmU1z6ysZvPuVorzclg4cyIfmzmRYyqLqSzJ00NNInJEUSIYBO/VNvGrtdv55ZptrPxzPWbw4WMq+NL841lw0gRV64jIEU2J4BAkEs7rm+uDwn/tNt6vbQbgpIml3DL/eD5x6iSqygozHOUw4Q5N26FuI+zeAPWbIJoLZVOgfGrwXVCW4SBFRjYlgoPQ0NbJ4mWb+fF/b2RLfSs5EWPeB8r5zIem8BcnjmPS6IJMhzi4ugvp7WuCz4610NYABaMhf3Sf71GQWwSxQsgthtzCYDiaG6yjYQvs2RJ8dw/Xb4K6TdDVmrRRA/o8wZw/KkgIhRUQ7ww/HZBIGo73HQ7nDyQnP4y1aO8nVggWgURXuJ4OiIfDngj2JZoTfudCJCdYT/6o/n8nuUXB7yJ5O7FC6GqDtnporYPW+nC4HtobobMFOpr3fjpbIBHf/zZi3fvQ/bsvgpy8NBwQQyiRABwiw/xqOhHe6p3qfrgHPxOJBrcLDhElghRs3t3Cw7/byFPLNtPU3sXcqeV85S9P4NwTxjKqcIjvz493QkdT/wUf7C0MYmFBHIns/bnmWmjaEXw310LzzqBQ6lvoxduDs/Mda6Fl195tl0yAgnKoCQuuzuZD24eCMiidBOXHwLF/ERTyZVOgbCqMnhzEWrexz2dDsM1oblDI5RWHBXIMIrG9w92FdDQnmN7fP5N7sN/JhW73cHeBHyuAvNK96zXrk4S6gnW01kHtuqAwb2tgnyR2KGLdhXpYyFskSMRt9dDekNo6onm9E0ZBWTCcM0CvqolE/8k1McDbwsyC388+ySm8eks+Lru/O5p6J77WumC4o7nP8dcBHhagBeVQPBaKKoNP8djghMC9n3g7oLMVOlqCY7OjORjuaNpbIKeNhycQYTyJzr0nEBD8DXsdp7FgWs+yScdVt77LR3PhvNth5qJBj16JYD9WVdfzL//1Hr9YvY2IGReePIFrz5zKyVWjD32l7tBYs/cse9e7QdnR90wzGgsKluYd0NRdcO8I/nkORqwwOKDa9+xnIUsqQMODblQVnHABjJsB404KPoXlvX+sq2PvP3XbnvCfryUsVJuC4Xg7FI8LCv7SSVA6MSjk9icnD8bPCD7DSSIeFNTdhV1yQdTZsnc4VrBvAV0wGvJKIKdgb/LuT7wr+F239dlGr4KvMTh2ev429dCwNUjs8QGukizSO5FGwuMxEiW4SuvD49D03t5t9LqqG4BFgiub5MQxenKQ8KJ5+15xeSI4Wen+H6j5U/Dd0ZgUc5//mVhB0lVYcXDsdV+Zplskum/BHQlPFLsTQ69kl0j6nSed0ESiSUmlzxVu6YS0hK5EMIC3tzVyyfd/R0FulM+d/QE+86EpTByo6scddr4Lm34bVHX0p6MZdrwF21cH/zzdiscFB0Ai+Y8eDueVQnElFI2FsdOg6KxgOK8kOLPre8YA/Z8NxTuDs6judfWcYY0JE8UhXn7n5AbrKh57aD8/0kSiQcGezjaNaA4UVQSfI0lX+94TArPeBWH38XmgJJeqeGd4hj3Mq42OIEoE/XB3vvbcaibktfH8tSdRVlIE0UZobdt7cO96Fzb+d1D4b/pdcMYOYXVEPwd7Th5UToOTPr73LHvsdDWEysiQkwcl44JPunWf9MigUSLox3N/2krLxuX8uvBuYj86wCVvaRUc81E4+ozgU3HMkDbyiIgcLiWCPhrbOvn+f77G4wX3kVNcAefcFtTX9W2UKp0UFPyjj1LBLyLDmhJBH9/95Vvc2XEP5bEm7NP/ARNmZjokEZG0UiJI8va2RqqW3cm86Dq4+EdKAiKSFQahCX9kcHd+/cS9XBX9Ja2z/xecfFmmQxIRGRJKBKHfLPk5/7P+fmoq5lFw/p2ZDkdEZMgoEQBNO6uZtvQL1EcrGHvNE8G92iIiWUKJoKuDuocvp8SbqFv4MNHiI+xBHRGRNMv6RLDrD48zuWkVz03+O6ad8uFMhyMiMuSyPhEkVjzKhsQ45iz8m0yHIiKSEWlNBGa2wMzeNrP1ZnZrP/OPMrMlZva6ma0yswvSGc8+dr5L5e4V/AfnMXVM8ZBuWkTkSJG2RGBmUeAB4HzgROByMzuxz2JfBRa7+yzg08D30xVPv1Y+SpwIa8deSESvjxSRLJXOK4K5wHp3f9/dO4AngYv7LONAaTg8Ctiaxnh66+rA33iCV/00JlZNGbLNiogcadKZCCYBm5PGq8Npye4ArjSzauBF4Ib+VmRm15nZcjNbXltbOzjRvfMLrLmWf+s8h5Mmlh54eRGRESrTjcWXAw+7exVwAfATs337cHb3B919trvPrqysHJwtr3yUtoJxLE2czIlKBCKSxdKZCLYAk5PGq8Jpya4FFgO4+++BfGBMGmMK7KmG9S/zesWFEMnh+HElad+kiMiRKp2JYBlwnJlNNbNcgsbg5/ss82fgPAAzm06QCAap7mc/Xn8McJ71czm2spj8mN50JCLZK22JwN27gC8CLwFvEdwdtMbMvmlmC8PFbgE+Z2ZvAE8AV7v7ILz9ez8ScXj9J/CBc1haW6T2ARHJemntVMfdXyRoBE6e9vWk4bXAGemMYR/vvwp7NtN41tfYtrZN7QMikvUy3Vg89FY+CgVlvFEc5B8lAhHJdtmVCJp3wrr/hJmXs3p7OwAnTlAiEJHsll2J4I0ng3cPz/pr1m5tYNLoAkYX5mY6KhGRjMqeROAeVAtVzYFxJ7K2pkHVQiIiZFMi2PxH2Pk2nHoVrR1x3q9tUrWQiAhZlQheg9wSOOkS1m1rIOHo1lEREbIpEZxxE/zvNyGvmDVbGwDdMSQiAtmUCAAKygBYW9PAqIIYk0YXZDggEZHMy65EEFqztYETJ5RipncQiIhkXSLoiidYV9Og9gERkVDWJYINO5tp70qofUBEJJR1iWBtjRqKRUSSZV0iWLO1gdycCMdU6mX1IiKQhYlg7dYGThhXQiyadbsuItKvrCoN3Z01W/eooVhEJElWJYJtDW3UtXSqfUBEJElWJYI1W4KGYl0RiIjslVWJYG1NA2ZwwnglAhGRbtmVCLY2MKWiiOK8tL6hU0RkWMmqRLCmZo/aB0RE+siaRLCntZPNu1v1DgIRkT6yJhG8VaOGYhGR/mRNIlirdxCIiPQraxLB3Knl/O2CExhbkp/pUEREjihZc/vMjEmjmDFpVKbDEBE54mTNFYGIiPRPiUBEJMspEYiIZDklAhGRLKdEICKS5ZQIRESynBKBiEiWUyIQEclySgQiIlkurYnAzBaY2dtmtt7Mbh1gmU+Z2VozW2Nmj6czHhER2VfaupgwsyjwADAfqAaWmdnz7r42aZnjgP8DnOHudWY2Nl3xiIhI/9J5RTAXWO/u77t7B/AkcHGfZT4HPODudQDuviON8YiISD/SmQgmAZuTxqvDacmOB443s/82s9fMbEF/KzKz68xsuZktr62tTVO4IiLZKdONxTnAccA5wOXAD81sdN+F3P1Bd5/t7rMrKyuHOEQRkZHtgInAzD5mZoeSMLYAk5PGq8JpyaqB59290903AO8QJAYRERkiqRTwi4B3zeweM5t2EOteBhxnZlPNLBf4NPB8n2V+SnA1gJmNIagqev8gtiEiIofpgInA3a8EZgHvAQ+b2e/DOvuSA/xcF/BF4CXgLWCxu68xs2+a2cJwsZeAXWa2FlgCfMXddx3G/oiIyEEyd09tQbMK4K+BmwkK9mOB+939u+kLb1+zZ8/25cuXD+UmRUSGPTNb4e6z+5uXShvBQjN7FngViAFz3f18YCZwy2AGKiIiQy+VB8ouBf7R3ZcmT3T3FjO7Nj1hiYjIUEklEdwB1HSPmFkBMM7dN7r7K+kKTEREhkYqdw39O5BIGo+H00REZARIJRHkhF1EABAO56YvJBERGUqpJILapNs9MbOLgZ3pC0lERIZSKm0EnwceM7PvAUbQf9BVaY1KRESGzAETgbu/B5xuZsXheFPaoxIRkSGT0vsIzOxC4CQg38wAcPdvpjEuEREZIqk8UPYvBP0N3UBQNXQZcHSa4xIRkSGSSmPxh939KqDO3b8BfIigczgRERkBUkkEbeF3i5lNBDqBCekLSUREhlIqbQQvhC+L+TawEnDgh2mNSkREhsx+E0H4QppX3L0eeMbMfgbku/ueIYlORETSbr9VQ+6eAB5IGm9XEhARGVlSaSN4xcwute77RkVEZERJJRH8DUEnc+1m1mBmjWbWkOa4RERkiKTyZPF+X0kpIiLD2wETgZmd3d/0vi+qERGR4SmV20e/kjScD8wFVgAfTUtEIiIypFKpGvpY8riZTQbuS1tEIiIypFJpLO6rGpg+2IGIiEhmpNJG8F2Cp4khSBynEDxhLCIiI0AqbQTLk4a7gCfc/b/TFI+IiAyxVBLB00Cbu8cBzCxqZoXu3pLe0EREZCik9GQxUJA0XgC8nJ5wRERkqKWSCPKTX08ZDhemLyQRERlKqSSCZjM7tXvEzE4DWtMXkoiIDKVU2ghuBv7dzLYSvKpyPMGrK0VEZARI5YGyZWY2DTghnPS2u3emNywRERkqqby8/gtAkbuvdvfVQLGZ/a/0hyYiIkMhlTaCz4VvKAPA3euAz6UvJBERGUqpJIJo8ktpzCwK5KYvJBERGUqpNBb/AnjKzH4Qjv8N8PP0hSQiIkMplUTwd8B1wOfD8VUEdw6JiMgIcMCqofAF9n8ANhK8i+CjwFuprNzMFpjZ22a23sxu3c9yl5qZm9ns1MIWEZHBMuAVgZkdD1wefnYCTwG4+7mprDhsS3gAmE/QdfUyM3ve3df2Wa4EuIkg2YiIyBDb3xXBOoKz/4vc/Ux3/y4QP4h1zwXWu/v77t4BPAlc3M9ydwJ/D7QdxLpFRGSQ7C8RXALUAEvM7Idmdh7Bk8WpmgRsThqvDqf1CLuumOzu/7m/FZnZdWa23MyW19bWHkQIIiJyIAMmAnf/qbt/GpgGLCHoamKsmf2zmf2Pw92wmUWAe4FbDrSsuz/o7rPdfXZlZeXhblpERJKk0ljc7O6Ph+8urgJeJ7iT6EC2AJOTxqvCad1KgBnAq2a2ETgdeF4NxiIiQ+ug3lns7nXh2fl5KSy+DDjOzKaaWS7waeD5pHXtcfcx7j7F3acArwEL3X15/6sTEZF0OJSX16fE3buALwIvEdxuutjd15jZN81sYbq2KyIiByeVB8oOmbu/CLzYZ9rXB1j2nHTGIiIi/UvbFYGIiAwPSgQiIllOiUBEJMspEYiIZDklAhGRLKdEICKS5ZQIRESynBKBiEiWUyIQEclySgQiIllOiUBEJMspEYiIZDklAhGRLKdEICKS5ZQIRESynBKBiEiWUyIQEclySgQiIllOiUBEJMspEYiIZDklAhGRLKdEICKS5ZQIRESynBKBiEiWUyIQEclySgQiIllOiUBEJMspEYiIZDklAhGRLKdEICKS5ZQIRESynBKBiEiWUyIQEclySgQiIlkurYnAzBaY2dtmtt7Mbu1n/pfMbK2ZrTKzV8zs6HTGIyIi+0pbIjCzKPAAcD5wInC5mZ3YZ7HXgdnufjLwNHBPuuIREZH+pfOKYC6w3t3fd/cO4Eng4uQF3H2Ju7eEo68BVWmMR0RE+pHORDAJ2Jw0Xh1OG8i1wM/7m2Fm15nZcjNbXltbO4ghiojIEdFYbGZXArOBb/c3390fdPfZ7j67srJyaIMTERnhctK47i3A5KTxqnBaL2b2F8D/BT7i7u1pjEdERPqRziuCZcBxZjbVzHKBTwPPJy9gZrOAHwAL3X1HGmMREZEBpC0RuHsX8EXgJeAtYLG7rzGzb5rZwnCxbwPFwL+b2Z/M7PkBViciImmSzqoh3P1F4MU+076eNPwX6dy+iIgcWFoTwVDp7Oykurqatra2TIciR4j8/HyqqqqIxWKZDkXkiDciEkF1dTUlJSVMmTIFM8t0OJJh7s6uXbuorq5m6tSpmQ5H5Ih3RNw+erja2tqoqKhQEhAAzIyKigpdIYqkaEQkAkBJQHrR8SCSuhGTCERE5NAoEQyC+vp6vv/97x/Sz15wwQXU19cPckQiIqlTIhgE+0sEXV1d+/3ZF198kdGjR6cjrMPi7iQSiUyHISJDYETcNZTsGy+sYe3WhkFd54kTS7n9YycNOP/WW2/lvffe45RTTmH+/PlceOGFfO1rX6OsrIx169bxzjvv8PGPf5zNmzfT1tbGTTfdxHXXXQfAlClTWL58OU1NTZx//vmceeaZ/O53v2PSpEk899xzFBQU9NrWCy+8wF133UVHRwcVFRU89thjjBs3jqamJm644QaWL1+OmXH77bdz6aWX8otf/ILbbruNeDzOmDFjeOWVV7jjjjsoLi7my1/+MgAzZszgZz/7GQB/+Zd/ybx581ixYgUvvvgi3/rWt1i2bBmtra188pOf5Bvf+AYAy5Yt46abbqK5uZm8vDxeeeUVLrzwQu6//35OOeUUAM4880weeOABZs6cOah/DxEZXCMuEWTCt771LVavXs2f/vQnAF599VVWrlzJ6tWre25ffOihhygvL6e1tZU5c+Zw6aWXUlFR0Ws97777Lk888QQ//OEP+dSnPsUzzzzDlVde2WuZM888k9deew0z40c/+hH33HMP//AP/8Cdd97JqFGjePPNNwGoq6ujtraWz33ucyxdupSpU6eye/fuA+7Lu+++yyOPPMLpp58OwN133015eTnxeLsahGsAAAwxSURBVJzzzjuPVatWMW3aNBYtWsRTTz3FnDlzaGhooKCggGuvvZaHH36Y++67j3feeYe2tjYlAZFhYMQlgv2duQ+luXPn9rqH/f777+fZZ58FYPPmzbz77rv7JIKpU6f2nE2fdtppbNy4cZ/1VldXs2jRImpqaujo6OjZxssvv8yTTz7Zs1xZWRkvvPACZ599ds8y5eXlB4z76KOP7kkCAIsXL+bBBx+kq6uLmpoa1q5di5kxYcIE5syZA0BpaSkAl112GXfeeSff/va3eeihh7j66qsPuD0RyTy1EaRJUVFRz/Crr77Kyy+/zO9//3veeOMNZs2a1e897nl5eT3D0Wi03/aFG264gS9+8Yu8+eab/OAHPzike+VzcnJ61f8nryM57g0bNvCd73yHV155hVWrVnHhhRfud3uFhYXMnz+f5557jsWLF3PFFVccdGwiMvSUCAZBSUkJjY2NA87fs2cPZWVlFBYWsm7dOl577bVD3taePXuYNCl4v88jjzzSM33+/Pk88MADPeN1dXWcfvrpLF26lA0bNgD0VA1NmTKFlStXArBy5cqe+X01NDRQVFTEqFGj2L59Oz//efDeoBNOOIGamhqWLVsGQGNjY0/S+uxnP8uNN97InDlzKCsrO+T9FJGho0QwCCoqKjjjjDOYMWMGX/nKV/aZv2DBArq6upg+fTq33nprr6qXg3XHHXdw2WWXcdpppzFmzJie6V/96lepq6tjxowZzJw5kyVLllBZWcmDDz7IJZdcwsyZM1m0aBEAl156Kbt37+akk07ie9/7Hscff3y/25o5cyazZs1i2rRp/NVf/RVnnHEGALm5uTz11FPccMMNzJw5k/nz5/dcKZx22mmUlpZyzTXXHPI+isjQMnfPdAwHZfbs2b58+fJe09566y2mT5+eoYgk2datWznnnHNYt24dkUhmzzN0XIjsZWYr3H12f/N0RSCD5tFHH2XevHncfffdGU8CIpK6EXfXkGTOVVddxVVXXZXpMETkIOm0TUQkyykRiIhkOSUCEZEsp0QgIpLllAgypLi4GAhut/zkJz/Z7zLnnHMOfW+V7eu+++6jpaWlZ1zdWovIwVIiyLCJEyfy9NNPH/LP900ER2q31gNRd9cimTfybh/9+a2w7c3BXef4D8L53xpw9q233srkyZP5whe+ANDTzfPnP/95Lr74Yurq6ujs7OSuu+7i4osv7vWzGzdu5KKLLmL16tW0trZyzTXX8MYbbzBt2jRaW1t7lrv++uv36Q76/vvvZ+vWrZx77rmMGTOGJUuW9HRrPWbMGO69914eeughIOj64eabb2bjxo3q7lpEehl5iSADFi1axM0339yTCBYvXsxLL71Efn4+zz77LKWlpezcuZPTTz+dhQsXDvg+3X/+53+msLCQt956i1WrVnHqqaf2zOuvO+gbb7yRe++9lyVLlvTqbgJgxYoV/PjHP+YPf/gD7s68efP4yEc+QllZmbq7FpFeRl4i2M+Ze7rMmjWLHTt2sHXrVmpraykrK2Py5Ml0dnZy2223sXTpUiKRCFu2bGH79u2MHz++3/UsXbqUG2+8EYCTTz6Zk08+uWdef91BJ8/v67e//S2f+MQnenoTveSSS/jNb37DwoUL1d21iPQy8hJBhlx22WU8/fTTbNu2radzt8cee4za2lpWrFhBLBZjypQph9RtdHd30MuWLaOsrIyrr776kNbTrW9318lVUN1uuOEGvvSlL7Fw4UJeffVV7rjjjoPezsF2d53q/vXt7nrFihUHHZuI7KXG4kGyaNEinnzySZ5++mkuu+wyIOgyeuzYscRiMZYsWcKmTZv2u46zzz6bxx9/HIDVq1ezatUqYODuoGHgLrDPOussfvrTn9LS0kJzczPPPvssZ511Vsr7o+6uRbKHEsEgOemkk2hsbGTSpElMmDABgCuuuILly5fzwQ9+kEcffZRp06btdx3XX389TU1NTJ8+na9//eucdtppwMDdQQNcd911LFiwgHPPPbfXuk499VSuvvpq5s6dy7x58/jsZz/LrFmzUt4fdXctkj3UDbUMS6l0d63jQmQvdUMtI4q6uxYZXGoslmFH3V2LDK4Rczo13Kq4JL10PIikbkQkgvz8fHbt2qV/fgGCJLBr1y7y8/MzHYrIsDAiqoaqqqqorq6mtrY206HIESI/P5+qqqpMhyEyLIyIRBCLxXqeahURkYOT1qohM1tgZm+b2Xozu7Wf+Xlm9lQ4/w9mNiWd8YiIyL7SlgjMLAo8AJwPnAhcbmYn9lnsWqDO3Y8F/hH4+3TFIyIi/UvnFcFcYL27v+/uHcCTwMV9lrkY6O6/4GngPBuoa04REUmLdLYRTAI2J41XA/MGWsbdu8xsD1AB7ExeyMyuA64LR5vM7O1DjGlM33VniWzdb8jefdd+Z5dU9vvogWYMi8Zid38QePBw12Nmywd6xHoky9b9huzdd+13djnc/U5n1dAWYHLSeFU4rd9lzCwHGAXsSmNMIiLSRzoTwTLgODObama5wKeB5/ss8zzwmXD4k8CvXU+FiYgMqbRVDYV1/l8EXgKiwEPuvsbMvgksd/fngX8FfmJm64HdBMkinQ67emmYytb9huzdd+13djms/R523VCLiMjgGhF9DYmIyKFTIhARyXJZkwgO1N3FSGFmD5nZDjNbnTSt3Mx+ZWbvht8j7iW/ZjbZzJaY2VozW2NmN4XTR/S+m1m+mf3RzN4I9/sb4fSpYbct68NuXHIzHWs6mFnUzF43s5+F4yN+v81so5m9aWZ/MrPl4bTDOs6zIhGk2N3FSPEwsKDPtFuBV9z9OOCVcHyk6QJucfcTgdOBL4R/45G+7+3AR919JnAKsMDMTiforuUfw+5b6gi6cxmJbgLeShrPlv0+191PSXp24LCO86xIBKTW3cWI4O5LCe7ASpbclccjwMeHNKgh4O417r4yHG4kKBwmMcL33QNN4Wgs/DjwUYJuW2AE7jeAmVUBFwI/CseNLNjvARzWcZ4tiaC/7i4mZSiWTBjn7jXh8DZgXCaDSbewF9tZwB/Ign0Pq0f+BOwAfgW8B9S7e1e4yEg93u8D/hZIhOMVZMd+O/BLM1sRdr8Dh3mcD4suJmTwuLub2Yi9Z9jMioFngJvdvSG5D8ORuu/uHgdOMbPRwLPAtAyHlHZmdhGww91XmNk5mY5niJ3p7lvMbCzwKzNblzzzUI7zbLkiSKW7i5Fsu5lNAAi/d2Q4nrQwsxhBEnjM3f8jnJwV+w7g7vXAEuBDwOiw2xYYmcf7GcBCM9tIUNX7UeCfGPn7jbtvCb93ECT+uRzmcZ4tiSCV7i5GsuSuPD4DPJfBWNIirB/+V+Atd783adaI3nczqwyvBDCzAmA+QfvIEoJuW2AE7re7/x93r3L3KQT/z7929ysY4fttZkVmVtI9DPwPYDWHeZxnzZPFZnYBQZ1id3cXd2c4pLQwsyeAcwi6pd0O3A78FFgMHAVsAj7l7n0blIc1MzsT+A3wJnvrjG8jaCcYsftuZicTNA5GCU7sFrv7N83sAwRnyuXA68CV7t6euUjTJ6wa+rK7XzTS9zvcv2fD0RzgcXe/28wqOIzjPGsSgYiI9C9bqoZERGQASgQiIllOiUBEJMspEYiIZDklAhGRLKdEINKHmcXDnh27P4PWUZ2ZTUnuGVbkSKAuJkT21erup2Q6CJGhoisCkRSF/cDfE/YF/0czOzacPsXMfm1mq8zsFTM7Kpw+zsyeDd8V8IaZfThcVdTMfhi+P+CX4RPBIhmjRCCyr4I+VUOLkubtcfcPAt8jeFId4LvAI+5+MvAYcH84/X7gv8J3BZwKrAmnHwc84O4nAfXApWneH5H90pPFIn2YWZO7F/czfSPBS2DeDzu42+buFWa2E5jg7p3h9Bp3H2NmtUBVchcHYRfZvwpfIIKZ/R0Qc/e70r9nIv3TFYHIwfEBhg9Gct83cdRWJxmmRCBycBYlff8+HP4dQQ+YAFcQdH4HwSsDr4eel8eMGqogRQ6GzkRE9lUQvvGr2y/cvfsW0jIzW0VwVn95OO0G4Mdm9hWgFrgmnH4T8KCZXUtw5n89UIPIEUZtBCIpCtsIZrv7zkzHIjKYVDUkIpLldEUgIpLldEUgIpLllAhERLKcEoGISJZTIhARyXJKBCIiWe7/Ax9EnhM2JjTeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTghsXN8vEpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d297e9d-a197-4b08-dbbd-1d282021a66d"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "      preds = torch.round(outputs)\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Informative', 'Informative'], digits=4))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not Informative     0.8779    0.7421    0.8043       504\n",
            "    Informative     0.8827    0.9495    0.9149      1030\n",
            "\n",
            "       accuracy                         0.8814      1534\n",
            "      macro avg     0.8803    0.8458    0.8596      1534\n",
            "   weighted avg     0.8811    0.8814    0.8785      1534\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}