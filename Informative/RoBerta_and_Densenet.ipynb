{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBerta_and_Densenet.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qc_rI6d-khY2","outputId":"a31aceec-f04a-4935-8813-fd250df69112"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QzejvNI6kk3A"},"source":["!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zrBErHTyknRL","outputId":"0c529cf0-50f4-449c-9c62-1dcaab668417"},"source":["import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"c9plONFGko6I"},"source":["import transformers\n","from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n","import torch\n","from torchvision import transforms\n","import torchvision\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import re\n","from matplotlib import rc\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import defaultdict\n","from textwrap import wrap\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","\n","\n","def clean_text(text):\n","    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n","    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n","    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n","    text = re.sub('\\t', ' ',  text)\n","    text = re.sub(r\" +\", ' ', text)\n","    return text\n","\n","def label_to_target(text):\n","  if text == \"informative\":\n","    return 1\n","  else:\n","    return 0\n","\n","df_train = pd.read_csv(\"./gdrive/MyDrive/ColabNotebooks/train.tsv\", sep='\\t')\n","df_train = df_train[['image', 'tweet_text', 'label_text']]\n","df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n","df_train['tweet_text'] = df_train['tweet_text'].apply(clean_text)\n","df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n","\n","df_val = pd.read_csv(\"./gdrive/MyDrive/ColabNotebooks/val.tsv\", sep='\\t')\n","df_val = df_val[['image', 'tweet_text', 'label_text']]\n","df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n","df_val['tweet_text'] = df_val['tweet_text'].apply(clean_text)\n","df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n","\n","df_test = pd.read_csv(\"./gdrive/MyDrive/ColabNotebooks/test.tsv\", sep='\\t')\n","df_test = df_test[['image', 'tweet_text', 'label_text']]\n","df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n","df_test['tweet_text'] = df_test['tweet_text'].apply(clean_text)\n","df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YOjM9tz8ksnJ"},"source":["data_dir = \"./gdrive/MyDrive/\"\n","class DisasterTweetDataset(Dataset):\n","\n","  def __init__(self, tweets, targets, paths, tokenizer, max_len):\n","    self.tweets = tweets\n","    self.targets = targets\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","    self.paths = paths\n","    self.transform = transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","  \n","  def __len__(self):\n","    return len(self.tweets)\n","  \n","  def __getitem__(self, item):\n","    tweet = str(self.tweets[item])\n","    target = self.targets[item]\n","    path = str(self.paths[item])\n","    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n","    img = self.transform(img)  \n","\n","    encoding = self.tokenizer.encode_plus(\n","      tweet,\n","      add_special_tokens=True,\n","      max_length=self.max_len,\n","      return_token_type_ids=False,\n","      padding='max_length',\n","      return_attention_mask=True,\n","      return_tensors='pt',\n","      truncation = True\n","    )\n","\n","    return {\n","      'tweet_text': tweet,\n","      'input_ids': encoding['input_ids'].flatten(),\n","      'attention_mask': encoding['attention_mask'].flatten(),\n","      'targets': torch.tensor(target, dtype=torch.long),\n","      'tweet_image': img\n","    }\n","\n","def create_data_loader(df, tokenizer, max_len, batch_size):\n","  ds = DisasterTweetDataset(\n","    tweets=df.tweet_text.to_numpy(),\n","    targets=df.label_text.to_numpy(),\n","    paths=df.image.to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=max_len\n","  )\n","\n","  return DataLoader(\n","    ds,\n","    batch_size=batch_size,\n","    num_workers=2\n","  )\n","\n","\n","class TweetClassifier(nn.Module):\n","\n","  def __init__(self):\n","    super(TweetClassifier, self).__init__()\n","    self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n","    for param in self.roberta.parameters():\n","      param.requires_grad = False\n","    \n","    self.densenet = torchvision.models.densenet161(pretrained=True)\n","    for param in self.densenet.parameters():\n","      param.requires_grad = False\n","    \n","    self.bn = nn.BatchNorm1d(self.roberta.config.hidden_size + 1000)\n","\n","    self.linear1 = nn.Linear(self.roberta.config.hidden_size + 1000, 1000)\n","    self.relu1    = nn.ReLU()\n","    self.dropout1 = nn.Dropout(p=0.4)\n","\n","    self.linear2 = nn.Linear(1000, 500)\n","    self.relu2    = nn.ReLU()\n","    self.dropout2 = nn.Dropout(p=0.2)\n","\n","    self.linear3 = nn.Linear(500, 250)\n","    self.relu3    = nn.ReLU()\n","    self.dropout3 = nn.Dropout(p=0.1)\n","\n","    self.linear4 = nn.Linear(250, 125)\n","    self.relu4    = nn.ReLU()\n","    self.dropout4 = nn.Dropout(p=0.02)\n","\n","    self.linear5 = nn.Linear(125, 1)\n","    self.sigmoid = nn.Sigmoid()\n","  \n","  def forward(self, input_ids, attention_mask, tweet_img):\n","    output = self.roberta(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask,\n","      return_dict=False\n","    )\n","    output = output[0]\n","    text_output = torch.mean(output, 1)\n","\n","    image_output = self.densenet(tweet_img)\n","    merged_output = torch.cat((text_output, image_output), dim=1)\n","    bn_output = self.bn(merged_output)\n","\n","    linear1_output = self.linear1(bn_output)\n","    relu1_output = self.relu1(linear1_output)\n","    dropout1_output = self.dropout1(relu1_output)\n","\n","    linear2_output = self.linear2(dropout1_output)\n","    relu2_output = self.relu2(linear2_output)\n","    dropout2_output = self.dropout2(relu2_output)\n","\n","    linear3_output = self.linear3(dropout2_output)\n","    relu3_output = self.relu3(linear3_output)\n","    dropout3_output = self.dropout3(relu3_output)\n","\n","    linear4_output = self.linear4(dropout3_output)\n","    relu4_output = self.relu4(linear4_output)\n","    dropout4_output = self.dropout4(relu4_output)\n","\n","    linear5_output = self.linear5(dropout4_output)\n","\n","\n","    probas = self.sigmoid(linear5_output)\n","    return probas\n","\n","\n","def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n","  model = model.train()\n","\n","  losses = []\n","  correct_predictions = 0\n","  \n","  for d in data_loader:\n","    tweet_imgs = d[\"tweet_image\"].to(device)\n","    input_ids = d[\"input_ids\"].to(device)\n","    attention_mask = d[\"attention_mask\"].to(device)\n","    targets = d[\"targets\"].reshape(-1, 1).float()\n","    targets = targets.to(device)\n","\n","    outputs = model(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask,\n","      tweet_img = tweet_imgs\n","    )\n","\n","\n","    loss = loss_fn(outputs, targets)\n","\n","    correct_predictions += torch.sum(torch.round(outputs) == targets)\n","    losses.append(loss.item())\n","\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","  model = model.eval()\n","\n","  losses = []\n","  correct_predictions = 0\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","      tweet_imgs = d[\"tweet_image\"].to(device)\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].reshape(-1, 1).float()\n","      targets = targets.to(device)\n","\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask,\n","        tweet_img = tweet_imgs\n","      )\n","\n","      loss = loss_fn(outputs, targets)\n","\n","      correct_predictions += torch.sum(torch.round(outputs) == targets)\n","      losses.append(loss.item())\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cP2k2wgLkyo2"},"source":["BATCH_SIZE = 512\n","MAX_LEN = 150\n","\n","PRE_TRAINED_MODEL_NAME = 'roberta-base'\n","tokenizer = RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","\n","train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n","val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n","\n","\n","model = TweetClassifier()\n","model = model.to(device)\n","\n","EPOCHS = 50\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS\n","\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n",")\n","\n","loss_fn = nn.BCELoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CK6j-pnDk1d6","outputId":"a2a0fb00-543a-4808-d2a4-41d2602ec019"},"source":["history = defaultdict(list)\n","start_epoch = 0\n","best_accuracy = -1\n","\n","# checkpoint = torch.load(\"./gdrive/MyDrive/Models/RobertaDensenet/checkpoint.t7\")\n","# model.load_state_dict(checkpoint['state_dict'])\n","# optimizer.load_state_dict(checkpoint['optimizer'])\n","# start_epoch = checkpoint['epoch']\n","# best_accuracy = checkpoint['best_accuracy']\n","\n","# print(start_epoch)\n","# print(best_accuracy)\n","\n","\n","for epoch in range(EPOCHS):\n","\n","  print(f'Epoch {start_epoch + epoch + 1}/{start_epoch + EPOCHS}')\n","  print('-' * 10)\n","\n","  train_acc, train_loss = train_epoch(\n","    model,\n","    train_data_loader,    \n","    loss_fn, \n","    optimizer, \n","    device, \n","    scheduler, \n","    len(df_train)\n","  )\n","\n","  print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","  val_acc, val_loss = eval_model(\n","    model,\n","    val_data_loader,\n","    loss_fn, \n","    device, \n","    len(df_val)\n","  )\n","\n","  print(f'Val   loss {val_loss} accuracy {val_acc}')\n","  print()\n","\n","  history['train_acc'].append(train_acc)\n","  history['train_loss'].append(train_loss)\n","  history['val_acc'].append(val_acc)\n","  history['val_loss'].append(val_loss)\n","\n","  if val_acc > best_accuracy:\n","    state = {\n","            'best_accuracy': val_acc,\n","            'epoch': start_epoch+epoch+1,\n","            'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","    }\n","    savepath= \"./gdrive/MyDrive/Models/RobertaDensenet/checkpoint.t7\"\n","    torch.save(state,savepath)\n","    best_accuracy = val_acc"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.6197546187200045 accuracy 0.663889178210603\n","Val   loss 0.508452869951725 accuracy 0.7412587412587412\n","\n","Epoch 2/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.42878324107119914 accuracy 0.8189771898760545\n","Val   loss 0.4024423658847809 accuracy 0.8264462809917356\n","\n","Epoch 3/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.3242659098223636 accuracy 0.872096656598271\n","Val   loss 0.33994537591934204 accuracy 0.8569612205975843\n","\n","Epoch 4/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.2775348370012484 accuracy 0.8882408082491406\n","Val   loss 0.33836252987384796 accuracy 0.863318499682136\n","\n","Epoch 5/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.2598102673103935 accuracy 0.8954275596292053\n","Val   loss 0.33428825438022614 accuracy 0.866497139224412\n","\n","Epoch 6/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.24778560196098529 accuracy 0.9017810644724508\n","Val   loss 0.3337847888469696 accuracy 0.8696757787666879\n","\n","Epoch 7/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.23498061064042544 accuracy 0.9073013227788772\n","Val   loss 0.3336864113807678 accuracy 0.8722186904005086\n","\n","Epoch 8/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.22098360485152194 accuracy 0.9150088532444537\n","Val   loss 0.33636368066072464 accuracy 0.8728544183089638\n","\n","Epoch 9/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.21246424317359924 accuracy 0.9159462555983752\n","Val   loss 0.33608536422252655 accuracy 0.8753973299427845\n","\n","Epoch 10/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.20392279248488576 accuracy 0.9181335277575252\n","Val   loss 0.33834636956453323 accuracy 0.8811188811188811\n","\n","Epoch 11/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.19670846195597397 accuracy 0.9231330069784397\n","Val   loss 0.33666805922985077 accuracy 0.8849332485696122\n","\n","Epoch 12/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.18613896715013603 accuracy 0.9268826163941256\n","Val   loss 0.3460252806544304 accuracy 0.8817546090273363\n","\n","Epoch 13/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.18307315362127205 accuracy 0.9265701489428184\n","Val   loss 0.34958045929670334 accuracy 0.8836617927527018\n","\n","Epoch 14/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.17320249896300466 accuracy 0.9345901468597021\n","Val   loss 0.3527352437376976 accuracy 0.8804831532104259\n","\n","Epoch 15/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.16857017341412997 accuracy 0.9319862514321424\n","Val   loss 0.35506413877010345 accuracy 0.8823903369357915\n","\n","Epoch 16/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.16422725194378904 accuracy 0.9389646911780023\n","Val   loss 0.36129549890756607 accuracy 0.8823903369357915\n","\n","Epoch 17/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.15654901768031873 accuracy 0.9381314446411831\n","Val   loss 0.3627358451485634 accuracy 0.8862047043865225\n","\n","Epoch 18/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.15836613115511441 accuracy 0.9395896260806166\n","Val   loss 0.36294256895780563 accuracy 0.8849332485696122\n","\n","Epoch 19/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.15205342479442296 accuracy 0.9425059889594833\n","Val   loss 0.3710329458117485 accuracy 0.8868404322949778\n","\n","Epoch 20/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.14829417514173607 accuracy 0.9444849494844286\n","Val   loss 0.36893465369939804 accuracy 0.8862047043865225\n","\n","Epoch 21/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.14131991800509 accuracy 0.9465680658264763\n","Val   loss 0.37616243958473206 accuracy 0.8849332485696122\n","\n","Epoch 22/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.13571300592861676 accuracy 0.9493802728882408\n","Val   loss 0.3776934742927551 accuracy 0.8893833439287985\n","\n","Epoch 23/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.13408832487307096 accuracy 0.9482345589001145\n","Val   loss 0.38458286970853806 accuracy 0.8874761602034329\n","\n","Epoch 24/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.1294630840420723 accuracy 0.9500052077908551\n","Val   loss 0.3834650591015816 accuracy 0.891290527654164\n","\n","Epoch 25/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.1281807716739805 accuracy 0.9495885845224455\n","Val   loss 0.3924914672970772 accuracy 0.8874761602034329\n","\n","Epoch 26/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.1274837490759398 accuracy 0.951880012498698\n","Val   loss 0.38909008353948593 accuracy 0.8881118881118881\n","\n","Epoch 27/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.12130211254483775 accuracy 0.9561504009998958\n","Val   loss 0.39205923676490784 accuracy 0.8906547997457088\n","\n","Epoch 28/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.12278581763568677 accuracy 0.9556296219143838\n","Val   loss 0.39953042566776276 accuracy 0.8887476160203434\n","\n","Epoch 29/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.1153973599797801 accuracy 0.9592750755129673\n","Val   loss 0.39831091463565826 accuracy 0.891290527654164\n","\n","Epoch 30/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.11448720372036884 accuracy 0.9576085824393292\n","Val   loss 0.4081357344985008 accuracy 0.8862047043865225\n","\n","Epoch 31/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.11129753644529142 accuracy 0.9616706593063222\n","Val   loss 0.4078091084957123 accuracy 0.8906547997457088\n","\n","Epoch 32/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.11278607539440456 accuracy 0.9601083220497865\n","Val   loss 0.41081351786851883 accuracy 0.8874761602034329\n","\n","Epoch 33/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.10901809248485063 accuracy 0.9603166336839912\n","Val   loss 0.41018904000520706 accuracy 0.8862047043865225\n","\n","Epoch 34/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.10720863703050111 accuracy 0.9597958545984793\n","Val   loss 0.4121255502104759 accuracy 0.8874761602034329\n","\n","Epoch 35/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.10846097688925893 accuracy 0.9615665034892198\n","Val   loss 0.4142134487628937 accuracy 0.8874761602034329\n","\n","Epoch 36/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.1041975986016424 accuracy 0.965107801270701\n","Val   loss 0.4186727851629257 accuracy 0.8900190718372537\n","\n","Epoch 37/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.10286509676983482 accuracy 0.9636496198312675\n","Val   loss 0.4194330647587776 accuracy 0.8893833439287985\n","\n","Epoch 38/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.1019319809581104 accuracy 0.9635454640141652\n","Val   loss 0.42339394241571426 accuracy 0.8900190718372537\n","\n","Epoch 39/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.09927870176340405 accuracy 0.9643787105509842\n","Val   loss 0.4230216071009636 accuracy 0.8900190718372537\n","\n","Epoch 40/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.10076013129008443 accuracy 0.96375377564837\n","Val   loss 0.4207986891269684 accuracy 0.8919262555626192\n","\n","Epoch 41/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.09615204993047212 accuracy 0.9668784501614415\n","Val   loss 0.42382103204727173 accuracy 0.8893833439287985\n","\n","Epoch 42/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.10176712666687213 accuracy 0.9642745547338819\n","Val   loss 0.423651859164238 accuracy 0.8900190718372537\n","\n","Epoch 43/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.10291450745181034 accuracy 0.9606291011352984\n","Val   loss 0.4262768253684044 accuracy 0.8893833439287985\n","\n","Epoch 44/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0988793294680746 accuracy 0.9636496198312675\n","Val   loss 0.42839762568473816 accuracy 0.8906547997457088\n","\n","Epoch 45/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.09851898527459095 accuracy 0.9648994896364962\n","Val   loss 0.42532218247652054 accuracy 0.891290527654164\n","\n","Epoch 46/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.09272845363930653 accuracy 0.9683366316008749\n","Val   loss 0.4257383719086647 accuracy 0.8906547997457088\n","\n","Epoch 47/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.09823737313088618 accuracy 0.9648994896364962\n","Val   loss 0.4265664145350456 accuracy 0.8893833439287985\n","\n","Epoch 48/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0968732088804245 accuracy 0.9657327361733152\n","Val   loss 0.4283563867211342 accuracy 0.8887476160203434\n","\n","Epoch 49/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.09745080651421296 accuracy 0.9658368919904177\n","Val   loss 0.4283009469509125 accuracy 0.8893833439287985\n","\n","Epoch 50/50\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.09358370853097815 accuracy 0.9668784501614415\n","Val   loss 0.42813387513160706 accuracy 0.8893833439287985\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CGFmTSaxYMw-"},"source":["state = {\n","        'epoch': start_epoch + EPOCHS,\n","        'state_dict': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","}\n","savepath= \"./gdrive/MyDrive/Models/RobertaDensenet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n","torch.save(state,savepath)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ScOj15BovCww","colab":{"base_uri":"https://localhost:8080/","height":295},"outputId":"d08deb15-0441-49dc-d76a-c18b19925c28"},"source":["plt.plot(history['train_acc'], label='train accuracy')\n","plt.plot(history['val_acc'], label='validation accuracy')\n","\n","plt.title('Training history')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1]);"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"pTghsXN8vEpo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a96b174d-0beb-423c-db76-506d92658f9e"},"source":["def get_predictions(model, data_loader):\n","  model = model.eval()\n","  \n","  predictions = []\n","  real_values = []\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","\n","      input_ids = d[\"input_ids\"].to(device)\n","      tweet_imgs = d[\"tweet_image\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].reshape(-1, 1).float()\n","      targets = targets.to(device)\n","\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask,\n","        tweet_img = tweet_imgs\n","      )\n","      preds = torch.round(outputs)\n","\n","\n","      predictions.extend(preds)\n","      real_values.extend(targets)\n","\n","  predictions = torch.stack(predictions).cpu()\n","  real_values = torch.stack(real_values).cpu()\n","  return predictions, real_values\n","\n","y_pred, y_test = get_predictions(\n","  model,\n","  test_data_loader\n",")\n","\n","print(classification_report(y_test, y_pred, target_names=['Not Informative', 'Informative'], digits=4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["                 precision    recall  f1-score   support\n","\n","Not Informative     0.8894    0.7659    0.8230       504\n","    Informative     0.8927    0.9534    0.9221      1030\n","\n","       accuracy                         0.8918      1534\n","      macro avg     0.8911    0.8596    0.8725      1534\n","   weighted avg     0.8916    0.8918    0.8895      1534\n","\n"],"name":"stdout"}]}]}