{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KimCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkYOJmxmJ2pQ",
        "outputId": "3afb07cf-aade-4b1e-8b97-85128efece0f"
      },
      "source": [
        "from google.colab import drive\n",
        " \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVfbg6pWv7o_"
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36GiLe3tUEw7",
        "outputId": "7b9bba0a-9b8f-4a75-b2fd-25821bd0cb65"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wwc4P3iJ-C0"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import re \n",
        "import numpy as np \n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKue_-UPEyQV"
      },
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"informative\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/Models/train_processed.tsv\", sep='\\t')\n",
        "df_train = df_train[['tweet_text', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['tweet_text'] = df_train['tweet_text'].apply(clean_text)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/Models/val_processed.tsv\", sep='\\t')\n",
        "df_val = df_val[['tweet_text', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['tweet_text'] = df_val['tweet_text'].apply(clean_text)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/Models/test_processed.tsv\", sep='\\t')\n",
        "df_test = df_test[['tweet_text', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(clean_text)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "568kP1-2E4PC"
      },
      "source": [
        "weights = KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/crisisNLP_word2vec_model/crisisNLP_word_vector.bin', binary=True)\n",
        "weights = torch.FloatTensor(weights.vectors)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMSghDbajV4b"
      },
      "source": [
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, targets):\n",
        "    self.tweets = tweets\n",
        "    self.targets = targets  \n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    tweet = self.tweets[item]\n",
        "    target = self.targets[item]\n",
        "\n",
        "    tweet = tweet.split()\n",
        "    tweet = list(map(int, tweet))\n",
        "\n",
        "    return {\n",
        "      'input_ids': torch.tensor(tweet),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    tweets=df.tweet_text.to_numpy(),\n",
        "    targets=df.label_text.to_numpy()\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkI4V6vvErsc"
      },
      "source": [
        "class kimCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The embedding layer + CNN model that will be used to perform analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weights, vocab_size, output_size, embedding_dim, num_filters=100, kernel_sizes=[3, 4, 5], freeze_embeddings=True):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(kimCNN, self).__init__()\n",
        "\n",
        "        # set class vars\n",
        "        self.num_filters = num_filters\n",
        "        self.embedding_dim = embedding_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
        "        print(self.embedding)\n",
        "        \n",
        "        if freeze_embeddings:\n",
        "            self.embedding.requires_grad = False\n",
        "        \n",
        "        \n",
        "        self.y = nn.Conv2d(1,300, (4, embedding_dim))\n",
        "        self.y_relu = nn.ReLU()\n",
        "        self.y_pool = nn.MaxPool2d((MAX_SEQUENCE_LENGTH - 4 + 1, 1))\n",
        "\n",
        "\n",
        "        self.z = nn.Conv2d(1,300, (3, embedding_dim))\n",
        "        self.z_relu = nn.ReLU()\n",
        "        self.z_pool = nn.MaxPool2d((MAX_SEQUENCE_LENGTH - 3 + 1, 1))\n",
        "\n",
        "        self.z1 = nn.Conv2d(1,300, (2, embedding_dim))\n",
        "        self.z1_relu = nn.ReLU()\n",
        "        self.z1_pool = nn.MaxPool2d((MAX_SEQUENCE_LENGTH - 2 + 1, 1))\n",
        "\n",
        "        self.w1 = nn.Conv2d(1,300, (1, embedding_dim))\n",
        "        self.w1_relu = nn.ReLU()\n",
        "        self.w1_pool = nn.MaxPool2d((MAX_SEQUENCE_LENGTH - 1 + 1, 1))\n",
        "\n",
        "\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.dropout_1 = nn.Dropout(0.02)\n",
        "        self.fc_1 = nn.Linear(1200, 100)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.fc_2 = nn.Linear(100, 50)\n",
        "        self.relu_3 = nn.ReLU()\n",
        "        self.fc_3 = nn.Linear(50, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines how a batch of inputs, x, passes through the model layers.\n",
        "        Returns a single, sigmoid-activated class score as output.\n",
        "        \"\"\"\n",
        "\n",
        "        # embedded vectors\n",
        "        embeds = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n",
        "        # embeds.unsqueeze(1) creates a channel dimension that conv layers expect\n",
        "        embeds = embeds.unsqueeze(1)\n",
        "\n",
        "        y_out = self.y(embeds)\n",
        "        y_relu_out = self.y_relu(y_out)\n",
        "        y_pool_out = self.y_pool(y_relu_out)\n",
        "\n",
        "        z_out = self.z(embeds)\n",
        "        z_relu_out = self.z_relu(z_out)\n",
        "        z_pool_out = self.z_pool(z_relu_out)\n",
        "\n",
        "        z1_out = self.z1(embeds)\n",
        "        z1_relu_out = self.z1_relu(z1_out)\n",
        "        z1_pool_out = self.z1_pool(z1_relu_out)\n",
        "\n",
        "        w1_out = self.w1(embeds)\n",
        "        w1_relu_out = self.w1_relu(w1_out)\n",
        "        w1_pool_out = self.w1_pool(w1_relu_out)\n",
        "\n",
        "        merged_output = torch.cat((y_pool_out, z_pool_out, z1_pool_out, w1_pool_out), dim=1)\n",
        "        merged_output = torch.squeeze(merged_output,3)\n",
        "        merged_output = torch.squeeze(merged_output,2)\n",
        "        #print(merged_output.size())\n",
        "\n",
        "        relu1_out = self.relu_1(merged_output)\n",
        "\n",
        "        dropout1_out = self.dropout_1(relu1_out)\n",
        "        fc1_out = self.fc_1(dropout1_out)\n",
        "        \n",
        "        relu2_out = self.relu_2(fc1_out)\n",
        "        fc2_out = self.fc_2(relu2_out)\n",
        "\n",
        "        relu3_out = self.relu_3(fc2_out)\n",
        "        fc3_out = self.fc_3(relu3_out)\n",
        "\n",
        "        probas = self.sigmoid(fc3_out)\n",
        "\n",
        "        return probas\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU_85zYGR_DU",
        "outputId": "34dcdaad-aebe-470d-c614-b1792b1925d8"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "\n",
        "vocab_size = 2152854 #len(pretrained_words)\n",
        "output_size = 1 # binary class (1 or 0)\n",
        "embedding_dim = 300 #len(embed_lookup[pretrained_words[0]])\n",
        "num_filters = 512\n",
        "kernel_sizes = [2, 3, 4]\n",
        "MAX_SEQUENCE_LENGTH = 25\n",
        "\n",
        "batch_size = 128\n",
        "EPOCHS = 300\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, batch_size)\n",
        "val_data_loader = create_data_loader(df_val, batch_size)\n",
        "test_data_loader = create_data_loader(df_test, batch_size)\n",
        "\n",
        "model = kimCNN(weights, vocab_size, output_size, embedding_dim, num_filters, kernel_sizes)\n",
        "model = model.to(device)\n",
        "\n",
        "lr=0.00001\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10, min_lr=0, verbose=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding(2152854, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOxVqO83GQ8T"
      },
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(x=input_ids)\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(x=input_ids)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IWeP0P6yGOhN",
        "outputId": "3f858761-d242-451f-fad0-facdde1c820a"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/FYP/Models/bert-checkpoint-40.t7\", map_location=torch.device('cpu'))\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "\n",
        "# print(start_epoch)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {start_epoch + epoch + 1}/{start_epoch + EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  scheduler.step(val_acc)\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/Models/KimCNN/checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc\n",
        "\n",
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/KimCNN/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)\n",
        "\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "----------\n",
            "Train loss 0.6699074726355704 accuracy 0.6608686595146339\n",
            "Val   loss 0.6588621414624728 accuracy 0.6713286713286714\n",
            "\n",
            "Epoch 2/300\n",
            "----------\n",
            "Train loss 0.6554013318137119 accuracy 0.6608686595146339\n",
            "Val   loss 0.6424580904153677 accuracy 0.6713286713286714\n",
            "\n",
            "Epoch 3/300\n",
            "----------\n",
            "Train loss 0.6400482191851264 accuracy 0.6608686595146339\n",
            "Val   loss 0.6261463394531837 accuracy 0.6713286713286714\n",
            "\n",
            "Epoch 4/300\n",
            "----------\n",
            "Train loss 0.6264428637529674 accuracy 0.6608686595146339\n",
            "Val   loss 0.6134198445540208 accuracy 0.6713286713286714\n",
            "\n",
            "Epoch 5/300\n",
            "----------\n",
            "Train loss 0.6163939496404246 accuracy 0.6608686595146339\n",
            "Val   loss 0.6045880409387442 accuracy 0.6713286713286714\n",
            "\n",
            "Epoch 6/300\n",
            "----------\n",
            "Train loss 0.6090343037718221 accuracy 0.6608686595146339\n",
            "Val   loss 0.5979245396760794 accuracy 0.6713286713286714\n",
            "\n",
            "Epoch 7/300\n",
            "----------\n",
            "Train loss 0.6022653760094392 accuracy 0.6608686595146339\n",
            "Val   loss 0.5912424142544086 accuracy 0.6713286713286714\n",
            "\n",
            "Epoch 8/300\n",
            "----------\n",
            "Train loss 0.594713178904433 accuracy 0.6608686595146339\n",
            "Val   loss 0.5838687053093543 accuracy 0.6713286713286714\n",
            "\n",
            "Epoch 9/300\n",
            "----------\n",
            "Train loss 0.5860303820748078 accuracy 0.6609728153317362\n",
            "Val   loss 0.5755890057637141 accuracy 0.6713286713286714\n",
            "\n",
            "Epoch 10/300\n",
            "----------\n",
            "Train loss 0.5763753001627169 accuracy 0.6615977502343505\n",
            "Val   loss 0.5666594230211698 accuracy 0.6757787666878576\n",
            "\n",
            "Epoch 11/300\n",
            "----------\n",
            "Train loss 0.5657941546095046 accuracy 0.666597229455265\n",
            "Val   loss 0.5575141700414511 accuracy 0.6827717736808646\n",
            "\n",
            "Epoch 12/300\n",
            "----------\n",
            "Train loss 0.5553109771326968 accuracy 0.6872200812415373\n",
            "Val   loss 0.5488727390766144 accuracy 0.7031150667514304\n",
            "\n",
            "Epoch 13/300\n",
            "----------\n",
            "Train loss 0.5453735550767497 accuracy 0.7152379960420789\n",
            "Val   loss 0.5411660648309268 accuracy 0.722822631913541\n",
            "\n",
            "Epoch 14/300\n",
            "----------\n",
            "Train loss 0.5364426197974306 accuracy 0.7333611082178939\n",
            "Val   loss 0.5346217934901898 accuracy 0.736808645899555\n",
            "\n",
            "Epoch 15/300\n",
            "----------\n",
            "Train loss 0.5282998394809271 accuracy 0.7447140922820539\n",
            "Val   loss 0.5289021684573247 accuracy 0.7406230133502861\n",
            "\n",
            "Epoch 16/300\n",
            "----------\n",
            "Train loss 0.5205346400800505 accuracy 0.7529424018331423\n",
            "Val   loss 0.5235785383444566 accuracy 0.7425301970756516\n",
            "\n",
            "Epoch 17/300\n",
            "----------\n",
            "Train loss 0.5135360462100882 accuracy 0.7553379856264972\n",
            "Val   loss 0.5195225706467261 accuracy 0.7431659249841068\n",
            "\n",
            "Epoch 18/300\n",
            "----------\n",
            "Train loss 0.5076548421853467 accuracy 0.7571086345172378\n",
            "Val   loss 0.5161784153718215 accuracy 0.7476160203432931\n",
            "\n",
            "Epoch 19/300\n",
            "----------\n",
            "Train loss 0.5023260473420745 accuracy 0.7579418810540568\n",
            "Val   loss 0.5134656887788039 accuracy 0.7476160203432931\n",
            "\n",
            "Epoch 20/300\n",
            "----------\n",
            "Train loss 0.4976644559126151 accuracy 0.7586709717737735\n",
            "Val   loss 0.5112682580947876 accuracy 0.7495232040686586\n",
            "\n",
            "Epoch 21/300\n",
            "----------\n",
            "Train loss 0.49301079307731827 accuracy 0.7597125299447974\n",
            "Val   loss 0.5094367472025064 accuracy 0.7463445645263828\n",
            "\n",
            "Epoch 22/300\n",
            "----------\n",
            "Train loss 0.489428348839283 accuracy 0.7622122695552547\n",
            "Val   loss 0.507890909910202 accuracy 0.7469802924348379\n",
            "\n",
            "Epoch 23/300\n",
            "----------\n",
            "Train loss 0.4853542298078537 accuracy 0.7643995417144047\n",
            "Val   loss 0.5065595622246082 accuracy 0.7463445645263828\n",
            "\n",
            "Epoch 24/300\n",
            "----------\n",
            "Train loss 0.48211144224593516 accuracy 0.765545255702531\n",
            "Val   loss 0.5053856051885165 accuracy 0.7488874761602035\n",
            "\n",
            "Epoch 25/300\n",
            "----------\n",
            "Train loss 0.4787773721312222 accuracy 0.7670034371419644\n",
            "Val   loss 0.5043476201020755 accuracy 0.7501589319771138\n",
            "\n",
            "Epoch 26/300\n",
            "----------\n",
            "Train loss 0.475550134715281 accuracy 0.7686699302156025\n",
            "Val   loss 0.5034165657483615 accuracy 0.7514303877940242\n",
            "\n",
            "Epoch 27/300\n",
            "----------\n",
            "Train loss 0.4725153077589838 accuracy 0.7704405791063431\n",
            "Val   loss 0.5025863785010117 accuracy 0.7514303877940242\n",
            "\n",
            "Epoch 28/300\n",
            "----------\n",
            "Train loss 0.4694490020996646 accuracy 0.7721070721799812\n",
            "Val   loss 0.5018413319037511 accuracy 0.750794659885569\n",
            "\n",
            "Epoch 29/300\n",
            "----------\n",
            "Train loss 0.4665315774710555 accuracy 0.7732527861681074\n",
            "Val   loss 0.5011859994668227 accuracy 0.7514303877940242\n",
            "\n",
            "Epoch 30/300\n",
            "----------\n",
            "Train loss 0.46370698511600494 accuracy 0.7743985001562337\n",
            "Val   loss 0.5005929928559524 accuracy 0.7488874761602035\n",
            "\n",
            "Epoch 31/300\n",
            "----------\n",
            "Train loss 0.46094626305918945 accuracy 0.7762733048640766\n",
            "Val   loss 0.5000481536755195 accuracy 0.7469802924348379\n",
            "\n",
            "Epoch 32/300\n",
            "----------\n",
            "Train loss 0.45816095290999664 accuracy 0.7775231746693052\n",
            "Val   loss 0.49955220176623416 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 33/300\n",
            "----------\n",
            "Train loss 0.4554538809155163 accuracy 0.7787730444745339\n",
            "Val   loss 0.4991388825269846 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 34/300\n",
            "----------\n",
            "Train loss 0.4525809091956992 accuracy 0.7798146026455577\n",
            "Val   loss 0.4987646295474126 accuracy 0.7457088366179275\n",
            "\n",
            "Epoch 35/300\n",
            "----------\n",
            "Train loss 0.4497117545259626 accuracy 0.7821060306218102\n",
            "Val   loss 0.49844541457983166 accuracy 0.7463445645263828\n",
            "\n",
            "Epoch 36/300\n",
            "----------\n",
            "Train loss 0.44713356071396876 accuracy 0.7838766795125508\n",
            "Val   loss 0.49817758340101975 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 37/300\n",
            "----------\n",
            "Train loss 0.44450878234286056 accuracy 0.7843974585980626\n",
            "Val   loss 0.4979530710440416 accuracy 0.743801652892562\n",
            "\n",
            "Epoch 38/300\n",
            "----------\n",
            "Train loss 0.44216662645339966 accuracy 0.7857514842203936\n",
            "Val   loss 0.4980248166964604 accuracy 0.7431659249841068\n",
            "\n",
            "Epoch 39/300\n",
            "----------\n",
            "Train loss 0.44194456621220235 accuracy 0.7860639516717008\n",
            "Val   loss 0.49807204191501325 accuracy 0.743801652892562\n",
            "\n",
            "Epoch 40/300\n",
            "----------\n",
            "Train loss 0.44182053208351135 accuracy 0.7857514842203936\n",
            "Val   loss 0.4980902396715604 accuracy 0.7431659249841068\n",
            "\n",
            "Epoch 41/300\n",
            "----------\n",
            "Train loss 0.44141293160225215 accuracy 0.7873138214769294\n",
            "Val   loss 0.4980929493904114 accuracy 0.743801652892562\n",
            "\n",
            "Epoch 42/300\n",
            "----------\n",
            "Train loss 0.4413142094486638 accuracy 0.7880429121966461\n",
            "Val   loss 0.4980909961920518 accuracy 0.7457088366179275\n",
            "\n",
            "Epoch 43/300\n",
            "----------\n",
            "Train loss 0.44113894865701075 accuracy 0.7873138214769294\n",
            "Val   loss 0.49808301146213824 accuracy 0.7457088366179275\n",
            "\n",
            "Epoch 44/300\n",
            "----------\n",
            "Train loss 0.44063482355130346 accuracy 0.7867930423914176\n",
            "Val   loss 0.4980697540136484 accuracy 0.7457088366179275\n",
            "\n",
            "Epoch 45/300\n",
            "----------\n",
            "Train loss 0.44046735449841146 accuracy 0.7881470680137486\n",
            "Val   loss 0.4980548734848316 accuracy 0.7457088366179275\n",
            "\n",
            "Epoch 46/300\n",
            "----------\n",
            "Train loss 0.4400540178543643 accuracy 0.7873138214769294\n",
            "Val   loss 0.49804962827609134 accuracy 0.7457088366179275\n",
            "\n",
            "Epoch 47/300\n",
            "----------\n",
            "Train loss 0.43999640016179337 accuracy 0.7879387563795438\n",
            "Val   loss 0.4980328587385324 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 48/300\n",
            "----------\n",
            "Train loss 0.43975047610307993 accuracy 0.7874179772940318\n",
            "Val   loss 0.49802422523498535 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 49/300\n",
            "----------\n",
            "Train loss 0.4394533845939134 accuracy 0.787730444745339\n",
            "Val   loss 0.49802303543457616 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 50/300\n",
            "----------\n",
            "Train loss 0.4393392593452805 accuracy 0.7872096656598271\n",
            "Val   loss 0.4980217699821179 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 51/300\n",
            "----------\n",
            "Train loss 0.4395057264911501 accuracy 0.7874179772940318\n",
            "Val   loss 0.4980207154384026 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 52/300\n",
            "----------\n",
            "Train loss 0.43929020158554377 accuracy 0.7882512238308509\n",
            "Val   loss 0.49801907401818496 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 53/300\n",
            "----------\n",
            "Train loss 0.4392397446851981 accuracy 0.7886678470992605\n",
            "Val   loss 0.49801783378307635 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 54/300\n",
            "----------\n",
            "Train loss 0.4391078419591251 accuracy 0.7882512238308509\n",
            "Val   loss 0.4980164628762465 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 55/300\n",
            "----------\n",
            "Train loss 0.43931716562885986 accuracy 0.7880429121966461\n",
            "Val   loss 0.49801518137638384 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 56/300\n",
            "----------\n",
            "Train loss 0.4393951979122664 accuracy 0.7871055098427246\n",
            "Val   loss 0.4980146380571219 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 57/300\n",
            "----------\n",
            "Train loss 0.4393171021028569 accuracy 0.787730444745339\n",
            "Val   loss 0.49801358809837926 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 58/300\n",
            "----------\n",
            "Train loss 0.439582008672388 accuracy 0.7875221331111343\n",
            "Val   loss 0.4980126779813033 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 59/300\n",
            "----------\n",
            "Train loss 0.43911202447979075 accuracy 0.7883553796479533\n",
            "Val   loss 0.49801152027570283 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 60/300\n",
            "----------\n",
            "Train loss 0.4391835026050869 accuracy 0.7879387563795438\n",
            "Val   loss 0.49801141252884495 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 61/300\n",
            "----------\n",
            "Train loss 0.4394034860949767 accuracy 0.7872096656598271\n",
            "Val   loss 0.49801124517734235 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 62/300\n",
            "----------\n",
            "Train loss 0.4389971067246638 accuracy 0.7876262889282366\n",
            "Val   loss 0.49801113972297084 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 63/300\n",
            "----------\n",
            "Train loss 0.43937325281532186 accuracy 0.7878346005624414\n",
            "Val   loss 0.4980109998813042 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 64/300\n",
            "----------\n",
            "Train loss 0.4392436476130235 accuracy 0.7875221331111343\n",
            "Val   loss 0.49801092193676877 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 65/300\n",
            "----------\n",
            "Train loss 0.4392787571016111 accuracy 0.7871055098427246\n",
            "Val   loss 0.4980107820951022 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 66/300\n",
            "----------\n",
            "Train loss 0.43945891488539546 accuracy 0.7880429121966461\n",
            "Val   loss 0.49801069727310765 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 67/300\n",
            "----------\n",
            "Train loss 0.4390684307405823 accuracy 0.7889803145505676\n",
            "Val   loss 0.49801055972392744 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 68/300\n",
            "----------\n",
            "Train loss 0.4392885738297513 accuracy 0.7882512238308509\n",
            "Val   loss 0.49801045197706956 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 69/300\n",
            "----------\n",
            "Train loss 0.439114285534934 accuracy 0.7875221331111343\n",
            "Val   loss 0.49801037174004775 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 70/300\n",
            "----------\n",
            "Train loss 0.43924314058140707 accuracy 0.7872096656598271\n",
            "Val   loss 0.4980103006729713 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 71/300\n",
            "----------\n",
            "Train loss 0.43911881588007273 accuracy 0.7876262889282366\n",
            "Val   loss 0.4980102020960588 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 72/300\n",
            "----------\n",
            "Train loss 0.43910755766065496 accuracy 0.7880429121966461\n",
            "Val   loss 0.49801004849947417 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 73/300\n",
            "----------\n",
            "Train loss 0.43928055778930064 accuracy 0.7885636912821581\n",
            "Val   loss 0.4980099613849933 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 74/300\n",
            "----------\n",
            "Train loss 0.4390730406892927 accuracy 0.7879387563795438\n",
            "Val   loss 0.4980098352982448 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 75/300\n",
            "----------\n",
            "Train loss 0.43924666627457265 accuracy 0.7882512238308509\n",
            "Val   loss 0.49800977340111363 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 76/300\n",
            "----------\n",
            "Train loss 0.4392341010664639 accuracy 0.787730444745339\n",
            "Val   loss 0.49800968399414647 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 77/300\n",
            "----------\n",
            "Train loss 0.43931804597377777 accuracy 0.7878346005624414\n",
            "Val   loss 0.4980095739548023 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 78/300\n",
            "----------\n",
            "Train loss 0.43932855011601196 accuracy 0.7878346005624414\n",
            "Val   loss 0.49800949601026684 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 79/300\n",
            "----------\n",
            "Train loss 0.4393128376258047 accuracy 0.7879387563795438\n",
            "Val   loss 0.4980093790934636 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 80/300\n",
            "----------\n",
            "Train loss 0.43913923674508143 accuracy 0.7879387563795438\n",
            "Val   loss 0.49800927822406477 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 81/300\n",
            "----------\n",
            "Train loss 0.4392328717206654 accuracy 0.7872096656598271\n",
            "Val   loss 0.49800909482515776 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 82/300\n",
            "----------\n",
            "Train loss 0.43936731509472193 accuracy 0.7873138214769294\n",
            "Val   loss 0.4980090008332179 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 83/300\n",
            "----------\n",
            "Train loss 0.43919384479522705 accuracy 0.7876262889282366\n",
            "Val   loss 0.49800894122857314 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 84/300\n",
            "----------\n",
            "Train loss 0.4392532631754875 accuracy 0.7873138214769294\n",
            "Val   loss 0.4980088426516606 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 85/300\n",
            "----------\n",
            "Train loss 0.4391581776895021 accuracy 0.7881470680137486\n",
            "Val   loss 0.4980087440747481 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 86/300\n",
            "----------\n",
            "Train loss 0.43924012819403097 accuracy 0.7874179772940318\n",
            "Val   loss 0.4980086409128629 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 87/300\n",
            "----------\n",
            "Train loss 0.4392398344843011 accuracy 0.7866888865743151\n",
            "Val   loss 0.49800848731627834 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 88/300\n",
            "----------\n",
            "Train loss 0.43919022577373606 accuracy 0.7872096656598271\n",
            "Val   loss 0.49800838415439314 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 89/300\n",
            "----------\n",
            "Train loss 0.43925447958080394 accuracy 0.7871055098427246\n",
            "Val   loss 0.4980082397277539 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 90/300\n",
            "----------\n",
            "Train loss 0.43925529992894125 accuracy 0.7867930423914176\n",
            "Val   loss 0.49800814344332767 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 91/300\n",
            "----------\n",
            "Train loss 0.43893305094618545 accuracy 0.7870013540256223\n",
            "Val   loss 0.4980079921392294 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 92/300\n",
            "----------\n",
            "Train loss 0.4389654343065463 accuracy 0.7880429121966461\n",
            "Val   loss 0.4980078935623169 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 93/300\n",
            "----------\n",
            "Train loss 0.43929578638390493 accuracy 0.787730444745339\n",
            "Val   loss 0.4980077881079454 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 94/300\n",
            "----------\n",
            "Train loss 0.4393320095382239 accuracy 0.7873138214769294\n",
            "Val   loss 0.4980076597287105 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 95/300\n",
            "----------\n",
            "Train loss 0.43953594447750793 accuracy 0.7873138214769294\n",
            "Val   loss 0.49800752905698925 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 96/300\n",
            "----------\n",
            "Train loss 0.43901095500117854 accuracy 0.7884595354650556\n",
            "Val   loss 0.4980074488199674 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 97/300\n",
            "----------\n",
            "Train loss 0.4390781235538031 accuracy 0.7880429121966461\n",
            "Val   loss 0.49800737316791827 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 98/300\n",
            "----------\n",
            "Train loss 0.43913807994440984 accuracy 0.7873138214769294\n",
            "Val   loss 0.49800728605343747 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 99/300\n",
            "----------\n",
            "Train loss 0.43902340884271424 accuracy 0.7876262889282366\n",
            "Val   loss 0.4980071439192845 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 100/300\n",
            "----------\n",
            "Train loss 0.43912884710650696 accuracy 0.7875221331111343\n",
            "Val   loss 0.49800702241750866 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 101/300\n",
            "----------\n",
            "Train loss 0.4391339891835263 accuracy 0.7873138214769294\n",
            "Val   loss 0.49800695593540484 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 102/300\n",
            "----------\n",
            "Train loss 0.4391007991997819 accuracy 0.7875221331111343\n",
            "Val   loss 0.4980068665284377 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 103/300\n",
            "----------\n",
            "Train loss 0.43895986833070455 accuracy 0.7876262889282366\n",
            "Val   loss 0.4980067633665525 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 104/300\n",
            "----------\n",
            "Train loss 0.43904017892323044 accuracy 0.7876262889282366\n",
            "Val   loss 0.49800663727980393 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 105/300\n",
            "----------\n",
            "Train loss 0.4391028598735207 accuracy 0.7878346005624414\n",
            "Val   loss 0.49800655933526844 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 106/300\n",
            "----------\n",
            "Train loss 0.4391472476877664 accuracy 0.7875221331111343\n",
            "Val   loss 0.4980064561733833 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 107/300\n",
            "----------\n",
            "Train loss 0.43934205292086853 accuracy 0.7881470680137486\n",
            "Val   loss 0.49800634384155273 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 108/300\n",
            "----------\n",
            "Train loss 0.4392627363926486 accuracy 0.7884595354650556\n",
            "Val   loss 0.49800622233977687 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 109/300\n",
            "----------\n",
            "Train loss 0.43887380902704437 accuracy 0.7880429121966461\n",
            "Val   loss 0.49800612834783703 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 110/300\n",
            "----------\n",
            "Train loss 0.43902934499477086 accuracy 0.7887720029163628\n",
            "Val   loss 0.4980060664507059 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 111/300\n",
            "----------\n",
            "Train loss 0.43922307852067444 accuracy 0.7881470680137486\n",
            "Val   loss 0.498005958703848 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 112/300\n",
            "----------\n",
            "Train loss 0.43912071342531006 accuracy 0.7881470680137486\n",
            "Val   loss 0.4980057959373181 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 113/300\n",
            "----------\n",
            "Train loss 0.4390469714999199 accuracy 0.7882512238308509\n",
            "Val   loss 0.49800562170835644 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 114/300\n",
            "----------\n",
            "Train loss 0.4392005130648613 accuracy 0.7881470680137486\n",
            "Val   loss 0.4980054818666898 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 115/300\n",
            "----------\n",
            "Train loss 0.4392391266791444 accuracy 0.7871055098427246\n",
            "Val   loss 0.4980053718273456 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 116/300\n",
            "----------\n",
            "Train loss 0.4390126435380233 accuracy 0.7879387563795438\n",
            "Val   loss 0.49800528012789214 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 117/300\n",
            "----------\n",
            "Train loss 0.4387876814917514 accuracy 0.787730444745339\n",
            "Val   loss 0.4980052090608157 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 118/300\n",
            "----------\n",
            "Train loss 0.4390165052915874 accuracy 0.7879387563795438\n",
            "Val   loss 0.49800514257871187 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 119/300\n",
            "----------\n",
            "Train loss 0.43923967762997274 accuracy 0.7874179772940318\n",
            "Val   loss 0.49800506234169006 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 120/300\n",
            "----------\n",
            "Train loss 0.4390253103093097 accuracy 0.7872096656598271\n",
            "Val   loss 0.4980049798121819 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 121/300\n",
            "----------\n",
            "Train loss 0.43907057533138677 accuracy 0.7881470680137486\n",
            "Val   loss 0.49800484684797436 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 122/300\n",
            "----------\n",
            "Train loss 0.4389257619255467 accuracy 0.7866888865743151\n",
            "Val   loss 0.4980047895358159 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 123/300\n",
            "----------\n",
            "Train loss 0.4392740467661305 accuracy 0.7875221331111343\n",
            "Val   loss 0.49800468866641706 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 124/300\n",
            "----------\n",
            "Train loss 0.43894879716007335 accuracy 0.7880429121966461\n",
            "Val   loss 0.49800456716464114 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 125/300\n",
            "----------\n",
            "Train loss 0.43894877716114644 accuracy 0.7876262889282366\n",
            "Val   loss 0.4980044594177833 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 126/300\n",
            "----------\n",
            "Train loss 0.43912899729452637 accuracy 0.7887720029163628\n",
            "Val   loss 0.49800437230330247 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 127/300\n",
            "----------\n",
            "Train loss 0.439143925120956 accuracy 0.7873138214769294\n",
            "Val   loss 0.49800423704660857 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 128/300\n",
            "----------\n",
            "Train loss 0.43909746251608195 accuracy 0.7872096656598271\n",
            "Val   loss 0.4980041843194228 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 129/300\n",
            "----------\n",
            "Train loss 0.4390179012951098 accuracy 0.7876262889282366\n",
            "Val   loss 0.4980040926199693 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 130/300\n",
            "----------\n",
            "Train loss 0.43902794114853205 accuracy 0.7871055098427246\n",
            "Val   loss 0.49800399862802947 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 131/300\n",
            "----------\n",
            "Train loss 0.4390366795031648 accuracy 0.7876262889282366\n",
            "Val   loss 0.4980038542013902 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 132/300\n",
            "----------\n",
            "Train loss 0.438951009198239 accuracy 0.7881470680137486\n",
            "Val   loss 0.4980037487470187 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 133/300\n",
            "----------\n",
            "Train loss 0.4391007109692222 accuracy 0.7881470680137486\n",
            "Val   loss 0.49800369372734654 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 134/300\n",
            "----------\n",
            "Train loss 0.4389464047394301 accuracy 0.7878346005624414\n",
            "Val   loss 0.49800358368800235 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 135/300\n",
            "----------\n",
            "Train loss 0.43894120148922267 accuracy 0.7880429121966461\n",
            "Val   loss 0.4980034851110898 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 136/300\n",
            "----------\n",
            "Train loss 0.4390556306431168 accuracy 0.7876262889282366\n",
            "Val   loss 0.49800339799660903 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 137/300\n",
            "----------\n",
            "Train loss 0.4389792927785924 accuracy 0.7878346005624414\n",
            "Val   loss 0.498003384241691 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 138/300\n",
            "----------\n",
            "Train loss 0.43904532649015127 accuracy 0.7875221331111343\n",
            "Val   loss 0.4980033177595872 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 139/300\n",
            "----------\n",
            "Train loss 0.43915667620144394 accuracy 0.787730444745339\n",
            "Val   loss 0.49800325815494245 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 140/300\n",
            "----------\n",
            "Train loss 0.43918124233421524 accuracy 0.7881470680137486\n",
            "Val   loss 0.49800314811559826 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 141/300\n",
            "----------\n",
            "Train loss 0.43905864969680186 accuracy 0.7875221331111343\n",
            "Val   loss 0.49800309997338515 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 142/300\n",
            "----------\n",
            "Train loss 0.4389345359645392 accuracy 0.7876262889282366\n",
            "Val   loss 0.4980030289063087 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 143/300\n",
            "----------\n",
            "Train loss 0.43904365403087514 accuracy 0.7876262889282366\n",
            "Val   loss 0.49800293491436887 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 144/300\n",
            "----------\n",
            "Train loss 0.43896732753828954 accuracy 0.7885636912821581\n",
            "Val   loss 0.49800280882762027 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 145/300\n",
            "----------\n",
            "Train loss 0.4389504276608166 accuracy 0.7879387563795438\n",
            "Val   loss 0.4980027217131395 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 146/300\n",
            "----------\n",
            "Train loss 0.4388350867911389 accuracy 0.7880429121966461\n",
            "Val   loss 0.498002643768604 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 147/300\n",
            "----------\n",
            "Train loss 0.4389715057454611 accuracy 0.7871055098427246\n",
            "Val   loss 0.49800254977666414 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 148/300\n",
            "----------\n",
            "Train loss 0.43891400765431554 accuracy 0.7880429121966461\n",
            "Val   loss 0.49800246495466965 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 149/300\n",
            "----------\n",
            "Train loss 0.43902630711856644 accuracy 0.7878346005624414\n",
            "Val   loss 0.49800237784018886 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 150/300\n",
            "----------\n",
            "Train loss 0.4387824684381485 accuracy 0.7880429121966461\n",
            "Val   loss 0.498002290725708 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 151/300\n",
            "----------\n",
            "Train loss 0.4389001072237366 accuracy 0.7876262889282366\n",
            "Val   loss 0.4980021898563092 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 152/300\n",
            "----------\n",
            "Train loss 0.43887891189048167 accuracy 0.7881470680137486\n",
            "Val   loss 0.49800210961928737 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 153/300\n",
            "----------\n",
            "Train loss 0.43883824348449707 accuracy 0.7882512238308509\n",
            "Val   loss 0.498002079816965 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 154/300\n",
            "----------\n",
            "Train loss 0.4389887075675161 accuracy 0.7881470680137486\n",
            "Val   loss 0.4980019835325388 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 155/300\n",
            "----------\n",
            "Train loss 0.4389600173423165 accuracy 0.7881470680137486\n",
            "Val   loss 0.4980018872481126 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 156/300\n",
            "----------\n",
            "Train loss 0.43892342126683187 accuracy 0.7875221331111343\n",
            "Val   loss 0.4980017657463367 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 157/300\n",
            "----------\n",
            "Train loss 0.4390047757249129 accuracy 0.7883553796479533\n",
            "Val   loss 0.49800166258445155 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 158/300\n",
            "----------\n",
            "Train loss 0.4389105882299574 accuracy 0.7884595354650556\n",
            "Val   loss 0.49800161444223845 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 159/300\n",
            "----------\n",
            "Train loss 0.4390613142597048 accuracy 0.7883553796479533\n",
            "Val   loss 0.49800149294046253 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 160/300\n",
            "----------\n",
            "Train loss 0.43902574910929326 accuracy 0.7872096656598271\n",
            "Val   loss 0.49800137831614566 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 161/300\n",
            "----------\n",
            "Train loss 0.4388476480778895 accuracy 0.7876262889282366\n",
            "Val   loss 0.49800132100398725 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 162/300\n",
            "----------\n",
            "Train loss 0.4388784621106951 accuracy 0.7882512238308509\n",
            "Val   loss 0.498001197209725 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 163/300\n",
            "----------\n",
            "Train loss 0.4389721586516029 accuracy 0.7879387563795438\n",
            "Val   loss 0.4980010986328125 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 164/300\n",
            "----------\n",
            "Train loss 0.43889514474492325 accuracy 0.7871055098427246\n",
            "Val   loss 0.4980009885934683 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 165/300\n",
            "----------\n",
            "Train loss 0.4390994933874984 accuracy 0.7879387563795438\n",
            "Val   loss 0.4980008602142334 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 166/300\n",
            "----------\n",
            "Train loss 0.43918145761678096 accuracy 0.7880429121966461\n",
            "Val   loss 0.4980007685147799 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 167/300\n",
            "----------\n",
            "Train loss 0.4391203867761712 accuracy 0.7883553796479533\n",
            "Val   loss 0.49800070891013515 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 168/300\n",
            "----------\n",
            "Train loss 0.43909476697444916 accuracy 0.7873138214769294\n",
            "Val   loss 0.49800066306040836 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 169/300\n",
            "----------\n",
            "Train loss 0.4389834247137371 accuracy 0.7873138214769294\n",
            "Val   loss 0.49800056448349583 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 170/300\n",
            "----------\n",
            "Train loss 0.4389288072523318 accuracy 0.7879387563795438\n",
            "Val   loss 0.49800049570890575 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 171/300\n",
            "----------\n",
            "Train loss 0.43912094478544433 accuracy 0.7880429121966461\n",
            "Val   loss 0.49800041088691127 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 172/300\n",
            "----------\n",
            "Train loss 0.439016902525174 accuracy 0.7875221331111343\n",
            "Val   loss 0.49800031918745774 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 173/300\n",
            "----------\n",
            "Train loss 0.4389830835555729 accuracy 0.7875221331111343\n",
            "Val   loss 0.49800024124292225 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 174/300\n",
            "----------\n",
            "Train loss 0.4388313834604464 accuracy 0.7883553796479533\n",
            "Val   loss 0.49800010598622835 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 175/300\n",
            "----------\n",
            "Train loss 0.4389402172283122 accuracy 0.787730444745339\n",
            "Val   loss 0.49799997760699344 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 176/300\n",
            "----------\n",
            "Train loss 0.43897094342269394 accuracy 0.7879387563795438\n",
            "Val   loss 0.497999906539917 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 177/300\n",
            "----------\n",
            "Train loss 0.4389965620480086 accuracy 0.787730444745339\n",
            "Val   loss 0.49799978503814113 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 178/300\n",
            "----------\n",
            "Train loss 0.4391035492482938 accuracy 0.7875221331111343\n",
            "Val   loss 0.4979996933386876 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 179/300\n",
            "----------\n",
            "Train loss 0.43887959185399505 accuracy 0.7880429121966461\n",
            "Val   loss 0.49799962456409747 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 180/300\n",
            "----------\n",
            "Train loss 0.4390233504144769 accuracy 0.7880429121966461\n",
            "Val   loss 0.49799957412939805 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 181/300\n",
            "----------\n",
            "Train loss 0.4389146507570618 accuracy 0.7872096656598271\n",
            "Val   loss 0.4979994824299446 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 182/300\n",
            "----------\n",
            "Train loss 0.4388345644662255 accuracy 0.7878346005624414\n",
            "Val   loss 0.49799938385303205 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 183/300\n",
            "----------\n",
            "Train loss 0.4387637278751323 accuracy 0.7879387563795438\n",
            "Val   loss 0.4979993082009829 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 184/300\n",
            "----------\n",
            "Train loss 0.4388703664666728 accuracy 0.7887720029163628\n",
            "Val   loss 0.49799920274661136 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 185/300\n",
            "----------\n",
            "Train loss 0.43881091867622574 accuracy 0.7886678470992605\n",
            "Val   loss 0.4979991179246169 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 186/300\n",
            "----------\n",
            "Train loss 0.4389537690501464 accuracy 0.7875221331111343\n",
            "Val   loss 0.49799901705521804 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 187/300\n",
            "----------\n",
            "Train loss 0.4389904435527952 accuracy 0.7879387563795438\n",
            "Val   loss 0.49799899183786833 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 188/300\n",
            "----------\n",
            "Train loss 0.43891497505338567 accuracy 0.7874179772940318\n",
            "Val   loss 0.4979988863834968 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 189/300\n",
            "----------\n",
            "Train loss 0.4388110657271586 accuracy 0.7886678470992605\n",
            "Val   loss 0.4979987832216116 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 190/300\n",
            "----------\n",
            "Train loss 0.4390574340757571 accuracy 0.7878346005624414\n",
            "Val   loss 0.49799863879497236 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 191/300\n",
            "----------\n",
            "Train loss 0.4388486029286134 accuracy 0.787730444745339\n",
            "Val   loss 0.49799859065275925 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 192/300\n",
            "----------\n",
            "Train loss 0.43890237180810227 accuracy 0.78908447036767\n",
            "Val   loss 0.49799851729319644 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 193/300\n",
            "----------\n",
            "Train loss 0.4390467820983184 accuracy 0.7873138214769294\n",
            "Val   loss 0.497998466858497 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 194/300\n",
            "----------\n",
            "Train loss 0.4388815101824309 accuracy 0.787730444745339\n",
            "Val   loss 0.49799834994169384 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 195/300\n",
            "----------\n",
            "Train loss 0.43883632672460454 accuracy 0.7878346005624414\n",
            "Val   loss 0.4979982880445627 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 196/300\n",
            "----------\n",
            "Train loss 0.4387759872173008 accuracy 0.7881470680137486\n",
            "Val   loss 0.4979982330248906 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 197/300\n",
            "----------\n",
            "Train loss 0.4388932966087994 accuracy 0.7885636912821581\n",
            "Val   loss 0.497998100060683 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 198/300\n",
            "----------\n",
            "Train loss 0.43892102492483037 accuracy 0.7878346005624414\n",
            "Val   loss 0.4979980427485246 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 199/300\n",
            "----------\n",
            "Train loss 0.43875595927238464 accuracy 0.7875221331111343\n",
            "Val   loss 0.4979979464640984 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 200/300\n",
            "----------\n",
            "Train loss 0.4391121699621803 accuracy 0.7873138214769294\n",
            "Val   loss 0.4979978364247542 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 201/300\n",
            "----------\n",
            "Train loss 0.43883973791411046 accuracy 0.7882512238308509\n",
            "Val   loss 0.497997705753033 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 202/300\n",
            "----------\n",
            "Train loss 0.4388760587102489 accuracy 0.7884595354650556\n",
            "Val   loss 0.49799761176109314 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 203/300\n",
            "----------\n",
            "Train loss 0.43878965118997976 accuracy 0.7880429121966461\n",
            "Val   loss 0.4979975384015303 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 204/300\n",
            "----------\n",
            "Train loss 0.43895143113638224 accuracy 0.7875221331111343\n",
            "Val   loss 0.49799747879688555 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 205/300\n",
            "----------\n",
            "Train loss 0.4387925620141782 accuracy 0.7875221331111343\n",
            "Val   loss 0.4979974008523501 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 206/300\n",
            "----------\n",
            "Train loss 0.4388491895638014 accuracy 0.7874179772940318\n",
            "Val   loss 0.49799733895521897 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 207/300\n",
            "----------\n",
            "Train loss 0.4389691380293746 accuracy 0.787730444745339\n",
            "Val   loss 0.4979972197459294 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 208/300\n",
            "----------\n",
            "Train loss 0.4387630596756935 accuracy 0.7885636912821581\n",
            "Val   loss 0.4979972243309021 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 209/300\n",
            "----------\n",
            "Train loss 0.4388208334383212 accuracy 0.7884595354650556\n",
            "Val   loss 0.49799716701874364 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 210/300\n",
            "----------\n",
            "Train loss 0.43897339270303126 accuracy 0.7873138214769294\n",
            "Val   loss 0.4979970180071317 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 211/300\n",
            "----------\n",
            "Train loss 0.43857142054720927 accuracy 0.787730444745339\n",
            "Val   loss 0.4979969354776236 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 212/300\n",
            "----------\n",
            "Train loss 0.4388965183967038 accuracy 0.7874179772940318\n",
            "Val   loss 0.49799688504292416 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 213/300\n",
            "----------\n",
            "Train loss 0.43874955412588623 accuracy 0.7876262889282366\n",
            "Val   loss 0.497996802513416 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 214/300\n",
            "----------\n",
            "Train loss 0.4389433401979898 accuracy 0.7873138214769294\n",
            "Val   loss 0.4979966855966128 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 215/300\n",
            "----------\n",
            "Train loss 0.43867867753693934 accuracy 0.7876262889282366\n",
            "Val   loss 0.4979966076520773 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 216/300\n",
            "----------\n",
            "Train loss 0.4388566738680789 accuracy 0.7883553796479533\n",
            "Val   loss 0.4979965320000282 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 217/300\n",
            "----------\n",
            "Train loss 0.4388650847893012 accuracy 0.7885636912821581\n",
            "Val   loss 0.49799643342311567 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 218/300\n",
            "----------\n",
            "Train loss 0.43873353929896103 accuracy 0.7879387563795438\n",
            "Val   loss 0.4979963486011212 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 219/300\n",
            "----------\n",
            "Train loss 0.43891587343655136 accuracy 0.7882512238308509\n",
            "Val   loss 0.49799620646696824 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 220/300\n",
            "----------\n",
            "Train loss 0.43886681057904897 accuracy 0.7878346005624414\n",
            "Val   loss 0.49799611476751476 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 221/300\n",
            "----------\n",
            "Train loss 0.43880929248897654 accuracy 0.7874179772940318\n",
            "Val   loss 0.49799597492584813 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 222/300\n",
            "----------\n",
            "Train loss 0.4389566673259986 accuracy 0.7884595354650556\n",
            "Val   loss 0.4979958488390996 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 223/300\n",
            "----------\n",
            "Train loss 0.43864568479751287 accuracy 0.7885636912821581\n",
            "Val   loss 0.4979957754795368 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 224/300\n",
            "----------\n",
            "Train loss 0.43890628728427383 accuracy 0.7879387563795438\n",
            "Val   loss 0.49799569982748765 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 225/300\n",
            "----------\n",
            "Train loss 0.4387597037773383 accuracy 0.7878346005624414\n",
            "Val   loss 0.4979956241754385 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 226/300\n",
            "----------\n",
            "Train loss 0.4385729416420585 accuracy 0.7886678470992605\n",
            "Val   loss 0.49799556686328006 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 227/300\n",
            "----------\n",
            "Train loss 0.4389055955566858 accuracy 0.7878346005624414\n",
            "Val   loss 0.4979954270216135 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 228/300\n",
            "----------\n",
            "Train loss 0.4387124792525643 accuracy 0.7880429121966461\n",
            "Val   loss 0.497995342199619 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 229/300\n",
            "----------\n",
            "Train loss 0.4386653849168828 accuracy 0.7874179772940318\n",
            "Val   loss 0.49799530093486494 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 230/300\n",
            "----------\n",
            "Train loss 0.43892739360269745 accuracy 0.7884595354650556\n",
            "Val   loss 0.4979952115278978 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 231/300\n",
            "----------\n",
            "Train loss 0.4389593495350135 accuracy 0.7878346005624414\n",
            "Val   loss 0.4979951404608213 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 232/300\n",
            "----------\n",
            "Train loss 0.43874738483052506 accuracy 0.7875221331111343\n",
            "Val   loss 0.4979950372989361 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 233/300\n",
            "----------\n",
            "Train loss 0.43884739946377904 accuracy 0.7881470680137486\n",
            "Val   loss 0.4979949639393733 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 234/300\n",
            "----------\n",
            "Train loss 0.43884828725927755 accuracy 0.7875221331111343\n",
            "Val   loss 0.4979948722399198 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 235/300\n",
            "----------\n",
            "Train loss 0.4391353789128755 accuracy 0.787730444745339\n",
            "Val   loss 0.49799472322830785 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 236/300\n",
            "----------\n",
            "Train loss 0.438681760900899 accuracy 0.7880429121966461\n",
            "Val   loss 0.4979945902641003 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 237/300\n",
            "----------\n",
            "Train loss 0.43904955724352285 accuracy 0.7879387563795438\n",
            "Val   loss 0.4979944916871878 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 238/300\n",
            "----------\n",
            "Train loss 0.438924303180293 accuracy 0.7874179772940318\n",
            "Val   loss 0.497994411450166 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 239/300\n",
            "----------\n",
            "Train loss 0.4385225208182084 accuracy 0.7874179772940318\n",
            "Val   loss 0.4979943014108218 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 240/300\n",
            "----------\n",
            "Train loss 0.4384736959871493 accuracy 0.7881470680137486\n",
            "Val   loss 0.49799420283390927 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 241/300\n",
            "----------\n",
            "Train loss 0.4388013540914184 accuracy 0.7874179772940318\n",
            "Val   loss 0.49799416156915516 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 242/300\n",
            "----------\n",
            "Train loss 0.43894000665137645 accuracy 0.7874179772940318\n",
            "Val   loss 0.49799410654948306 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 243/300\n",
            "----------\n",
            "Train loss 0.43869175448229436 accuracy 0.7876262889282366\n",
            "Val   loss 0.4979939323205214 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 244/300\n",
            "----------\n",
            "Train loss 0.43892275502807215 accuracy 0.787730444745339\n",
            "Val   loss 0.49799386125344497 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 245/300\n",
            "----------\n",
            "Train loss 0.4388113025771944 accuracy 0.7880429121966461\n",
            "Val   loss 0.4979938062337729 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 246/300\n",
            "----------\n",
            "Train loss 0.43863535280290405 accuracy 0.7874179772940318\n",
            "Val   loss 0.4979937237042647 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 247/300\n",
            "----------\n",
            "Train loss 0.43897999274103267 accuracy 0.7886678470992605\n",
            "Val   loss 0.4979936205423795 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 248/300\n",
            "----------\n",
            "Train loss 0.4387829209628858 accuracy 0.7875221331111343\n",
            "Val   loss 0.4979935127955217 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 249/300\n",
            "----------\n",
            "Train loss 0.43898074681821625 accuracy 0.7880429121966461\n",
            "Val   loss 0.4979934600683359 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 250/300\n",
            "----------\n",
            "Train loss 0.43869402181161077 accuracy 0.7884595354650556\n",
            "Val   loss 0.4979933500289917 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 251/300\n",
            "----------\n",
            "Train loss 0.4388199350551555 accuracy 0.7875221331111343\n",
            "Val   loss 0.49799328125440157 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 252/300\n",
            "----------\n",
            "Train loss 0.438943392352054 accuracy 0.7872096656598271\n",
            "Val   loss 0.49799317809251636 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 253/300\n",
            "----------\n",
            "Train loss 0.4387760856433919 accuracy 0.7881470680137486\n",
            "Val   loss 0.4979931001479809 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 254/300\n",
            "----------\n",
            "Train loss 0.4387848243901604 accuracy 0.787730444745339\n",
            "Val   loss 0.49799304283582246 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 255/300\n",
            "----------\n",
            "Train loss 0.43879115071735886 accuracy 0.7883553796479533\n",
            "Val   loss 0.49799296259880066 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 256/300\n",
            "----------\n",
            "Train loss 0.43881706187599584 accuracy 0.7870013540256223\n",
            "Val   loss 0.49799293738145095 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 257/300\n",
            "----------\n",
            "Train loss 0.4386992972148092 accuracy 0.7895010936360796\n",
            "Val   loss 0.4979928686068608 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 258/300\n",
            "----------\n",
            "Train loss 0.43874632763235194 accuracy 0.7883553796479533\n",
            "Val   loss 0.4979927448125986 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 259/300\n",
            "----------\n",
            "Train loss 0.43875062151959066 accuracy 0.7875221331111343\n",
            "Val   loss 0.4979926554056314 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 260/300\n",
            "----------\n",
            "Train loss 0.4387748064963441 accuracy 0.7878346005624414\n",
            "Val   loss 0.49799256599866426 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 261/300\n",
            "----------\n",
            "Train loss 0.43867374289976924 accuracy 0.7874179772940318\n",
            "Val   loss 0.4979924811766698 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 262/300\n",
            "----------\n",
            "Train loss 0.43872414176401336 accuracy 0.7874179772940318\n",
            "Val   loss 0.4979923917697026 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 263/300\n",
            "----------\n",
            "Train loss 0.43871499166676875 accuracy 0.7875221331111343\n",
            "Val   loss 0.49799230007024914 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 264/300\n",
            "----------\n",
            "Train loss 0.4386339387611339 accuracy 0.7882512238308509\n",
            "Val   loss 0.4979922198332273 accuracy 0.7450731087094724\n",
            "\n",
            "Epoch 265/300\n",
            "----------\n",
            "Train loss 0.43851433459081146 accuracy 0.7882512238308509\n",
            "Val   loss 0.4979921052089104 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 266/300\n",
            "----------\n",
            "Train loss 0.4387513426573653 accuracy 0.7878346005624414\n",
            "Val   loss 0.4979919928770799 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 267/300\n",
            "----------\n",
            "Train loss 0.4387170433213836 accuracy 0.7878346005624414\n",
            "Val   loss 0.4979918965926537 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 268/300\n",
            "----------\n",
            "Train loss 0.43878074186412913 accuracy 0.7881470680137486\n",
            "Val   loss 0.4979917865533095 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 269/300\n",
            "----------\n",
            "Train loss 0.439070136923539 accuracy 0.7881470680137486\n",
            "Val   loss 0.4979917246561784 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 270/300\n",
            "----------\n",
            "Train loss 0.43856317706798253 accuracy 0.787730444745339\n",
            "Val   loss 0.4979916192018069 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 271/300\n",
            "----------\n",
            "Train loss 0.43875382291643245 accuracy 0.7888761587334653\n",
            "Val   loss 0.49799153437981236 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 272/300\n",
            "----------\n",
            "Train loss 0.4385550339755259 accuracy 0.7880429121966461\n",
            "Val   loss 0.4979914426803589 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 273/300\n",
            "----------\n",
            "Train loss 0.4387151732256538 accuracy 0.7889803145505676\n",
            "Val   loss 0.4979913303485283 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 274/300\n",
            "----------\n",
            "Train loss 0.4388990386536247 accuracy 0.7870013540256223\n",
            "Val   loss 0.49799124323404753 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 275/300\n",
            "----------\n",
            "Train loss 0.43870986958867625 accuracy 0.7885636912821581\n",
            "Val   loss 0.4979911469496213 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 276/300\n",
            "----------\n",
            "Train loss 0.4386813722943005 accuracy 0.7883553796479533\n",
            "Val   loss 0.49799105754265416 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 277/300\n",
            "----------\n",
            "Train loss 0.4386315612416518 accuracy 0.7883553796479533\n",
            "Val   loss 0.497990940625851 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 278/300\n",
            "----------\n",
            "Train loss 0.4389398725409257 accuracy 0.7875221331111343\n",
            "Val   loss 0.49799082370904774 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 279/300\n",
            "----------\n",
            "Train loss 0.4387660089292024 accuracy 0.7875221331111343\n",
            "Val   loss 0.49799073659456694 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 280/300\n",
            "----------\n",
            "Train loss 0.43878806460844844 accuracy 0.7878346005624414\n",
            "Val   loss 0.4979906059228457 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 281/300\n",
            "----------\n",
            "Train loss 0.43876547562448603 accuracy 0.7881470680137486\n",
            "Val   loss 0.49799058070549596 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 282/300\n",
            "----------\n",
            "Train loss 0.4385520604095961 accuracy 0.7882512238308509\n",
            "Val   loss 0.4979905119309059 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 283/300\n",
            "----------\n",
            "Train loss 0.43869540409037944 accuracy 0.7879387563795438\n",
            "Val   loss 0.497990424816425 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 284/300\n",
            "----------\n",
            "Train loss 0.43874960941703695 accuracy 0.7876262889282366\n",
            "Val   loss 0.4979902941447038 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 285/300\n",
            "----------\n",
            "Train loss 0.4387529751187877 accuracy 0.7886678470992605\n",
            "Val   loss 0.4979902184926547 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 286/300\n",
            "----------\n",
            "Train loss 0.4387454021918146 accuracy 0.7880429121966461\n",
            "Val   loss 0.49799015659552354 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 287/300\n",
            "----------\n",
            "Train loss 0.43866552961500066 accuracy 0.7874179772940318\n",
            "Val   loss 0.4979900282162886 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 288/300\n",
            "----------\n",
            "Train loss 0.43861087725350734 accuracy 0.7883553796479533\n",
            "Val   loss 0.49798990671451276 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 289/300\n",
            "----------\n",
            "Train loss 0.4388945937940949 accuracy 0.787730444745339\n",
            "Val   loss 0.49798985169484067 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 290/300\n",
            "----------\n",
            "Train loss 0.43856076621695567 accuracy 0.7879387563795438\n",
            "Val   loss 0.4979897324855511 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 291/300\n",
            "----------\n",
            "Train loss 0.4386983419719495 accuracy 0.7874179772940318\n",
            "Val   loss 0.49798966829593366 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 292/300\n",
            "----------\n",
            "Train loss 0.43863139811315033 accuracy 0.7879387563795438\n",
            "Val   loss 0.49798954220918507 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 293/300\n",
            "----------\n",
            "Train loss 0.43877032908954117 accuracy 0.7876262889282366\n",
            "Val   loss 0.49798945967967695 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 294/300\n",
            "----------\n",
            "Train loss 0.4386407242793786 accuracy 0.7886678470992605\n",
            "Val   loss 0.49798938632011414 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 295/300\n",
            "----------\n",
            "Train loss 0.4385303952976277 accuracy 0.78908447036767\n",
            "Val   loss 0.49798936339525074 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 296/300\n",
            "----------\n",
            "Train loss 0.43866866630943196 accuracy 0.7880429121966461\n",
            "Val   loss 0.4979892694033109 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 297/300\n",
            "----------\n",
            "Train loss 0.43873339891433716 accuracy 0.7867930423914176\n",
            "Val   loss 0.49798915019402135 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 298/300\n",
            "----------\n",
            "Train loss 0.4387797230952664 accuracy 0.7868971982085199\n",
            "Val   loss 0.4979890722494859 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 299/300\n",
            "----------\n",
            "Train loss 0.43875895911141444 accuracy 0.7873138214769294\n",
            "Val   loss 0.49798901264484113 accuracy 0.7444373808010172\n",
            "\n",
            "Epoch 300/300\n",
            "----------\n",
            "Train loss 0.43858457435118525 accuracy 0.7874179772940318\n",
            "Val   loss 0.4979888957280379 accuracy 0.7444373808010172\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFmTSaxYMw-"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/KimCNN/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "wTzhGWp12XdE",
        "outputId": "020a8459-4fc5-4dfd-8457-8879a713726a"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVh1s2cLCahH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b5d442c-e1f8-48c4-a267-e578711e5654"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        x=input_ids,\n",
        "      )\n",
        "      preds = torch.round(outputs)\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Informative', 'Informative'], digits = 4))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not Informative     0.7091    0.4643    0.5612       504\n",
            "    Informative     0.7757    0.9068    0.8362      1030\n",
            "\n",
            "       accuracy                         0.7614      1534\n",
            "      macro avg     0.7424    0.6855    0.6987      1534\n",
            "   weighted avg     0.7538    0.7614    0.7458      1534\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}