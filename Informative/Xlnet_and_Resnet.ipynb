{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Xlnet_and_Resnet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "215d6137-9318-4bc6-b409-e516c7172f8a"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzejvNI6kk3A",
        "outputId": "ed6a98e4-6b5e-4670-e38c-2d30d5ac82cd"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3 sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Collecting textwrap3\n",
            "  Downloading https://files.pythonhosted.org/packages/77/9c/a53e561d496ee5866bbeea4d3a850b3b545ed854f8a21007c1e0d872e94d/textwrap3-0.9.2-py2.py3-none-any.whl\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 36.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 44.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (54.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=80c67144c25f027e1ae5f50182af749e00088a0f1bcc0d6a9b6ade86103b3f58\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, textwrap3, sentencepiece\n",
            "Successfully installed sacremoses-0.0.44 sentencepiece-0.1.95 textwrap3-0.9.2 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "f3406466-b812-478a-9aed-be282765a286"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification, BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'xlnet-base-cased'\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"informative\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/FYP/task_informative_text_img_agreed_lab_train.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'tweet_text', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['tweet_text'] = df_train['tweet_text'].apply(clean_text)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/FYP/task_informative_text_img_agreed_lab_dev.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'tweet_text', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['tweet_text'] = df_val['tweet_text'].apply(clean_text)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/FYP/task_informative_text_img_agreed_lab_test.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'tweet_text', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(clean_text)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/FYP/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, targets, paths, tokenizer, max_len):\n",
        "    self.tweets = tweets\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.paths = paths\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    tweet = str(self.tweets[item])\n",
        "    target = self.targets[item]\n",
        "    path = str(self.paths[item])\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "      truncation = True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'tweet_text': tweet,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long),\n",
        "      'tweet_image': img\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    tweets=df.tweet_text.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "    paths=df.image.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.bert = XLNetModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    for param in self.bert.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.resnet = torchvision.models.resnet18(pretrained=True)\n",
        "    for param in self.resnet.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(self.bert.config.hidden_size + 1000)\n",
        "\n",
        "    self.linear1 = nn.Linear(self.bert.config.hidden_size + 1000, 1000)\n",
        "    self.relu1    = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(p=0.4)\n",
        "\n",
        "    self.linear2 = nn.Linear(1000, 500)\n",
        "    self.relu2    = nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "    self.linear3 = nn.Linear(500, 250)\n",
        "    self.relu3    = nn.ReLU()\n",
        "    self.dropout3 = nn.Dropout(p=0.1)\n",
        "\n",
        "    self.linear4 = nn.Linear(250, 125)\n",
        "    self.relu4    = nn.ReLU()\n",
        "    self.dropout4 = nn.Dropout(p=0.02)\n",
        "\n",
        "    self.linear5 = nn.Linear(125, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask, tweet_img):\n",
        "    text_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False\n",
        "    )\n",
        "    image_output = self.resnet(tweet_img)\n",
        "    merged_output = torch.cat((torch.mean(text_output[0], 1), image_output), dim=1)\n",
        "    bn_output = self.bn(merged_output)\n",
        "\n",
        "    linear1_output = self.linear1(bn_output)\n",
        "    relu1_output = self.relu1(linear1_output)\n",
        "    dropout1_output = self.dropout1(relu1_output)\n",
        "\n",
        "    linear2_output = self.linear2(dropout1_output)\n",
        "    relu2_output = self.relu2(linear2_output)\n",
        "    dropout2_output = self.dropout2(relu2_output)\n",
        "\n",
        "    linear3_output = self.linear3(dropout2_output)\n",
        "    relu3_output = self.relu3(linear3_output)\n",
        "    dropout3_output = self.dropout3(relu3_output)\n",
        "\n",
        "    linear4_output = self.linear4(dropout3_output)\n",
        "    relu4_output = self.relu4(linear4_output)\n",
        "    dropout4_output = self.dropout4(relu4_output)\n",
        "\n",
        "    linear5_output = self.linear5(dropout4_output)\n",
        "\n",
        "\n",
        "    probas = self.sigmoid(linear5_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "MAX_LEN = 150\n",
        "\n",
        "tokenizer = XLNetTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.BCELoss().to(device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-eqL1yqUP5a",
        "outputId": "f0f92ca6-2bd4-4a0c-d600-abec45529423"
      },
      "source": [
        "checkpoint = torch.load(\"./gdrive/MyDrive/FYP/XlnetResNet-checkpoint.t7\")\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "start_epoch = checkpoint['epoch']\n",
        "best_accuracy = checkpoint['best_accuracy']\n",
        "\n",
        "print(start_epoch)\n",
        "print(best_accuracy)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40\n",
            "tensor(0.8493, device='cuda:0', dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "9973efed-59ec-4105-d3c6-93ddc85aaea9"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/FYP/BertResNet-checkpoint.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "# best_accuracy = checkpoint['best_accuracy']\n",
        "# print(start_epoch)\n",
        "# print(best_accuracy)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {start_epoch + epoch + 1}/{start_epoch + EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/FYP/XlnetResNet-checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6494963765144348 accuracy 0.5949380272888241\n",
            "Val   loss 0.52590411901474 accuracy 0.7069294342021615\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5012179456259075 accuracy 0.7638787626288928\n",
            "Val   loss 0.4709901064634323 accuracy 0.7991099809281628\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4144526751417863 accuracy 0.8229351109259452\n",
            "Val   loss 0.41846640408039093 accuracy 0.8124602670057216\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.36477417538040563 accuracy 0.8373086136860743\n",
            "Val   loss 0.42301981151103973 accuracy 0.821360457724094\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.33993052965716314 accuracy 0.855327570044787\n",
            "Val   loss 0.43029023706912994 accuracy 0.8226319135410045\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3298443273494118 accuracy 0.860222893448599\n",
            "Val   loss 0.4247180446982384 accuracy 0.821360457724094\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.31463960597389623 accuracy 0.8672013331944589\n",
            "Val   loss 0.40788542479276657 accuracy 0.8302606484424666\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.31025151516261856 accuracy 0.8658473075721279\n",
            "Val   loss 0.40502508729696274 accuracy 0.8366179275270185\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3036999561284718 accuracy 0.8687636704509947\n",
            "Val   loss 0.39875128120183945 accuracy 0.8372536554354736\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.29909672078333405 accuracy 0.8705343193417352\n",
            "Val   loss 0.3981952369213104 accuracy 0.8385251112523839\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.29179458869130986 accuracy 0.874804707842933\n",
            "Val   loss 0.40281815081834793 accuracy 0.8334392879847425\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.28565599886994614 accuracy 0.8799083428809499\n",
            "Val   loss 0.3966451585292816 accuracy 0.8429752066115702\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2831450198826037 accuracy 0.8787626288928236\n",
            "Val   loss 0.39020347595214844 accuracy 0.8442466624284807\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.27685616832030446 accuracy 0.8839704197479429\n",
            "Val   loss 0.3862042650580406 accuracy 0.8417037507946599\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.27627319881790563 accuracy 0.8837621081137381\n",
            "Val   loss 0.38784849643707275 accuracy 0.8423394787031151\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.27066039568499517 accuracy 0.8839704197479429\n",
            "Val   loss 0.3832852989435196 accuracy 0.8423394787031151\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2726866555841346 accuracy 0.8844911988334548\n",
            "Val   loss 0.38558994978666306 accuracy 0.8429752066115702\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.26906082504674006 accuracy 0.8883449640662431\n",
            "Val   loss 0.38092635571956635 accuracy 0.8436109345200254\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.26692169198864385 accuracy 0.8862618477241954\n",
            "Val   loss 0.36913179606199265 accuracy 0.8474253019707565\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.26113777411611455 accuracy 0.8900114571398813\n",
            "Val   loss 0.3723835200071335 accuracy 0.8480610298792117\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2600213963734476 accuracy 0.8913654827622123\n",
            "Val   loss 0.3783477619290352 accuracy 0.845518118245391\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.25438765789333145 accuracy 0.8914696385793146\n",
            "Val   loss 0.38107891380786896 accuracy 0.8461538461538461\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2546902863602889 accuracy 0.8951150921778981\n",
            "Val   loss 0.38410665839910507 accuracy 0.845518118245391\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.25082042185883774 accuracy 0.8970940527028434\n",
            "Val   loss 0.37783437222242355 accuracy 0.8429752066115702\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2539988764022526 accuracy 0.894802624726591\n",
            "Val   loss 0.38550063967704773 accuracy 0.8436109345200254\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2426950962919938 accuracy 0.9017810644724508\n",
            "Val   loss 0.38123663514852524 accuracy 0.8436109345200254\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.25346447448981435 accuracy 0.8928236642016456\n",
            "Val   loss 0.3798246830701828 accuracy 0.8436109345200254\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2518510692997983 accuracy 0.8963649619831268\n",
            "Val   loss 0.3791632577776909 accuracy 0.8442466624284807\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2418305113127357 accuracy 0.9019893761066555\n",
            "Val   loss 0.375741183757782 accuracy 0.845518118245391\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24303071200847626 accuracy 0.896989896885741\n",
            "Val   loss 0.3774314969778061 accuracy 0.8442466624284807\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24398194253444672 accuracy 0.8981356108738673\n",
            "Val   loss 0.3814033642411232 accuracy 0.8436109345200254\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2379953445572602 accuracy 0.9006353504843245\n",
            "Val   loss 0.37807507812976837 accuracy 0.8436109345200254\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23944096031941867 accuracy 0.8978231434225601\n",
            "Val   loss 0.3775141313672066 accuracy 0.8429752066115702\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2424737960100174 accuracy 0.899697948130403\n",
            "Val   loss 0.3724463954567909 accuracy 0.8467895740623014\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24240495107675852 accuracy 0.9008436621185293\n",
            "Val   loss 0.3700687065720558 accuracy 0.8461538461538461\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23825732187220924 accuracy 0.9025101551921675\n",
            "Val   loss 0.3718583211302757 accuracy 0.845518118245391\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23534143676883296 accuracy 0.904280804082908\n",
            "Val   loss 0.3696829155087471 accuracy 0.8480610298792117\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24073973062791323 accuracy 0.901051973752734\n",
            "Val   loss 0.37100545316934586 accuracy 0.8461538461538461\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2348968731729608 accuracy 0.903760024997396\n",
            "Val   loss 0.369330994784832 accuracy 0.8486967577876668\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23158887420829974 accuracy 0.9025101551921675\n",
            "Val   loss 0.3700249716639519 accuracy 0.8493324856961221\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2379052239029031 accuracy 0.9014685970211436\n",
            "Val   loss 0.37164975702762604 accuracy 0.8467895740623014\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2290939814166019 accuracy 0.9062597646078533\n",
            "Val   loss 0.37019094824790955 accuracy 0.8474253019707565\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2329754374529186 accuracy 0.905634829705239\n",
            "Val   loss 0.36855463683605194 accuracy 0.8486967577876668\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23104076244329153 accuracy 0.9062597646078533\n",
            "Val   loss 0.37172386795282364 accuracy 0.8467895740623014\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23116246022676168 accuracy 0.9053223622539318\n",
            "Val   loss 0.3710905984044075 accuracy 0.8474253019707565\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23557474424964503 accuracy 0.9021976877408603\n",
            "Val   loss 0.37185677886009216 accuracy 0.8461538461538461\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23333551930753807 accuracy 0.9030309342776793\n",
            "Val   loss 0.3727661147713661 accuracy 0.8474253019707565\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23245044680018173 accuracy 0.9070930111446724\n",
            "Val   loss 0.37109018862247467 accuracy 0.8474253019707565\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.22934416328605853 accuracy 0.9040724924487032\n",
            "Val   loss 0.3717152401804924 accuracy 0.8480610298792117\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2266901715805656 accuracy 0.9112592438287678\n",
            "Val   loss 0.3727584779262543 accuracy 0.8480610298792117\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFmTSaxYMw-"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/FYP/XlnetResNet-checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ScOj15BovCww",
        "outputId": "5fa837cd-916e-45d5-82b2-e1dd625f6e78"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTghsXN8vEpo",
        "outputId": "74eb1cea-ef35-449f-ad34-c340df4a5402"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "      preds = torch.round(outputs)\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Informative', 'Informative'], digits=4))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not Informative     0.8858    0.6310    0.7370       504\n",
            "    Informative     0.8417    0.9602    0.8971      1030\n",
            "\n",
            "       accuracy                         0.8520      1534\n",
            "      macro avg     0.8637    0.7956    0.8170      1534\n",
            "   weighted avg     0.8562    0.8520    0.8445      1534\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}