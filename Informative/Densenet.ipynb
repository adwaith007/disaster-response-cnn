{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Densenet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a0b094409e14ca4adc79bae31ff0f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_52937f07f81642f792fb479801b2719f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_02d022dcf4b84db7a3559f46bd39d94a",
              "IPY_MODEL_75665687294d44829719bc9963a13d4b"
            ]
          }
        },
        "52937f07f81642f792fb479801b2719f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02d022dcf4b84db7a3559f46bd39d94a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_435e41a7b5e84452801d3b440c62788f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 115730790,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 115730790,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d70f6ec72aee4addbf01217727235db8"
          }
        },
        "75665687294d44829719bc9963a13d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_67d40643e68f4ae994ea8a4688b8f9ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 110M/110M [00:03&lt;00:00, 34.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_139a0bc6dabf48e1982cdd95992fa942"
          }
        },
        "435e41a7b5e84452801d3b440c62788f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d70f6ec72aee4addbf01217727235db8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67d40643e68f4ae994ea8a4688b8f9ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "139a0bc6dabf48e1982cdd95992fa942": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "a65b2db4-e29c-4c38-bea7-0ef790b85be9"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzejvNI6kk3A"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3 sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwWQL0acqLuH",
        "outputId": "9e5cdfe6-477d-4c7b-b621-cc832f8efcd3"
      },
      "source": [
        "!ls gdrive/MyDrive/data_image"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "california_wildfires  hurricane_irma   iraq_iran_earthquake  srilanka_floods\n",
            "hurricane_harvey      hurricane_maria  mexico_earthquake\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "5ea1c9dc-1cdf-43ff-dc3a-fee51c0a0553"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"informative\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/ColabNotebooks/train.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/ColabNotebooks/val.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/ColabNotebooks/test.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, paths, targets):\n",
        "    self.paths = paths\n",
        "    self.targets = targets\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.paths)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    path = str(self.paths[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    return {\n",
        "      'tweet_image': img,\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    paths=df.image.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.densenet = torchvision.models.densenet161(pretrained=True)\n",
        "    # for param in self.resnext.parameters():\n",
        "    #   param.requires_grad = False\n",
        "\n",
        "    self.linear1 = nn.Linear(1000, 256)\n",
        "    self.relu    = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=0.4)\n",
        "    self.linear2 = nn.Linear(256, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, tweet_img):\n",
        "    output = self.densenet(tweet_img)\n",
        "    linear1_output = self.linear1(output)\n",
        "    relu_output = self.relu(linear1_output)\n",
        "    dropout_output = self.dropout(relu_output)\n",
        "    linear2_output = self.linear2(dropout_output)\n",
        "    probas = self.sigmoid(linear2_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "5a0b094409e14ca4adc79bae31ff0f5d",
            "52937f07f81642f792fb479801b2719f",
            "02d022dcf4b84db7a3559f46bd39d94a",
            "75665687294d44829719bc9963a13d4b",
            "435e41a7b5e84452801d3b440c62788f",
            "d70f6ec72aee4addbf01217727235db8",
            "67d40643e68f4ae994ea8a4688b8f9ba",
            "139a0bc6dabf48e1982cdd95992fa942"
          ]
        },
        "outputId": "fe5bb49d-68b5-4fd6-c3f9-3e3d42d48628"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 40\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.BCELoss().to(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/hub/checkpoints/densenet161-8d451a50.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a0b094409e14ca4adc79bae31ff0f5d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=115730790.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "e10c26ff-000c-4d8c-9b39-fcdf0f672079"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/Models/Densenet/checkpoint.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "\n",
        "# print(start_epoch)\n",
        "# print(checkpoint['best_accuracy'])\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/Models/Densenet/checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.41617253100951246 accuracy 0.8115821268617852\n",
            "Val   loss 0.3472689478844404 accuracy 0.8442466624284807\n",
            "\n",
            "Epoch 2/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20387751257597805 accuracy 0.9229246953442349\n",
            "Val   loss 0.41907860038802025 accuracy 0.8378893833439288\n",
            "\n",
            "Epoch 3/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0730616560134653 accuracy 0.9771898760545776\n",
            "Val   loss 0.6909712540553301 accuracy 0.8385251112523839\n",
            "\n",
            "Epoch 4/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.03553158806071194 accuracy 0.9920841579002186\n",
            "Val   loss 0.9672106402025383 accuracy 0.8493324856961221\n",
            "\n",
            "Epoch 5/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.02185920522583502 accuracy 0.996771169669826\n",
            "Val   loss 1.232481684647646 accuracy 0.8442466624284807\n",
            "\n",
            "Epoch 6/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.013059416491534612 accuracy 0.9980210394750546\n",
            "Val   loss 1.268951507034635 accuracy 0.8283534647171011\n",
            "\n",
            "Epoch 7/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.01508761555703926 accuracy 0.9991667534631808\n",
            "Val   loss 1.445082741809565 accuracy 0.8340750158931978\n",
            "\n",
            "Epoch 8/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.008601919459893465 accuracy 0.9992709092802833\n",
            "Val   loss 1.480407326624554 accuracy 0.831532104259377\n",
            "\n",
            "Epoch 9/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.008250696449778444 accuracy 0.9996875325486928\n",
            "Val   loss 1.4490821631572361 accuracy 0.8347107438016529\n",
            "\n",
            "Epoch 10/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.006367697667932577 accuracy 0.9995833767315905\n",
            "Val   loss 1.4864367904076972 accuracy 0.8429752066115702\n",
            "\n",
            "Epoch 11/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.002851119805428271 accuracy 0.9996875325486928\n",
            "Val   loss 1.5151419754625 accuracy 0.8417037507946599\n",
            "\n",
            "Epoch 12/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0029274226104519696 accuracy 0.9995833767315905\n",
            "Val   loss 1.773229590282408 accuracy 0.8385251112523839\n",
            "\n",
            "Epoch 13/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0010068592985291928 accuracy 0.9997916883657952\n",
            "Val   loss 1.869894684792207 accuracy 0.8461538461538461\n",
            "\n",
            "Epoch 14/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.001978937468705957 accuracy 0.9995833767315905\n",
            "Val   loss 1.7018951286418451 accuracy 0.831532104259377\n",
            "\n",
            "Epoch 15/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0006704809264493237 accuracy 0.9997916883657952\n",
            "Val   loss 1.8069837917241967 accuracy 0.8397965670692944\n",
            "\n",
            "Epoch 16/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0001198237267337777 accuracy 1.0\n",
            "Val   loss 1.834924671173403 accuracy 0.8378893833439288\n",
            "\n",
            "Epoch 17/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 5.493184267661309e-05 accuracy 1.0\n",
            "Val   loss 1.7867002542622321 accuracy 0.8410680228862047\n",
            "\n",
            "Epoch 18/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 9.278754272340575e-06 accuracy 1.0\n",
            "Val   loss 1.8612325644494285 accuracy 0.8397965670692944\n",
            "\n",
            "Epoch 19/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.019655871215698e-05 accuracy 1.0\n",
            "Val   loss 1.8697800939977856 accuracy 0.8417037507946599\n",
            "\n",
            "Epoch 20/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 5.660525088451111e-06 accuracy 1.0\n",
            "Val   loss 1.9332430681288588 accuracy 0.8391608391608392\n",
            "\n",
            "Epoch 21/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 4.2890487514853815e-06 accuracy 1.0\n",
            "Val   loss 1.943239358738083 accuracy 0.8391608391608392\n",
            "\n",
            "Epoch 22/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 3.460908392403545e-06 accuracy 1.0\n",
            "Val   loss 2.0541646554768214 accuracy 0.8391608391608392\n",
            "\n",
            "Epoch 23/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 4.09081238524247e-06 accuracy 1.0\n",
            "Val   loss 2.06327025017143 accuracy 0.8385251112523839\n",
            "\n",
            "Epoch 24/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.877036774068899e-06 accuracy 1.0\n",
            "Val   loss 2.071889476567512 accuracy 0.8385251112523839\n",
            "\n",
            "Epoch 25/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.7414601102685406e-06 accuracy 1.0\n",
            "Val   loss 2.1318598039448307 accuracy 0.8397965670692944\n",
            "\n",
            "Epoch 26/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.842847832111588e-06 accuracy 1.0\n",
            "Val   loss 2.1438022670894883 accuracy 0.8391608391608392\n",
            "\n",
            "Epoch 27/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.5211055636516502e-06 accuracy 1.0\n",
            "Val   loss 2.1484997668415335 accuracy 0.8391608391608392\n",
            "\n",
            "Epoch 28/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.7596600803654457e-06 accuracy 1.0\n",
            "Val   loss 2.1562110427171013 accuracy 0.8404322949777495\n",
            "\n",
            "Epoch 29/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.7368603789134303e-06 accuracy 1.0\n",
            "Val   loss 2.1620527833551186 accuracy 0.8397965670692944\n",
            "\n",
            "Epoch 30/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.513874034149095e-06 accuracy 1.0\n",
            "Val   loss 2.1688818191289916 accuracy 0.8397965670692944\n",
            "\n",
            "Epoch 31/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.4809179786993242e-06 accuracy 1.0\n",
            "Val   loss 2.17617452196777 accuracy 0.8404322949777495\n",
            "\n",
            "Epoch 32/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.1731036229658129e-06 accuracy 1.0\n",
            "Val   loss 2.183595735400916 accuracy 0.8404322949777495\n",
            "\n",
            "Epoch 33/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.2212574498201688e-06 accuracy 1.0\n",
            "Val   loss 2.1895072176009425 accuracy 0.8404322949777495\n",
            "\n",
            "Epoch 34/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.1573250629229305e-06 accuracy 1.0\n",
            "Val   loss 2.1951046699881562 accuracy 0.8410680228862047\n",
            "\n",
            "Epoch 35/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.1448530056400862e-06 accuracy 1.0\n",
            "Val   loss 2.2000188908278946 accuracy 0.8410680228862047\n",
            "\n",
            "Epoch 36/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 9.0604123083735e-07 accuracy 1.0\n",
            "Val   loss 2.204000748574734 accuracy 0.8410680228862047\n",
            "\n",
            "Epoch 37/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 9.380283492797114e-07 accuracy 1.0\n",
            "Val   loss 2.206867838084698 accuracy 0.8404322949777495\n",
            "\n",
            "Epoch 38/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 7.710998354713788e-07 accuracy 1.0\n",
            "Val   loss 2.2093753595203167 accuracy 0.8404322949777495\n",
            "\n",
            "Epoch 39/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 9.414178827944095e-07 accuracy 1.0\n",
            "Val   loss 2.211447841480375 accuracy 0.8404322949777495\n",
            "\n",
            "Epoch 40/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 8.992197222877406e-07 accuracy 1.0\n",
            "Val   loss 2.2116476963758473 accuracy 0.8404322949777495\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Taz58ppcm8n8"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/Densenet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScOj15BovCww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "73a1d9f1-8d02-482e-d3e3-53f6e8e63f92"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTghsXN8vEpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ea1ff7-cb92-4987-9be0-239571dfb067"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(tweet_img=tweet_imgs)\n",
        "      preds = torch.round(outputs)\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Informative', 'Informative']))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not Informative       0.79      0.75      0.77       504\n",
            "    Informative       0.88      0.90      0.89      1030\n",
            "\n",
            "       accuracy                           0.85      1534\n",
            "      macro avg       0.84      0.83      0.83      1534\n",
            "   weighted avg       0.85      0.85      0.85      1534\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}