{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Xlnet_and_Densenet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "856598119ec9437eba0c01463b64b342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_41bd78b816b646259ec82f63243957b0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7db985d32be44f7b9e4b21a1ed62ff22",
              "IPY_MODEL_22dee30fe2654056bacfd24b59b381da"
            ]
          }
        },
        "41bd78b816b646259ec82f63243957b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7db985d32be44f7b9e4b21a1ed62ff22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8e89facddf5147b38a259c9339695d78",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 798011,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 798011,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09d30fb6ddc2494b93040fa412081e74"
          }
        },
        "22dee30fe2654056bacfd24b59b381da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dcdf727838fb4f85808e92a6c54258c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 798k/798k [00:00&lt;00:00, 10.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_696e5a95f9e4407aad7251103dcd96cb"
          }
        },
        "8e89facddf5147b38a259c9339695d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09d30fb6ddc2494b93040fa412081e74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dcdf727838fb4f85808e92a6c54258c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "696e5a95f9e4407aad7251103dcd96cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2adeee52653f4d64b4ceb37bf20aeed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_988c9d94758148fb9c0ef8af7854988b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_afe48ffbe33d4756975e6a40104d8c0b",
              "IPY_MODEL_13cbc5f09b1746999291e34ce8c8d5bd"
            ]
          }
        },
        "988c9d94758148fb9c0ef8af7854988b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afe48ffbe33d4756975e6a40104d8c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_09282dae0b4547ad80d95f8be3673b58",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1382015,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1382015,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0bb9cbf9ae2747a48dd8f4cf86f3b017"
          }
        },
        "13cbc5f09b1746999291e34ce8c8d5bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd7b11173e904ba0890f5c45121e2d11",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.38M/1.38M [00:00&lt;00:00, 7.15MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_356e5108c67142fa98985e9ed375d254"
          }
        },
        "09282dae0b4547ad80d95f8be3673b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0bb9cbf9ae2747a48dd8f4cf86f3b017": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd7b11173e904ba0890f5c45121e2d11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "356e5108c67142fa98985e9ed375d254": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "53989a12310b44ac831b9a1b0427eec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_df5f2c4a4c174249b9d1acb43dcc4540",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_12d23823a9d645bc83248e7f538a6fe1",
              "IPY_MODEL_01ceee62427b4fa9acd86904624f4a56"
            ]
          }
        },
        "df5f2c4a4c174249b9d1acb43dcc4540": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12d23823a9d645bc83248e7f538a6fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b00a746e0c7a4eb6a527e7b18691c402",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 760,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 760,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ee13d4000734a4999206bbfb5dc4d1d"
          }
        },
        "01ceee62427b4fa9acd86904624f4a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4e0a0d044ca34b9c87969dab1a116419",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 760/760 [00:00&lt;00:00, 2.30kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b272b39bb4f042d8adffab58ed67bf5d"
          }
        },
        "b00a746e0c7a4eb6a527e7b18691c402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ee13d4000734a4999206bbfb5dc4d1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e0a0d044ca34b9c87969dab1a116419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b272b39bb4f042d8adffab58ed67bf5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "528f4afafa70401194d99cdb7b87fde9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e1259113696441438aa1937a34a41a59",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cf76d35591014022945650b6cd1a7c58",
              "IPY_MODEL_f10b1375da0e44e09b158a757d3e4a6f"
            ]
          }
        },
        "e1259113696441438aa1937a34a41a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cf76d35591014022945650b6cd1a7c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_729c6d379b4844179d3db383380a6c0f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 467042463,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 467042463,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_218306504a134e6d8617f15dab300c97"
          }
        },
        "f10b1375da0e44e09b158a757d3e4a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4b9eca41851848288f11b19ef81a111c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 467M/467M [00:14&lt;00:00, 33.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15724f8e436145e9a41716a07b40139d"
          }
        },
        "729c6d379b4844179d3db383380a6c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "218306504a134e6d8617f15dab300c97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b9eca41851848288f11b19ef81a111c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15724f8e436145e9a41716a07b40139d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43f07290172442caa94d8ec7e06b14d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cf5a02521f5d4ae1b7a1915716c2a4ee",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e7f56c07ec924b2ab8338e8591e59ce3",
              "IPY_MODEL_ae337ea257444436976650e4fb09675d"
            ]
          }
        },
        "cf5a02521f5d4ae1b7a1915716c2a4ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7f56c07ec924b2ab8338e8591e59ce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d91b0ec5efea4de0af8b540f7c9540d5",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 115730790,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 115730790,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2820c51c93994bafa510ccb71f70b7e9"
          }
        },
        "ae337ea257444436976650e4fb09675d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c80380ca0c314d4b927d97c7564b2f76",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 110M/110M [00:02&lt;00:00, 44.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e593e11592e545c38fc012e2765e9f7f"
          }
        },
        "d91b0ec5efea4de0af8b540f7c9540d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2820c51c93994bafa510ccb71f70b7e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c80380ca0c314d4b927d97c7564b2f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e593e11592e545c38fc012e2765e9f7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "e4719489-a01f-4c83-e112-7d381889b719"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzejvNI6kk3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717451da-03f6-44f4-9d12-04581717d639"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3 sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.0MB 16.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Collecting textwrap3\n",
            "  Downloading https://files.pythonhosted.org/packages/77/9c/a53e561d496ee5866bbeea4d3a850b3b545ed854f8a21007c1e0d872e94d/textwrap3-0.9.2-py2.py3-none-any.whl\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 40.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 36.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (54.2.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=d8ca5b8aec35343e9e88f97ecdf4e1c09d35bf3cd7082f654203fb422efd7b2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, textwrap3, sentencepiece\n",
            "Successfully installed sacremoses-0.0.44 sentencepiece-0.1.95 textwrap3-0.9.2 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "09bf496d-640f-4ec7-a05b-11110710989a"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification, BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'xlnet-base-cased'\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"informative\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/FYP/task_informative_text_img_agreed_lab_train.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'tweet_text', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['tweet_text'] = df_train['tweet_text'].apply(clean_text)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/FYP/task_informative_text_img_agreed_lab_dev.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'tweet_text', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['tweet_text'] = df_val['tweet_text'].apply(clean_text)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/FYP/task_informative_text_img_agreed_lab_test.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'tweet_text', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(clean_text)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/FYP/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, targets, paths, tokenizer, max_len):\n",
        "    self.tweets = tweets\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.paths = paths\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    tweet = str(self.tweets[item])\n",
        "    target = self.targets[item]\n",
        "    path = str(self.paths[item])\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "      truncation = True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'tweet_text': tweet,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long),\n",
        "      'tweet_image': img\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    tweets=df.tweet_text.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "    paths=df.image.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.bert = XLNetModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    for param in self.bert.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.resnet = torchvision.models.densenet161(pretrained=True)\n",
        "    for param in self.resnet.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(self.bert.config.hidden_size + 1000)\n",
        "\n",
        "    self.linear1 = nn.Linear(self.bert.config.hidden_size + 1000, 1000)\n",
        "    self.relu1    = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(p=0.4)\n",
        "\n",
        "    self.linear2 = nn.Linear(1000, 500)\n",
        "    self.relu2    = nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "    self.linear3 = nn.Linear(500, 250)\n",
        "    self.relu3    = nn.ReLU()\n",
        "    self.dropout3 = nn.Dropout(p=0.1)\n",
        "\n",
        "    self.linear4 = nn.Linear(250, 125)\n",
        "    self.relu4    = nn.ReLU()\n",
        "    self.dropout4 = nn.Dropout(p=0.02)\n",
        "\n",
        "    self.linear5 = nn.Linear(125, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask, tweet_img):\n",
        "    text_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False\n",
        "    )\n",
        "    image_output = self.resnet(tweet_img)\n",
        "    merged_output = torch.cat((torch.mean(text_output[0], 1), image_output), dim=1)\n",
        "    bn_output = self.bn(merged_output)\n",
        "\n",
        "    linear1_output = self.linear1(bn_output)\n",
        "    relu1_output = self.relu1(linear1_output)\n",
        "    dropout1_output = self.dropout1(relu1_output)\n",
        "\n",
        "    linear2_output = self.linear2(dropout1_output)\n",
        "    relu2_output = self.relu2(linear2_output)\n",
        "    dropout2_output = self.dropout2(relu2_output)\n",
        "\n",
        "    linear3_output = self.linear3(dropout2_output)\n",
        "    relu3_output = self.relu3(linear3_output)\n",
        "    dropout3_output = self.dropout3(relu3_output)\n",
        "\n",
        "    linear4_output = self.linear4(dropout3_output)\n",
        "    relu4_output = self.relu4(linear4_output)\n",
        "    dropout4_output = self.dropout4(relu4_output)\n",
        "\n",
        "    linear5_output = self.linear5(dropout4_output)\n",
        "\n",
        "\n",
        "    probas = self.sigmoid(linear5_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279,
          "referenced_widgets": [
            "856598119ec9437eba0c01463b64b342",
            "41bd78b816b646259ec82f63243957b0",
            "7db985d32be44f7b9e4b21a1ed62ff22",
            "22dee30fe2654056bacfd24b59b381da",
            "8e89facddf5147b38a259c9339695d78",
            "09d30fb6ddc2494b93040fa412081e74",
            "dcdf727838fb4f85808e92a6c54258c9",
            "696e5a95f9e4407aad7251103dcd96cb",
            "2adeee52653f4d64b4ceb37bf20aeed3",
            "988c9d94758148fb9c0ef8af7854988b",
            "afe48ffbe33d4756975e6a40104d8c0b",
            "13cbc5f09b1746999291e34ce8c8d5bd",
            "09282dae0b4547ad80d95f8be3673b58",
            "0bb9cbf9ae2747a48dd8f4cf86f3b017",
            "bd7b11173e904ba0890f5c45121e2d11",
            "356e5108c67142fa98985e9ed375d254",
            "53989a12310b44ac831b9a1b0427eec4",
            "df5f2c4a4c174249b9d1acb43dcc4540",
            "12d23823a9d645bc83248e7f538a6fe1",
            "01ceee62427b4fa9acd86904624f4a56",
            "b00a746e0c7a4eb6a527e7b18691c402",
            "3ee13d4000734a4999206bbfb5dc4d1d",
            "4e0a0d044ca34b9c87969dab1a116419",
            "b272b39bb4f042d8adffab58ed67bf5d",
            "528f4afafa70401194d99cdb7b87fde9",
            "e1259113696441438aa1937a34a41a59",
            "cf76d35591014022945650b6cd1a7c58",
            "f10b1375da0e44e09b158a757d3e4a6f",
            "729c6d379b4844179d3db383380a6c0f",
            "218306504a134e6d8617f15dab300c97",
            "4b9eca41851848288f11b19ef81a111c",
            "15724f8e436145e9a41716a07b40139d",
            "43f07290172442caa94d8ec7e06b14d0",
            "cf5a02521f5d4ae1b7a1915716c2a4ee",
            "e7f56c07ec924b2ab8338e8591e59ce3",
            "ae337ea257444436976650e4fb09675d",
            "d91b0ec5efea4de0af8b540f7c9540d5",
            "2820c51c93994bafa510ccb71f70b7e9",
            "c80380ca0c314d4b927d97c7564b2f76",
            "e593e11592e545c38fc012e2765e9f7f"
          ]
        },
        "outputId": "9f8409e1-e37f-47ac-d7e7-ab4824d333cd"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "MAX_LEN = 150\n",
        "\n",
        "tokenizer = XLNetTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.BCELoss().to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "856598119ec9437eba0c01463b64b342",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2adeee52653f4d64b4ceb37bf20aeed3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1382015.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53989a12310b44ac831b9a1b0427eec4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "528f4afafa70401194d99cdb7b87fde9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=467042463.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/hub/checkpoints/densenet161-8d451a50.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43f07290172442caa94d8ec7e06b14d0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=115730790.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-eqL1yqUP5a",
        "outputId": "1e0b5fce-a1fa-47aa-a60f-06663119ed97"
      },
      "source": [
        "checkpoint = torch.load(\"./gdrive/MyDrive/FYP/XlnetDenseNet-checkpoint.t7\",map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "start_epoch = checkpoint['epoch']\n",
        "best_accuracy = checkpoint['best_accuracy']\n",
        "\n",
        "print(start_epoch)\n",
        "print(best_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n",
            "tensor(0.8608, dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "6e531d3f-54a5-4aa5-be64-7121b1926fc8"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/FYP/BertResNet-checkpoint.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "# best_accuracy = checkpoint['best_accuracy']\n",
        "# print(start_epoch)\n",
        "# print(best_accuracy)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {start_epoch + epoch + 1}/{start_epoch + EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/FYP/XlnetDenseNet-checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6283384498796964 accuracy 0.6608686595146339\n",
            "Val   loss 0.5200000926852226 accuracy 0.6713286713286714\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4963518914423491 accuracy 0.74148526195188\n",
            "Val   loss 0.4666155055165291 accuracy 0.8073744437380801\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4083490857952519 accuracy 0.8359545880637433\n",
            "Val   loss 0.43005045503377914 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3447404864587282 accuracy 0.852619518800125\n",
            "Val   loss 0.43914075195789337 accuracy 0.8378893833439288\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.31689108045477615 accuracy 0.8660556192063327\n",
            "Val   loss 0.4355715438723564 accuracy 0.8366179275270185\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3047790903794138 accuracy 0.8727215915008852\n",
            "Val   loss 0.4282035008072853 accuracy 0.8372536554354736\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2945401723447599 accuracy 0.8738673054890115\n",
            "Val   loss 0.42117369174957275 accuracy 0.8385251112523839\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2829990786941428 accuracy 0.8831371732111238\n",
            "Val   loss 0.41651417315006256 accuracy 0.8429752066115702\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.275643884351379 accuracy 0.8858452244557858\n",
            "Val   loss 0.41285619884729385 accuracy 0.8486967577876668\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.27286938381822484 accuracy 0.8842828871992501\n",
            "Val   loss 0.4111899882555008 accuracy 0.8474253019707565\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.263441590886367 accuracy 0.8926153525674408\n",
            "Val   loss 0.4082261249423027 accuracy 0.8506039415130324\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.26245065974561793 accuracy 0.8916779502135194\n",
            "Val   loss 0.4103905186057091 accuracy 0.8506039415130324\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2558985423100622 accuracy 0.8950109363607958\n",
            "Val   loss 0.4067070484161377 accuracy 0.8512396694214877\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.248439537851434 accuracy 0.8967815852515363\n",
            "Val   loss 0.414140522480011 accuracy 0.8506039415130324\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24936995851366142 accuracy 0.8966774294344338\n",
            "Val   loss 0.40889425575733185 accuracy 0.8556897647806739\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24710906022473386 accuracy 0.8995937923133007\n",
            "Val   loss 0.3985000401735306 accuracy 0.8556897647806739\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24009959556554494 accuracy 0.9012602853869388\n",
            "Val   loss 0.4092273786664009 accuracy 0.8486967577876668\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24072562550243579 accuracy 0.9013644412040412\n",
            "Val   loss 0.4130200147628784 accuracy 0.8499682136045773\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23755711238635213 accuracy 0.9018852202895531\n",
            "Val   loss 0.4214118868112564 accuracy 0.8499682136045773\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2303505484995089 accuracy 0.9049057389855223\n",
            "Val   loss 0.41411617398262024 accuracy 0.8506039415130324\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23299890050762578 accuracy 0.907509634413082\n",
            "Val   loss 0.41566654294729233 accuracy 0.8512396694214877\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2271720516054254 accuracy 0.9094885949380273\n",
            "Val   loss 0.4252639338374138 accuracy 0.8486967577876668\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.22425221142015958 accuracy 0.9068846995104677\n",
            "Val   loss 0.4149976074695587 accuracy 0.8512396694214877\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.22408988601283023 accuracy 0.9063639204249557\n",
            "Val   loss 0.41362815350294113 accuracy 0.8537825810553084\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.218344332356202 accuracy 0.911571711280075\n",
            "Val   loss 0.41454017162323 accuracy 0.8531468531468531\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.21476431975239202 accuracy 0.9118841787313822\n",
            "Val   loss 0.40407029539346695 accuracy 0.856325492689129\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.22022278685318797 accuracy 0.908863660035413\n",
            "Val   loss 0.4066637009382248 accuracy 0.8537825810553084\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2157282476362429 accuracy 0.912404957816894\n",
            "Val   loss 0.4069979786872864 accuracy 0.8518753973299428\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2107383585289905 accuracy 0.9158420997812727\n",
            "Val   loss 0.4053039848804474 accuracy 0.8556897647806739\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20773591807014063 accuracy 0.9166753463180919\n",
            "Val   loss 0.4074869751930237 accuracy 0.8544183089637635\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.21317485602278458 accuracy 0.9139672950734298\n",
            "Val   loss 0.4071211740374565 accuracy 0.8569612205975843\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20335348499448677 accuracy 0.9175085928549109\n",
            "Val   loss 0.4065582901239395 accuracy 0.8569612205975843\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20160982169603048 accuracy 0.9193833975627539\n",
            "Val   loss 0.4031887352466583 accuracy 0.8588684043229498\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20608167664000862 accuracy 0.9179252161233205\n",
            "Val   loss 0.4041738212108612 accuracy 0.859504132231405\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20181600984774137 accuracy 0.9184459952088324\n",
            "Val   loss 0.40512924641370773 accuracy 0.859504132231405\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20387342885920876 accuracy 0.916467034683887\n",
            "Val   loss 0.4119744151830673 accuracy 0.8550540368722187\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2055130577401111 accuracy 0.9160504114154775\n",
            "Val   loss 0.4098929315805435 accuracy 0.856325492689129\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20251081491771497 accuracy 0.9191750859285491\n",
            "Val   loss 0.41219837963581085 accuracy 0.8569612205975843\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19787097290942543 accuracy 0.922403916258723\n",
            "Val   loss 0.4104027897119522 accuracy 0.8575969485060394\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19656885689810702 accuracy 0.9215706697219039\n",
            "Val   loss 0.4085155725479126 accuracy 0.8582326764144946\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20237982429956136 accuracy 0.916987813769399\n",
            "Val   loss 0.41053202748298645 accuracy 0.8582326764144946\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1982601339879789 accuracy 0.9199041766482657\n",
            "Val   loss 0.40829722583293915 accuracy 0.859504132231405\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19419307693054802 accuracy 0.9230288511613374\n",
            "Val   loss 0.409753680229187 accuracy 0.859504132231405\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19887304384457438 accuracy 0.9200083324653682\n",
            "Val   loss 0.40952862799167633 accuracy 0.8588684043229498\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19291809122813375 accuracy 0.9221956046245182\n",
            "Val   loss 0.407737173140049 accuracy 0.8607755880483153\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19319367957742592 accuracy 0.9217789813561087\n",
            "Val   loss 0.4060199409723282 accuracy 0.8607755880483153\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20025098010113365 accuracy 0.9193833975627539\n",
            "Val   loss 0.40563011914491653 accuracy 0.8601398601398601\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19544740256510282 accuracy 0.9211540464534944\n",
            "Val   loss 0.4070287421345711 accuracy 0.8601398601398601\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1926906650003634 accuracy 0.9228205395271326\n",
            "Val   loss 0.4072047770023346 accuracy 0.8601398601398601\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19613003966055417 accuracy 0.9213623580876992\n",
            "Val   loss 0.406931571662426 accuracy 0.8601398601398601\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFmTSaxYMw-"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/FYP/XlnetDenseNet-checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScOj15BovCww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "22d6219c-9fd7-436b-b5cd-8b7c564b18a4"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTghsXN8vEpo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "0ff3e986-22a3-42b5-a392-da783e63fe9b"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "      preds = torch.round(outputs)\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Informative', 'Informative']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f82cab6363d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m y_pred, y_test = get_predictions(\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}