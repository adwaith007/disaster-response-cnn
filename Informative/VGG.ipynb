{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "31d51284-3073-4980-8bcc-e94bddc6cd2e"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzejvNI6kk3A"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3 sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwWQL0acqLuH",
        "outputId": "73a14d43-30df-4e4e-894e-60a0bec3d1c8"
      },
      "source": [
        "!ls gdrive/MyDrive/data_image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "california_wildfires  hurricane_irma   iraq_iran_earthquake  srilanka_floods\n",
            "hurricane_harvey      hurricane_maria  mexico_earthquake\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "e5e3d2a1-11b4-4e9c-f3cf-525053844383"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"informative\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/Models/train.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/Models/val.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/Models/test.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, paths, targets):\n",
        "    self.paths = paths\n",
        "    self.targets = targets\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.paths)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    path = str(self.paths[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    return {\n",
        "      'tweet_image': img,\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    paths=df.image.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.vgg = torchvision.models.vgg16(pretrained=True)\n",
        "    # for param in self.vgg.parameters():\n",
        "    #   param.requires_grad = False\n",
        "\n",
        "    # self.linear1 = nn.Linear(1000, 256)\n",
        "    # self.relu    = nn.ReLU()\n",
        "    # self.dropout = nn.Dropout(p=0.4)\n",
        "    # self.linear2 = nn.Linear(256, 1)\n",
        "    self.linear = nn.Linear(1000, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, tweet_img):\n",
        "    output = self.vgg(tweet_img)\n",
        "    # linear1_output = self.linear1(output)\n",
        "    # relu_output = self.relu(linear1_output)\n",
        "    # dropout_output = self.dropout(relu_output)\n",
        "    # linear2_output = self.linear2(dropout_output)\n",
        "    # probas = self.sigmoid(linear2_output)\n",
        "\n",
        "    linear_output = self.linear(output)\n",
        "    probas = self.sigmoid(linear_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 40\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.BCELoss().to(device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "65e16f76-c0f7-467c-f1e0-c6666738ff05"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "checkpoint = torch.load(\"./gdrive/MyDrive/Models/VGG/checkpoint.t7\")\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "start_epoch = checkpoint['epoch']\n",
        "\n",
        "print(start_epoch)\n",
        "\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "\n",
        "#   print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "#   print('-' * 10)\n",
        "\n",
        "#   train_acc, train_loss = train_epoch(\n",
        "#     model,\n",
        "#     train_data_loader,    \n",
        "#     loss_fn, \n",
        "#     optimizer, \n",
        "#     device, \n",
        "#     scheduler, \n",
        "#     len(df_train)\n",
        "#   )\n",
        "\n",
        "#   print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "#   val_acc, val_loss = eval_model(\n",
        "#     model,\n",
        "#     val_data_loader,\n",
        "#     loss_fn, \n",
        "#     device, \n",
        "#     len(df_val)\n",
        "#   )\n",
        "\n",
        "#   print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "#   print()\n",
        "\n",
        "#   history['train_acc'].append(train_acc)\n",
        "#   history['train_loss'].append(train_loss)\n",
        "#   history['val_acc'].append(val_acc)\n",
        "#   history['val_loss'].append(val_loss)\n",
        "\n",
        "#   if val_acc > best_accuracy:\n",
        "#     state = {\n",
        "#             'best_accuracy': val_acc,\n",
        "#             'epoch': start_epoch+epoch+1,\n",
        "#             'state_dict': model.state_dict(),\n",
        "#             'optimizer': optimizer.state_dict(),\n",
        "#     }\n",
        "#     savepath= \"./gdrive/MyDrive/Models/VGG/checkpoint.t7\"\n",
        "#     torch.save(state,savepath)\n",
        "#     best_accuracy = val_acc\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.437845589298951 accuracy 0.7946047286740964\n",
            "Val   loss 0.37855299848776597 accuracy 0.8251748251748252\n",
            "\n",
            "Epoch 2/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.27331077286285826 accuracy 0.8809499010519737\n",
            "Val   loss 0.37337011328110326 accuracy 0.8302606484424666\n",
            "\n",
            "Epoch 3/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.16016215321310243 accuracy 0.9333402770544734\n",
            "Val   loss 0.4568040989912473 accuracy 0.8239033693579149\n",
            "\n",
            "Epoch 4/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0875329089988219 accuracy 0.964066243099677\n",
            "Val   loss 0.56482388652288 accuracy 0.8270820089001907\n",
            "\n",
            "Epoch 5/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.06856929182418083 accuracy 0.9706280595771274\n",
            "Val   loss 0.683548051577348 accuracy 0.8372536554354736\n",
            "\n",
            "Epoch 6/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.04652220062837985 accuracy 0.9816685761899802\n",
            "Val   loss 0.7595583383853619 accuracy 0.8264462809917356\n",
            "\n",
            "Epoch 7/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.04309998784600267 accuracy 0.9826059785439016\n",
            "Val   loss 0.7668388761006869 accuracy 0.8251748251748252\n",
            "\n",
            "Epoch 8/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.029536538591951524 accuracy 0.9876054577648161\n",
            "Val   loss 0.9932226974230546 accuracy 0.8391608391608392\n",
            "\n",
            "Epoch 9/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.02888537758324099 accuracy 0.9904176648265806\n",
            "Val   loss 1.0280222846911504 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 10/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.027469283657601887 accuracy 0.9902093531923758\n",
            "Val   loss 1.0661499454424932 accuracy 0.810553083280356\n",
            "\n",
            "Epoch 11/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.025156496122546838 accuracy 0.9906259764607853\n",
            "Val   loss 1.0159716606140137 accuracy 0.8111888111888113\n",
            "\n",
            "Epoch 12/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.015418726801726734 accuracy 0.9946880533277783\n",
            "Val   loss 1.1712627640137305 accuracy 0.8283534647171011\n",
            "\n",
            "Epoch 13/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0093541583389053 accuracy 0.9971877929382356\n",
            "Val   loss 1.2160707666323736 accuracy 0.8251748251748252\n",
            "\n",
            "Epoch 14/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.008556880530082517 accuracy 0.9979168836579523\n",
            "Val   loss 1.361337382059831 accuracy 0.8302606484424666\n",
            "\n",
            "Epoch 15/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.007313809921183029 accuracy 0.9978127278408498\n",
            "Val   loss 1.2797598151060252 accuracy 0.8219961856325493\n",
            "\n",
            "Epoch 16/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.003943404377950125 accuracy 0.9983335069263618\n",
            "Val   loss 1.6141118498948903 accuracy 0.8270820089001907\n",
            "\n",
            "Epoch 17/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.004037375692453344 accuracy 0.9982293511092594\n",
            "Val   loss 1.5867379720394428 accuracy 0.8321678321678322\n",
            "\n",
            "Epoch 18/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0035020161683764614 accuracy 0.9985418185605666\n",
            "Val   loss 1.5489393289272602 accuracy 0.8302606484424666\n",
            "\n",
            "Epoch 19/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.002894534734495162 accuracy 0.9985418185605666\n",
            "Val   loss 1.3224701377061696 accuracy 0.8270820089001907\n",
            "\n",
            "Epoch 20/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0018598916443639418 accuracy 0.9990625976460785\n",
            "Val   loss 1.607399514088264 accuracy 0.8340750158931978\n",
            "\n",
            "Epoch 21/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.001714800110840698 accuracy 0.9991667534631808\n",
            "Val   loss 1.5869275423196645 accuracy 0.8321678321678322\n",
            "\n",
            "Epoch 22/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0015155786043602635 accuracy 0.9991667534631808\n",
            "Val   loss 1.7219022237337553 accuracy 0.8296249205340115\n",
            "\n",
            "Epoch 23/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0015048246459029638 accuracy 0.9990625976460785\n",
            "Val   loss 2.2462342977523804 accuracy 0.8334392879847425\n",
            "\n",
            "Epoch 24/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0013364492689573269 accuracy 0.9993750650973856\n",
            "Val   loss 2.101997292958773 accuracy 0.8302606484424666\n",
            "\n",
            "Epoch 25/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0016099697915861772 accuracy 0.999479220914488\n",
            "Val   loss 2.3876139108951273 accuracy 0.8347107438016529\n",
            "\n",
            "Epoch 26/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.001103358251743645 accuracy 0.999479220914488\n",
            "Val   loss 2.1138739585876465 accuracy 0.8334392879847425\n",
            "\n",
            "Epoch 27/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.000973472424020297 accuracy 0.9993750650973856\n",
            "Val   loss 2.7623496514100294 accuracy 0.8289891926255563\n",
            "\n",
            "Epoch 28/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0009832958941848645 accuracy 0.9993750650973856\n",
            "Val   loss 1.9562490857564485 accuracy 0.8296249205340115\n",
            "\n",
            "Epoch 29/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0006602722176909849 accuracy 0.9997916883657952\n",
            "Val   loss 2.7355210322600145 accuracy 0.8302606484424666\n",
            "\n",
            "Epoch 30/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0009307437409383487 accuracy 0.9995833767315905\n",
            "Val   loss 2.73296234699396 accuracy 0.8302606484424666\n",
            "\n",
            "Epoch 31/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0006405422234126347 accuracy 0.9997916883657952\n",
            "Val   loss 2.7221677761811476 accuracy 0.8340750158931978\n",
            "\n",
            "Epoch 32/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0005732273792474237 accuracy 0.9997916883657952\n",
            "Val   loss 2.781761105243976 accuracy 0.8340750158931978\n",
            "\n",
            "Epoch 33/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0005187492083872545 accuracy 0.9996875325486928\n",
            "Val   loss 2.845486274132362 accuracy 0.8328035600762873\n",
            "\n",
            "Epoch 34/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0006979206978718665 accuracy 0.9996875325486928\n",
            "Val   loss 2.678099884436681 accuracy 0.8289891926255563\n",
            "\n",
            "Epoch 35/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0005993279175234085 accuracy 0.9997916883657952\n",
            "Val   loss 2.6825066667336683 accuracy 0.8296249205340115\n",
            "\n",
            "Epoch 36/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0007687518931453795 accuracy 0.999479220914488\n",
            "Val   loss 2.7464685027415934 accuracy 0.8359821996185632\n",
            "\n",
            "Epoch 37/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0006178899856157733 accuracy 0.9995833767315905\n",
            "Val   loss 2.6821346237109256 accuracy 0.8308963763509218\n",
            "\n",
            "Epoch 38/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0005609337364480899 accuracy 0.9995833767315905\n",
            "Val   loss 2.623068717809824 accuracy 0.8296249205340115\n",
            "\n",
            "Epoch 39/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0005187783937057439 accuracy 0.9995833767315905\n",
            "Val   loss 2.6755780623509335 accuracy 0.831532104259377\n",
            "\n",
            "Epoch 40/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0006005083831349854 accuracy 0.9995833767315905\n",
            "Val   loss 2.678024273652297 accuracy 0.8302606484424666\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Taz58ppcm8n8"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/VGG/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ScOj15BovCww",
        "outputId": "09c9aef6-e8eb-48bf-a4f8-5b1c751d7a0d"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTghsXN8vEpo",
        "outputId": "02ff8cae-71e2-4ae4-9122-a25496768304"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(tweet_img=tweet_imgs)\n",
        "      preds = torch.round(outputs)\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Informative', 'Informative'], digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not Informative     0.7794    0.7500    0.7644       504\n",
            "    Informative     0.8799    0.8961    0.8879      1030\n",
            "\n",
            "       accuracy                         0.8481      1534\n",
            "      macro avg     0.8296    0.8231    0.8262      1534\n",
            "   weighted avg     0.8469    0.8481    0.8473      1534\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}