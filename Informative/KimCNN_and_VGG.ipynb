{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KimCNN and VGG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d82f60d74d904588b10fe448ea099313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d470e4a54f6f4ac8b813161816ebefb3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2b4ec11ef2224ece9ecfa65280d58682",
              "IPY_MODEL_8a9f72cc877f411183a0bf721598cd21"
            ]
          }
        },
        "d470e4a54f6f4ac8b813161816ebefb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b4ec11ef2224ece9ecfa65280d58682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_652517ddd23e4b8da3d5893d6592b2d6",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b303e6f57ef54bf3a77e3010737b7a41"
          }
        },
        "8a9f72cc877f411183a0bf721598cd21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1b37f77a692b40c7b0016b664e3bad74",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [1:14:32&lt;00:00, 124kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_24307f4e638f43bc8b770fe22243a8cd"
          }
        },
        "652517ddd23e4b8da3d5893d6592b2d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b303e6f57ef54bf3a77e3010737b7a41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b37f77a692b40c7b0016b664e3bad74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24307f4e638f43bc8b770fe22243a8cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkYOJmxmJ2pQ",
        "outputId": "335eb87c-8fca-4044-b770-bf96b91f42c8"
      },
      "source": [
        "from google.colab import drive\n",
        " \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVfbg6pWv7o_"
      },
      "source": [
        "!pip3 install transformers torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36GiLe3tUEw7",
        "outputId": "017eb327-1756-49ca-e06d-fe0963243667"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wwc4P3iJ-C0"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import re \n",
        "import numpy as np \n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "from gensim.models import KeyedVectors\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "from PIL import Image"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKue_-UPEyQV"
      },
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"informative\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/Models/train_processed.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'tweet_text', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['tweet_text'] = df_train['tweet_text'].apply(clean_text)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/Models/val_processed.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'tweet_text', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['tweet_text'] = df_val['tweet_text'].apply(clean_text)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/Models/test_processed.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'tweet_text', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(clean_text)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "568kP1-2E4PC"
      },
      "source": [
        "weights = KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/crisisNLP_word2vec_model/crisisNLP_word_vector.bin', binary=True)\n",
        "weights = torch.FloatTensor(weights.vectors)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMSghDbajV4b"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, targets, paths):\n",
        "    self.tweets = tweets\n",
        "    self.targets = targets  \n",
        "    self.paths = paths\n",
        "    self.transform = transforms.Compose([\n",
        "      transforms.Resize(size=256),\n",
        "      transforms.CenterCrop(size=224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    tweet = self.tweets[item]\n",
        "    target = self.targets[item]\n",
        "\n",
        "    tweet = tweet.split()\n",
        "    tweet = list(map(int, tweet))\n",
        "\n",
        "    path = str(self.paths[item])\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img) \n",
        "\n",
        "    return {\n",
        "      'input_ids': torch.tensor(tweet),\n",
        "      'targets': torch.tensor(target, dtype=torch.long),\n",
        "      'tweet_image': img\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    tweets=df.tweet_text.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "    paths=df.image.to_numpy()\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2,\n",
        "    drop_last=True\n",
        "  )"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkI4V6vvErsc"
      },
      "source": [
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, weights, vocab_size, embedding_dim):\n",
        "\n",
        "        super(TweetClassifier, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
        "        self.embedding.requires_grad = False\n",
        "\n",
        "        self.vgg = torchvision.models.vgg16(pretrained=True)\n",
        "        self.image_dense = nn.Linear(1000, 1000)\n",
        "        self.image_bn = nn.BatchNorm1d(1000)\n",
        "        for param in self.vgg.parameters():\n",
        "          param.requires_grad = False\n",
        "        \n",
        "        \n",
        "        self.y = nn.Conv2d(1,300, (4, embedding_dim))\n",
        "        self.y_relu = nn.ReLU()\n",
        "        self.y_pool = nn.MaxPool2d((MAX_SEQUENCE_LENGTH - 4 + 1, 1))\n",
        "\n",
        "\n",
        "        self.z = nn.Conv2d(1,300, (3, embedding_dim))\n",
        "        self.z_relu = nn.ReLU()\n",
        "        self.z_pool = nn.MaxPool2d((MAX_SEQUENCE_LENGTH - 3 + 1, 1))\n",
        "\n",
        "        self.z1 = nn.Conv2d(1,300, (2, embedding_dim))\n",
        "        self.z1_relu = nn.ReLU()\n",
        "        self.z1_pool = nn.MaxPool2d((MAX_SEQUENCE_LENGTH - 2 + 1, 1))\n",
        "\n",
        "        self.w1 = nn.Conv2d(1,300, (1, embedding_dim))\n",
        "        self.w1_relu = nn.ReLU()\n",
        "        self.w1_pool = nn.MaxPool2d((MAX_SEQUENCE_LENGTH - 1 + 1, 1))\n",
        "\n",
        "        self.text_dense = nn.Linear(1200, 1000)\n",
        "        self.text_bn = nn.BatchNorm1d(1000)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(2000)\n",
        "        self.linear1 = nn.Linear(2000, 1000)\n",
        "        self.relu1    = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=0.4)\n",
        "\n",
        "        self.linear2 = nn.Linear(1000, 500)\n",
        "        self.relu2    = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "        self.linear3 = nn.Linear(500, 100)\n",
        "        self.relu3    = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(p=0.02)\n",
        "\n",
        "        self.linear4 = nn.Linear(100, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, input_ids, tweet_img):\n",
        "\n",
        "        # embedded vectors\n",
        "        embeds = self.embedding(input_ids) # (batch_size, seq_length, embedding_dim)\n",
        "        # embeds.unsqueeze(1) creates a channel dimension that conv layers expect\n",
        "        embeds = embeds.unsqueeze(1)\n",
        "\n",
        "        y_out = self.y(embeds)\n",
        "        y_out = self.y_relu(y_out)\n",
        "        y_out = self.y_pool(y_out)\n",
        "\n",
        "        z_out = self.z(embeds)\n",
        "        z_out = self.z_relu(z_out)\n",
        "        z_out = self.z_pool(z_out)\n",
        "\n",
        "        z1_out = self.z1(embeds)\n",
        "        z1_out = self.z1_relu(z1_out)\n",
        "        z1_out = self.z1_pool(z1_out)\n",
        "\n",
        "        w1_out = self.w1(embeds)\n",
        "        w1_out = self.w1_relu(w1_out)\n",
        "        w1_out = self.w1_pool(w1_out)\n",
        "\n",
        "        text_output = torch.cat((y_out, z_out, z1_out, w1_out), dim=1)\n",
        "        text_output = torch.squeeze(text_output,3)\n",
        "        text_output = torch.squeeze(text_output,2)\n",
        "        \n",
        "        text_output = self.text_dense(text_output)\n",
        "        text_output = self.text_bn(text_output)\n",
        "\n",
        "        image_output = self.vgg(tweet_img)\n",
        "        image_output = self.image_dense(image_output)\n",
        "        image_output = self.image_bn(image_output)\n",
        "\n",
        "        merged_output = torch.cat((text_output, image_output), dim=1)\n",
        "\n",
        "        merged_output = self.bn(merged_output)\n",
        "        merged_output = self.linear1(merged_output)\n",
        "        merged_output = self.relu1(merged_output)\n",
        "        merged_output = self.dropout1(merged_output)\n",
        "\n",
        "        merged_output = self.linear2(merged_output)\n",
        "        merged_output = self.relu2(merged_output)\n",
        "        merged_output = self.dropout2(merged_output)\n",
        "\n",
        "        merged_output = self.linear3(merged_output)\n",
        "        merged_output = self.relu3(merged_output)\n",
        "        merged_output = self.dropout3(merged_output)\n",
        "\n",
        "        merged_output = self.linear4(merged_output)\n",
        "\n",
        "        probas = self.sigmoid(merged_output)\n",
        "        return probas\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU_85zYGR_DU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "d82f60d74d904588b10fe448ea099313",
            "d470e4a54f6f4ac8b813161816ebefb3",
            "2b4ec11ef2224ece9ecfa65280d58682",
            "8a9f72cc877f411183a0bf721598cd21",
            "652517ddd23e4b8da3d5893d6592b2d6",
            "b303e6f57ef54bf3a77e3010737b7a41",
            "1b37f77a692b40c7b0016b664e3bad74",
            "24307f4e638f43bc8b770fe22243a8cd"
          ]
        },
        "outputId": "c8f1bc5f-c9af-4a42-e513-e09277b2a280"
      },
      "source": [
        "vocab_size = 2152854 #len(pretrained_words)\n",
        "embedding_dim = 300 #len(embed_lookup[pretrained_words[0]])\n",
        "MAX_SEQUENCE_LENGTH = 25\n",
        "\n",
        "batch_size = 128\n",
        "EPOCHS = 100\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, batch_size)\n",
        "val_data_loader = create_data_loader(df_val, batch_size)\n",
        "test_data_loader = create_data_loader(df_test, batch_size)\n",
        "\n",
        "model = TweetClassifier(weights, vocab_size, embedding_dim)\n",
        "model = model.to(device)\n",
        "\n",
        "lr=0.00001\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10, min_lr=0, verbose=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d82f60d74d904588b10fe448ea099313",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOxVqO83GQ8T"
      },
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "        input_ids = input_ids, \n",
        "        tweet_img = tweet_imgs\n",
        "        )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids = input_ids, \n",
        "        tweet_img = tweet_imgs\n",
        "        )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWeP0P6yGOhN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9716cf91-1266-44e0-de78-c93324962f60"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/Models/KimVGG/checkpoint.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "\n",
        "# print(start_epoch)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {start_epoch + epoch + 1}/{start_epoch + EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  scheduler.step(val_acc)\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/Models/KimVGG/checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc\n",
        "\n",
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/KimVGG/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)\n",
        "\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6375099889437358 accuracy 0.7335694198520987\n",
            "Val   loss 0.5702500442663828 accuracy 0.7533375715193897\n",
            "\n",
            "Epoch 2/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5090065924326579 accuracy 0.7921049890636391\n",
            "Val   loss 0.4593449831008911 accuracy 0.782581055308328\n",
            "\n",
            "Epoch 3/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4186499547958374 accuracy 0.8174148526195187\n",
            "Val   loss 0.4046705091993014 accuracy 0.7952956134774317\n",
            "\n",
            "Epoch 4/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3702521409591039 accuracy 0.8440787417977293\n",
            "Val   loss 0.37777769813934964 accuracy 0.8130959949141767\n",
            "\n",
            "Epoch 5/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3427971951166789 accuracy 0.8537652327882512\n",
            "Val   loss 0.3660999710361163 accuracy 0.8207247298156389\n",
            "\n",
            "Epoch 6/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3252270495891571 accuracy 0.8623060097906468\n",
            "Val   loss 0.3584919298688571 accuracy 0.8226319135410045\n",
            "\n",
            "Epoch 7/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3059338984886805 accuracy 0.868867826268097\n",
            "Val   loss 0.3561056926846504 accuracy 0.8258105530832803\n",
            "\n",
            "Epoch 8/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2910545603434245 accuracy 0.8744922403916259\n",
            "Val   loss 0.3549407223860423 accuracy 0.8321678321678322\n",
            "\n",
            "Epoch 9/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2714917137225469 accuracy 0.8850119779189668\n",
            "Val   loss 0.36205459882815677 accuracy 0.8302606484424666\n",
            "\n",
            "Epoch 10/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2504341514905294 accuracy 0.8959483387147172\n",
            "Val   loss 0.3677958957850933 accuracy 0.8321678321678322\n",
            "\n",
            "Epoch 11/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23283490796883902 accuracy 0.9039683366316008\n",
            "Val   loss 0.38434548179308575 accuracy 0.8283534647171011\n",
            "\n",
            "Epoch 12/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.21093797187010446 accuracy 0.9120924903655869\n",
            "Val   loss 0.40150785570343334 accuracy 0.8270820089001907\n",
            "\n",
            "Epoch 13/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19249153594175974 accuracy 0.9227163837100302\n",
            "Val   loss 0.43111882855494815 accuracy 0.8239033693579149\n",
            "\n",
            "Epoch 14/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.17230826159318288 accuracy 0.9335485886886782\n",
            "Val   loss 0.4506545538703601 accuracy 0.8270820089001907\n",
            "\n",
            "Epoch 15/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.15864042351643245 accuracy 0.9369857306530569\n",
            "Val   loss 0.48389443506797153 accuracy 0.8283534647171011\n",
            "\n",
            "Epoch 16/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14718413352966309 accuracy 0.9427143005936881\n",
            "Val   loss 0.4914860154191653 accuracy 0.8264462809917356\n",
            "\n",
            "Epoch 17/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13201497664054235 accuracy 0.9492761170711383\n",
            "Val   loss 0.5286596938967705 accuracy 0.8264462809917356\n",
            "\n",
            "Epoch 18/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12291210919618606 accuracy 0.9534423497552338\n",
            "Val   loss 0.5755540455381075 accuracy 0.8239033693579149\n",
            "\n",
            "Epoch 19/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1143612069884936 accuracy 0.9547963753775648\n",
            "Val   loss 0.5932669987281164 accuracy 0.8251748251748252\n",
            "\n",
            "Epoch 20/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10659298678239186 accuracy 0.9589626080616602\n",
            "Val   loss 0.5192042291164398 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 21/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10614249085386594 accuracy 0.9567753359025101\n",
            "Val   loss 0.5192759310205778 accuracy 0.8150031786395423\n",
            "\n",
            "Epoch 22/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10312710200746854 accuracy 0.9620872825747318\n",
            "Val   loss 0.5226873507102331 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 23/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10463301435112954 accuracy 0.9577127382564316\n",
            "Val   loss 0.5245574191212654 accuracy 0.8169103623649079\n",
            "\n",
            "Epoch 24/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10373984465996425 accuracy 0.9620872825747318\n",
            "Val   loss 0.5268317138155302 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 25/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10151365642746289 accuracy 0.961358191855015\n",
            "Val   loss 0.5293658375740051 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 26/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10050219580531121 accuracy 0.9620872825747318\n",
            "Val   loss 0.531556191543738 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 27/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09998230497042337 accuracy 0.9631288407457557\n",
            "Val   loss 0.534733717640241 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 28/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10034744108716646 accuracy 0.9608374127695032\n",
            "Val   loss 0.536879780391852 accuracy 0.8156389065479974\n",
            "\n",
            "Epoch 29/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09899850577116012 accuracy 0.961045724403708\n",
            "Val   loss 0.5383273636301359 accuracy 0.8156389065479974\n",
            "\n",
            "Epoch 30/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0976070707043012 accuracy 0.9626080616602437\n",
            "Val   loss 0.5426317950089773 accuracy 0.8169103623649079\n",
            "\n",
            "Epoch 31/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09861074045300483 accuracy 0.9609415685866055\n",
            "Val   loss 0.5419530769189199 accuracy 0.8169103623649079\n",
            "\n",
            "Epoch 32/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0977088690797488 accuracy 0.9611498802208103\n",
            "Val   loss 0.5428164253632227 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 33/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0986522034307321 accuracy 0.9616706593063222\n",
            "Val   loss 0.5426123713453611 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 34/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09703084021806717 accuracy 0.9626080616602437\n",
            "Val   loss 0.5430711855491003 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 35/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09787411918242772 accuracy 0.9631288407457557\n",
            "Val   loss 0.5434179281195005 accuracy 0.8169103623649079\n",
            "\n",
            "Epoch 36/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09665361752112707 accuracy 0.9633371523799604\n",
            "Val   loss 0.5434999465942383 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 37/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09960208440820376 accuracy 0.9620872825747318\n",
            "Val   loss 0.543757309516271 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 38/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09843254851798217 accuracy 0.9619831267576294\n",
            "Val   loss 0.5443142081300417 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 39/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09685244172811508 accuracy 0.9620872825747318\n",
            "Val   loss 0.5446944211920103 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 40/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0997220083574454 accuracy 0.9602124778668888\n",
            "Val   loss 0.5447954585154852 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 41/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09707151263952256 accuracy 0.9629205291115508\n",
            "Val   loss 0.5451083679993948 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 42/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09807479878266652 accuracy 0.962712217477346\n",
            "Val   loss 0.5455048307776451 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 43/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.095280459523201 accuracy 0.9626080616602437\n",
            "Val   loss 0.5450372099876404 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 44/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09678253173828125 accuracy 0.963232996562858\n",
            "Val   loss 0.5454533199469248 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 45/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09675969342390696 accuracy 0.9635454640141652\n",
            "Val   loss 0.545418823758761 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 46/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09643008003632228 accuracy 0.961878970940527\n",
            "Val   loss 0.5454320982098579 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 47/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09791795492172241 accuracy 0.961045724403708\n",
            "Val   loss 0.5450599665443102 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 48/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09602647935350736 accuracy 0.9626080616602437\n",
            "Val   loss 0.5455138882001241 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 49/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0955553854505221 accuracy 0.9630246849286532\n",
            "Val   loss 0.545288510620594 accuracy 0.8162746344564527\n",
            "\n",
            "Epoch 50/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09626902560393015 accuracy 0.9625039058431413\n",
            "Val   loss 0.5453269332647324 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 51/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09554146851102512 accuracy 0.96375377564837\n",
            "Val   loss 0.545448270936807 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 52/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0945165445903937 accuracy 0.9634413081970628\n",
            "Val   loss 0.5457724258303642 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 53/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09725535273551941 accuracy 0.9620872825747318\n",
            "Val   loss 0.5454819252093633 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 54/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09743146747350692 accuracy 0.961878970940527\n",
            "Val   loss 0.545638973514239 accuracy 0.8156389065479974\n",
            "\n",
            "Epoch 55/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09593497380614281 accuracy 0.9644828663680867\n",
            "Val   loss 0.5455517992377281 accuracy 0.8162746344564527\n",
            "\n",
            "Epoch 56/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0972397826363643 accuracy 0.9622955942089365\n",
            "Val   loss 0.5457456161578497 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 57/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09687478865186373 accuracy 0.9631288407457557\n",
            "Val   loss 0.5456676657001177 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 58/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09676402007540066 accuracy 0.9608374127695032\n",
            "Val   loss 0.5459146251281103 accuracy 0.8169103623649079\n",
            "\n",
            "Epoch 59/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09677906826138497 accuracy 0.9630246849286532\n",
            "Val   loss 0.5459839577476183 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 60/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09613944917917251 accuracy 0.9635454640141652\n",
            "Val   loss 0.5458449895183245 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 61/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09778616761167844 accuracy 0.9609415685866055\n",
            "Val   loss 0.5457032173871994 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 62/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09667043864727021 accuracy 0.964066243099677\n",
            "Val   loss 0.5460721999406815 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 63/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0949817293137312 accuracy 0.96375377564837\n",
            "Val   loss 0.5460475410024325 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 64/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09532963891824087 accuracy 0.9643787105509842\n",
            "Val   loss 0.5460149422287941 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 65/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09677257637182872 accuracy 0.9633371523799604\n",
            "Val   loss 0.5460544029871622 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 66/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09720536584655444 accuracy 0.961358191855015\n",
            "Val   loss 0.5460129355390867 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 67/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09658943211038908 accuracy 0.9626080616602437\n",
            "Val   loss 0.5461098998785019 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 68/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09694340134660404 accuracy 0.9633371523799604\n",
            "Val   loss 0.5459866647919019 accuracy 0.8169103623649079\n",
            "\n",
            "Epoch 69/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09812930966416995 accuracy 0.963232996562858\n",
            "Val   loss 0.5458871945738792 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 70/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09641421303153037 accuracy 0.9620872825747318\n",
            "Val   loss 0.546390101313591 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 71/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.096943918466568 accuracy 0.9629205291115508\n",
            "Val   loss 0.5462000668048859 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 72/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.097735652277867 accuracy 0.962399750026039\n",
            "Val   loss 0.5461359346906344 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 73/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0956351688504219 accuracy 0.9619831267576294\n",
            "Val   loss 0.5464346135656039 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 74/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09886327341198921 accuracy 0.9625039058431413\n",
            "Val   loss 0.5464200774828593 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 75/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09783032829562822 accuracy 0.9601083220497865\n",
            "Val   loss 0.5468124970793724 accuracy 0.8162746344564527\n",
            "\n",
            "Epoch 76/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09553473343451818 accuracy 0.9620872825747318\n",
            "Val   loss 0.5466655790805817 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 77/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09690567870934805 accuracy 0.9612540360379127\n",
            "Val   loss 0.5465294892589251 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 78/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09535658518473307 accuracy 0.9636496198312675\n",
            "Val   loss 0.5466607213020325 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 79/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09747935399413109 accuracy 0.961358191855015\n",
            "Val   loss 0.546687218050162 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 80/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09636447643240292 accuracy 0.961878970940527\n",
            "Val   loss 0.5467659855882326 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 81/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09642587120334307 accuracy 0.9621914383918342\n",
            "Val   loss 0.5466922620932261 accuracy 0.8162746344564527\n",
            "\n",
            "Epoch 82/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09769952113429706 accuracy 0.962712217477346\n",
            "Val   loss 0.5467612147331238 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 83/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09430994247396787 accuracy 0.9631288407457557\n",
            "Val   loss 0.5469156056642532 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 84/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09460131560762723 accuracy 0.9644828663680867\n",
            "Val   loss 0.5468214924136797 accuracy 0.8169103623649079\n",
            "\n",
            "Epoch 85/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09559855699539184 accuracy 0.9639620872825747\n",
            "Val   loss 0.546760027607282 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 86/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09596861809492112 accuracy 0.9622955942089365\n",
            "Val   loss 0.5469173664848009 accuracy 0.8169103623649079\n",
            "\n",
            "Epoch 87/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09553279002507528 accuracy 0.9625039058431413\n",
            "Val   loss 0.546967476606369 accuracy 0.8156389065479974\n",
            "\n",
            "Epoch 88/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09559422478079796 accuracy 0.962399750026039\n",
            "Val   loss 0.5468793114026388 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 89/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0953035786251227 accuracy 0.9629205291115508\n",
            "Val   loss 0.5469684874018034 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 90/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09699008330702781 accuracy 0.9615665034892198\n",
            "Val   loss 0.547227015097936 accuracy 0.8169103623649079\n",
            "\n",
            "Epoch 91/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0967200593650341 accuracy 0.960004166232684\n",
            "Val   loss 0.5473327164848646 accuracy 0.8162746344564527\n",
            "\n",
            "Epoch 92/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09645840575297673 accuracy 0.9628163732944485\n",
            "Val   loss 0.5468361477057139 accuracy 0.817546090273363\n",
            "\n",
            "Epoch 93/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09694200033942858 accuracy 0.9603166336839912\n",
            "Val   loss 0.5470436364412308 accuracy 0.8162746344564527\n",
            "\n",
            "Epoch 94/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09652150611082713 accuracy 0.962712217477346\n",
            "Val   loss 0.5470510025819143 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 95/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09632261152068775 accuracy 0.9631288407457557\n",
            "Val   loss 0.5473161687453588 accuracy 0.8162746344564527\n",
            "\n",
            "Epoch 96/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09658264711499215 accuracy 0.9622955942089365\n",
            "Val   loss 0.5474603995680809 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 97/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09649320478240649 accuracy 0.9615665034892198\n",
            "Val   loss 0.5473869095245997 accuracy 0.8188175460902734\n",
            "\n",
            "Epoch 98/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09649209360281627 accuracy 0.961878970940527\n",
            "Val   loss 0.5472303529580435 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 99/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09787361140052478 accuracy 0.9606291011352984\n",
            "Val   loss 0.5477302198608717 accuracy 0.8181818181818182\n",
            "\n",
            "Epoch 100/100\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09659636398156483 accuracy 0.9636496198312675\n",
            "Val   loss 0.5474732890725136 accuracy 0.8181818181818182\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3wV9bnv8c+zLrkHCCTcEbCioCgioHZrrdayt5cW2nqhrdatx5ZdT72d2p7Nabtba+3ZrXZ3u23pRVur7fZG7bbF1tZdKR5sq5XgBRFQUEDCNUAg92Stlef8MZNkERIIkJWQrO/79VqvzOW3Zp5ZM5ln5jczvzF3R0REslekrwMQEZG+pUQgIpLllAhERLKcEoGISJZTIhARyXJKBCIiWU6JQAY0M/u9mf1jT5c9zBjON7OKg4z/kZn9S0/PV6S7TM8RyLHGzGrTeguAJiAV9v+Tuz/c+1EdOTM7H/hPdx97lNPZCHza3Z/tibhEWsX6OgCRjty9qLX7YDs/M4u5e7I3Y+uv9FvJwahqSPqN1ioWM/tnM9sO/MzMSszst2ZWaWZVYffYtO88Z2afDruvNbM/m9l3wrIbzOziIyw70cyWmVmNmT1rZgvN7D8PEf9tZrbTzLaZ2XVpwx80szvD7tJwGfaa2R4ze97MImb2C+A44CkzqzWz/x2Wn2Nmb4TlnzOzKWnT3Rj+ViuBOjP7opn9qkNM95rZfxzJ+pCBQ4lA+puRwFBgPDCfYBv+Wdh/HNAAfP8g3z8LeBMoBe4CfmpmdgRlHwFeAoYBtwOf6kbcg4ExwPXAQjMr6aTcbUAFUAaMAL4EuLt/CngX+LC7F7n7XWZ2IvAocGtY/mmCRJGTNr1PAJcCQ4D/BC4ysyEQnCUAHwd+fojYZYBTIpD+pgX4mrs3uXuDu+9291+5e7271wDfBN5/kO9vcvf73T0FPASMItjhdrusmR0HzAK+6u7N7v5nYPEh4k4Ad7h7wt2fBmqBk7ooNwoYH5Z93ru+kDcP+J27/9HdE8B3gHzg79LK3Ovum8PfahuwDLgiHHcRsMvdVxwidhnglAikv6l098bWHjMrMLMfm9kmM6sm2NENMbNoF9/f3trh7vVhZ9Fhlh0N7EkbBrD5EHHv7lBHX9/FfO8G1gP/bWbvmNmCg0xzNLApLcaWMI4xB4nrIeDqsPtq4BeHiFuygBKB9Dcdj45vIziyPsvdBwHnhcO7qu7pCduAoWZWkDZsXE9M2N1r3P02dz8emAN83swubB3dofhWgioxAMJqq3HAlvRJdvjOr4HTzGwq8CGgX92BJZmhRCD9XTHBdYG9ZjYU+FqmZ+jum4By4HYzyzGz9wIf7olpm9mHzOyEcKe+j+C22ZZw9A7g+LTii4BLzexCM4sTJMUm4K8Hib0ReILwGoe7v9sTcUv/pkQg/d09BPXiu4AXgT/00nyvAt4L7AbuBB4n2AkfrUnAswTXEF4AfuDuS8Nx/wp8JbxD6Avu/iZB9c73CJb/wwQXk5sPMY+HgFNRtZCE9ECZSA8ws8eBte6e8TOSoxVe7F4LjHT36r6OR/qezghEjoCZzTKz94T3+F8EzCWofz+mmVkE+DzwmJKAtMpYIjCzB8KHZ1Z1Md7Ch1nWm9lKMzsjU7GIZMBI4DmCKpx7gRvc/ZU+jegQzKwQqAZm0wvXUqT/yFjVkJmdR/BP8nN3n9rJ+EuAm4BLCB7c+Q93PysjwYiISJcydkbg7suAPQcpMpcgSbi7v0hw7/eoTMUjIiKd68tG58aw/8MuFeGwbR0Lmtl8guYEKCwsnDF58uReCVBEZKBYsWLFLncv62xcv2h91N3vA+4DmDlzppeXl/dxRCIi/YuZbepqXF/eNbSF/Z/GHMv+T0SKiEgv6MtEsBi4Jrx76GxgX9goloiI9KKMVQ2Z2aPA+UCpBa/p+xoQB3D3HxE0mXsJQQNb9cB1nU9JREQyKWOJwN0/cYjxDnwuU/MXEZHu0ZPFIiJZrl/cNSQ9o7oxQU40Ql58/6b6a5uSVNU1U5QbozA3RsSgrilFbXOSmsYEVXUJ9tY30+JwfFkhE0sLO53G9n0N7KlLUFacy6jBeQeUAUimWmhIpKhuTFLdkGBfQ4KG5hQNiRSNiRTJlJNscVItLeTnxCgtyqG0KLctLsMwgxZ3Whzy41GGF+cSiQStTjclU6zbUcvmPfVEIkYsYkQiRtSMiBmO09Ccoj6cZ3OyhUSqhRZ3yopzGT04n9FD8hlSEKcwJ0YkYjQnW9hR3cj26kZqm5I0J1toSrYQNSM/J0JeLEpjMsWu2mZ21TbRlGghHjXi0QiFuTHKinMZXpzLkIIcYhEjGjFqm5Ks2VbN6m3VVFY3cdywAt5TVsToIfk0JlLUNCZoSrZw3NACThheRHFenIbmFO/sqmXjrnrqmpJt8Y8pyWfKqEGMH1rAnvpmXnl3L69t3kttU5KcWIR41EimnPrmFHXNSeKRCEMK4wzJz2HEoFzGDytg/LBCinJj7KkLlqGqPkFtY5K6piSNyRRmRsQIvlsQZ1hRDsV5ceqaktQ0JqlvThKPRsiPR8mNR8mNRciLR4hHIyRSLTQmWmhKphgxKI8xQ/JpfdFbS4tTWdtEqsUpzotRmBNjX0OCt3bU8NaOGhw4oayIE0YUUVqYS2MyRWOipW1baU4F6691+2kK59OUbKG+OcWumiZ21jRRVd8cbPs5UQriUUYOzmP0kHxGDs4jahZuc44ZBJuShXEH8yvMjYbrMY+Sgnhb/ACNiRRrt9fQnGzhpBHFDC6It22LG3bVsXVvA02JFppTLaRanKLcGMV5cYrzYsEy58bIj0dJtjjJVAspd4bk55ATC47TaxoTvF6xj1c27+WCk4Zz8uhBPbRHaKdEMMC8U1nLX9/eTVFujPeUFTFuaD4vvrOHJ1ZU8NybO8mJRThvUhmzTx5BfSLFH1fv4IW3d5FIdf8J84jB8OI8nOCfpzHRQm3Tge9FLymIE0n7J2tMpEi29PyT7DmxCONK8olHI6zfWdtj8zCDwpwYdc1JMtU2YzxqDCvM5clXtxx0HkMK4uytTxx0WjmxCM3JoMXqWMTIz4mSSLWQSDmxiFGQE6UgJ0aypYWq+kRb2d5WlBvjPcOLaGxOsWlPHY2J9jjM6PHfenB+nKGFOW079rqm4CDgSOVEI4wcnMfIwXlUNyRYt7OWVNo2N3JQHoW5UTburt9v+JHEXZQbY+u+hrbfpDgvlpFE0O9aH9VzBIHGRIpX3t3Ltn0NVNY0UVHVwJ/X72LDrrpOyw8vzuUj08dQ35zkj6t3sKM6aDF5Ymkhs08ewQllRdQ1J6ltTNLiUJQXoyg3SlFunJKCOEMKcnCctyvrWL+zlm17G4iGR9u5sQgjBwX/GEMKcthV08TWvQ3sqAleJBY1IxqJkBsPjhjz4hEG5cUZnB98CnJj5MWDI+t4LBIcxZtR35xsO8puaE7R4o57cDYQjQRnBrWNSTZXNfDu7nqakimmjBrEKaMHM6E0eGdMqiU4w/DwDAKCs4iCnCj5OVFyohHisQgG7KxpYtveRrbubWBfQ4KaxgQ1TUkG58cZPTifEYPzKM6LkRuLkBuLkGoJ1kNDIkVOLEJZUS6lRbnkxSMkUk4iFSTIypomdtY0sq8hQaoFUi0t5MainDSymPeUFZETi9CYCI4et+1roDAnOGKMR42Nu+tZv7OWzVX1jBqUx3uGFzFhWCGDC+Lkx6PEosamXfWs2VbNmztqGF6cyxnjS5g6ejD5OV29pA3cnYZEiu37Gtm0u56Nu+uoa0pSWpTLsKJchhbGKcyNUZQbIy8exT34TlOyhb31CXbXNVHTmAyPbmMU5MTajsxbz1RaP/FYhLxY8DtvqWpg3Y4a1lfWkh+PMWFYAeOHFRCPRqhpTFLdmGBQXpxJI4o4cUQx0Yixfmct63bUsLchQV48Sn48Sk4sQk40QixqbUf6ebFg28qNRdu2tWFFOeTGogcse3Vjki1VDeyobsRxopEI0dazFHda3MmJRtrObuqaklTWNrGzuokdNY1s39fItr2N5OVEOXXMIE4dM5jceJQ3t9fw5vYa6pqSnDiimEkjijhuaAF5YcwRM2rD5axpTFDblGo7u4tFjJxwW6yqT7Crtol9DQneU1bEtHFDmDZ2MEMKcjpZm91jZivcfWan45QI+o/apiRPvbaVJWt28Jf1u/c7qinIiTJrwlA+MHk47z+xjESqhbcra9m4u56TRhTzvkmlxKLBqWZLi7N6WzV58QjvKSva7zRXRAamgyUCVQ31A5U1TTz41w384oVNVDcmGVuSz5Uzx3L+ScOZUFpIWXEuhTnRA3bok0YUdzq9SMSYOmZwb4QuIv2AEsExyt15+d29PPK3d3lq5VYSqRYuOmUk8887ntPHDdFRvIj0GCWCY0h9c5KXN+3lpQ27+e/VO1i7vYbCnChXzhzL/zhnIseXFfV1iCIyACkRHAMSqRa+/fu1PPjXjSRbgguhp40dzP/96KnMOX00RblaTSKSOdrD9LFt+xq48ZFXWLGpiitnjuXS00YzY3yJdv4i0mu0t+lDy96q5NbHX6UpkeL7n5zOh04b3dchiUgWUiLoA6kW555n3+L7S9czaXgRP7hqBicMV/2/iPQNJYJetqO6kVsee4UX39nDFTPGcsfcqQd98EdEJNOUCHpJYyLFA3/ZwMI/rafF4TtXTOPyGWP7OiwRESWCTHN3nnljO998eg2b9zQw++QRfPmSKUwoLezr0EREACWCjFq1ZR93/HY1L23Yw4kjivjP68/i3EmlfR2WiMh+lAgyoDGR4hu/Xc0jL71LSUEOd35kKh+fNa6trR8RkWOJEkEPe3d3PTc8vII3tlZz/bkTueWDkxiUF+/rsEREuqRE0IOWrNnB/3r8VQAeuHYmH5g8oo8jEhE5NCWCHpBItXD3M29y37J3OGX0IH509QzGDS3o67BERLpFieAobd5Tz02PvsKrm/fyqbPH8+VLp3T6ikYRkWOVEsERcHeWb6zi0Zfe5XevbyM3GmHhJ8/g0tNG9XVoIiKHTYngMDQlU/zmla3c//w7rNtZS3FujHkzxzH/vONVFSQi/ZYSQTc0JVP84oVN3P/8O+yobmLKqEHcdflpfOi0URTk6CcUkf5Ne7FDWPZWJV9b/AYbdtXx3uOHcdfl0zhvUqneECYiA4YSQRcqa5q4ffEb/O71bUwYVsCD183i/JOG93VYIiI9TomgE79/fRtf/vUqapuS3Db7ROa//3hyY7oTSEQGJiWCNE3JFAt+9TpPvrKFU8cM5rtXTmPSiOK+DktEJKOUCNI8+JeNPPnKFm6+cBI3feAE4mobSESygBJBaF9Dgh889zbnn1TG52ef2NfhiIj0GiWC0H3L3mZfQ4Iv/sNJ3ftCsglWLoLyn0IqASUTYOjEYPieDVC1EfKHwPHnw/EXwJgzIJ7fvWm3pILp5OjZBBHJPCUCYGd1Iw/8eSNzpo3mlNGDuy7YVAtbVsDG52HFQ1C3E0ZMhUFjYNc6WPdHiObA0AkwfDLU7IDn/w2W3R18P3cQFJZByXgYeyaMOxMKhkJFOWx+CXauhtodUL8bvAUKhgUJZvBYiISrKhKDYZNgxClQeiIk6qC2Epr2BQmnYGimfy4RGWCUCIDv/Wk9iVTL/lVCiQZY9SuoXBsc3e9+ByrXBDtogBM+CO+9MTjib32mwD34m/6MQcPeIHFUvgl1lcGOftc6WHZX+7QAikbCqGkwZgYUDQ/OHvZuhqoNsDNtvskmWPl45wuSOwjOvRXOukFnEyLSbVmfCDbtruPRl95l3qxxwesjU0l47RFY+q9QsxWiucERfMlEmHwpjDsLxs6A/JIDJ9bZQ2b5Q2DKh4NPusbq4OyioQrGzoTB4zr/fmeaamDnWti9DnKKoChs7vov98CSO+Cl++Gki4OzleFTgnnseCP4pJohpxDiBYBDcx0010NzLSTqg/5Uc9oyRYPyOYWQN6S9Cqx4FETCW2pTzWGSq4SGPe0JsSUZnN3UVULdLvBU+3Rj+UGyyimEeDj9eH5Qvmpj8IEgKRYOh2g8jLUuiLv1e5FI+zKkmtqn35IMhiXqg6R+KGbBOi0cDoWlQfVc628SzWn/DVrLFJUF66G1GrB+dxBHoj6oKmwViQXLGS8MpjvilOAzeBwkG4Pv1O2CneH62bOhPelHosHZYMlEGDwm2GbqKoNPU237/NriKwi23+a64EwxEgvWc05h8Pu1xRRvX55UIjizra0MlrdVPB+GjA/WdcFQqNsdlGtdzua64KAknt8+j9ZPLDfY5morg/KRWPu4QWOCbahkfLCOaneG287O9jiSjR1+v/C70Zz2/5F4QTidCcE6qd8dTCM9vpZEcFZdWAZ5g2DfluDAqnpb8D8zNPx+0chgOysYGhx87XgjOPhK1LXHkVscbhtlwfxat91kcxj3zmC9FZYF00olgjP8HW8EB4PDpwTrvXgU7HsX9myE6ooDf8vW/wPr4kaVC74Mp15+6O35MJm3/tP2EzNnzvTy8vIem97/fHgFS9dW8twXz2dEagc8Mi848h8zEz54O4w/J9jZ9BebXgiqoirKg+qiNhZs9DlFwQbeVBvsaDr+E8cL9v+Ha92hNtdB/S6o2gTJg+xY8wYHyQOCjblgWPhPNqx9Z+TevhNs3Zk11wbzyS8Jdj5Dxgffr90R7Chaku0xYu1JqyXVvhOM5rbHbdFwB1wQXps5RJL1FNTvaU9a0Xj7P2UqEcyvqTZIdK1VcdGc9p1lYdn+v1+rlkSYkOqCHdCON6B2e4eZWzCNEafAsBOCHTUECXbf5iA5VG8NftvW3zJvULAu4/lBudZkGI2FwwuCZWrbKSbbZ5dqbl+nkWj7ziu3uP13aq4N1nXVxmCZC0qDRFZYGky/dYefaAzXXV37Okk2BgcNRWXB91rjaKqF6i3B8tRsbd8+WhNr4fDwbLhg/1gT9Qcm+qaa9viSDUFMhWVhfIVBfyQarNPandC4L0imJROCnXHtziApVG0M1jdp+8FBY2HEycHv3bq9NlW3J63G6jBpht9pPTiIRNuTUSQaVN2OOCWYzs61sGMVNO4NEk/JBBgy7sDfMhGuR7rYL59+FbzngoNvy10wsxXuPrOzcRk9IzCzi4D/AKLAT9z9Wx3GHwc8BAwJyyxw96czGVO68o17ePr17dxy4SRG5Dv89OrgH+7KXwRH8P2xGYnx74VP/Vew8e6rCI5s8kuCI5LcoqOfvnuwc67ZTtvGGom17yiiWfI2tmRTsMM+koOEul1Qsy1MwkXBTr27NxIMFMmmYLuJHOWDmq0HFUfz+6XCM9f63TBoVOdn+53NN9EQLEMsZ/9xqWRwdtBxuHuQ2GK5Rx5rhmQsEZhZFFgIzAYqgOVmttjdV6cV+wqwyN1/aGYnA08DEzIVUzp3587frWF4cS7/dN5EeOpzsP11+OQiOPHveyOEzDILjjiGjOv56RaPDD7Z7Gj+mVuPrLNZT+0MzY4+iUZjUDwi+BzOfLu6DhftYrdqdkwmAYBM1nmcCax393fcvRl4DJjboYwDg8LuwcDWDMazn6dWbuPVzXv5wt+fRMErPw0uwF7w5YGRBEREDkMmE8EYYHNaf0U4LN3twNVmVkFwNnBTZxMys/lmVm5m5ZWVlUcdWGMixbd/v5bJI4u5rOg1eOZLcNKl8L7bjnraIiL9TV9fBf0E8KC7jwUuAX5hduDlcne/z91nuvvMsrKyo57p4te2smVvA9878TWiiz4V3Lb50R/1r4vCIiI9JJMXi7cA6RXUY8Nh6a4HLgJw9xfMLA8oBXZmMC5eXL+LBfm/ZtJLi+CE2XDlQ+HdKCIi2SeTh8DLgUlmNtHMcoCPA4s7lHkXuBDAzKYAecDR1/0cwvD1i/isLwpuxfrEo0oCIpLVMpYI3D0J3Ag8A6whuDvoDTO7w8zmhMVuAz5jZq8BjwLXeoYfbKjYtY+rmn9J5aCpMHdh9tzuKCLShYw+RxA+E/B0h2FfTeteDZyTyRg62vGXXzAjUsmm9367fz4nICLSw7Lr6mhLinGrf8xaxjP2zI/2dTQiIseE7EoEq3/D8KZ3WVL6KaJ66YyICJBNjc65k/x/d7OxZTTRUzo+1yYikr2y57D4rT8Qq1zNwuRcZh1/9M8iiIgMFNmTCBINvFt4Gv8dPZdTxxzk5TMiIlkme6qGpn6Mzy4pY9pxcXJi2ZP/REQOJWv2iPsaEqzZXs2ZE/UqRxGRdFmTCFZs2oM7nDlBiUBEJF3WJILVW6uJRYzpx3XjpRMiIlkka64R3PiBScybdRz5OUf5RiQRkQEma84IAMqKj823A4mI9KWsSgQiInIgJQIRkSynRCAikuWUCEREspwSgYhIllMiEBHJckoEIiJZTolARCTLKRGIiGQ5JQIRkSynRCAikuWUCEREspwSgYhIllMiEBHJckoEIiJZTolARCTLKRGIiGQ5JQIRkSynRCAikuWUCEREspwSgYhIllMiEBHJckoEIiJZLqOJwMwuMrM3zWy9mS3oosyVZrbazN4ws0cyGY+IiBwolqkJm1kUWAjMBiqA5Wa22N1Xp5WZBPwf4Bx3rzKz4ZmKR0REOpfJM4IzgfXu/o67NwOPAXM7lPkMsNDdqwDcfWcG4xERkU5kMhGMATan9VeEw9KdCJxoZn8xsxfN7KLOJmRm882s3MzKKysrMxSuiEh26uuLxTFgEnA+8AngfjMb0rGQu9/n7jPdfWZZWVkvhygiMrAdMhGY2YfN7EgSxhZgXFr/2HBYugpgsbsn3H0D8BZBYhARkV7SnR38PGCdmd1lZpMPY9rLgUlmNtHMcoCPA4s7lPk1wdkAZlZKUFX0zmHMQ0REjtIhE4G7Xw1MB94GHjSzF8I6++JDfC8J3Ag8A6wBFrn7G2Z2h5nNCYs9A+w2s9XAUuCL7r77KJZHREQOk7l79wqaDQM+BdxKsGM/AbjX3b+XufAONHPmTC8vL+/NWYqI9HtmtsLdZ3Y2rjvXCOaY2ZPAc0AcONPdLwamAbf1ZKAiItL7uvNA2WXAv7v7svSB7l5vZtdnJiwREekt3UkEtwPbWnvMLB8Y4e4b3X1JpgITEZHe0Z27hn4JtKT1p8JhIiIyAHQnEcTCJiIACLtzMheSiIj0pu4kgsq02z0xs7nArsyFJCIivak71wg+CzxsZt8HjKD9oGsyGpWIiPSaQyYCd38bONvMisL+2oxHJSIivaZb7yMws0uBU4A8MwPA3e/IYFwiItJLuvNA2Y8I2hu6iaBq6ApgfIbjEhGRXtKdi8V/5+7XAFXu/nXgvQSNw4mIyADQnUTQGP6tN7PRQAIYlbmQRESkN3XnGsFT4cti7gZeBhy4P6NRiYhIrzloIghfSLPE3fcCvzKz3wJ57r6vV6ITEZGMO2jVkLu3AAvT+puUBEREBpbuXCNYYmaXWet9oyIiMqB0JxH8E0Ejc01mVm1mNWZWneG4RESkl3TnyeKDvpJSRET6t0MmAjM7r7PhHV9UIyIi/VN3bh/9Ylp3HnAmsAL4QEYiEhGRXtWdqqEPp/eb2TjgnoxFJCIivao7F4s7qgCm9HQgIiLSN7pzjeB7BE8TQ5A4Tid4wlhERAaA7lwjKE/rTgKPuvtfMhSPiIj0su4kgieARndPAZhZ1MwK3L0+s6GJiEhv6NaTxUB+Wn8+8GxmwhERkd7WnUSQl/56yrC7IHMhiYhIb+pOIqgzszNae8xsBtCQuZBERKQ3decawa3AL81sK8GrKkcSvLpSREQGgO48ULbczCYDJ4WD3nT3RGbDEhGR3tKdl9d/Dih091XuvgooMrP/mfnQRESkN3TnGsFnwjeUAeDuVcBnMheSiIj0pu4kgmj6S2nMLArkZC4kERHpTd25WPwH4HEz+3HY/0/A7zMXkoiI9KbuJIJ/BuYDnw37VxLcOSQiIgPAIauGwhfY/w3YSPAugg8Aa7ozcTO7yMzeNLP1ZrbgIOUuMzM3s5ndC1tERHpKl2cEZnYi8Inwswt4HMDdL+jOhMNrCQuB2QRNVy83s8XuvrpDuWLgFoJkIyIivexgZwRrCY7+P+Tu57r794DUYUz7TGC9u7/j7s3AY8DcTsp9A/g20HgY0xYRkR5ysETwMWAbsNTM7jezCwmeLO6uMcDmtP6KcFibsOmKce7+u4NNyMzmm1m5mZVXVlYeRggiInIoXSYCd/+1u38cmAwsJWhqYriZ/dDM/v5oZ2xmEeC7wG2HKuvu97n7THefWVZWdrSzFhGRNN25WFzn7o+E7y4eC7xCcCfRoWwBxqX1jw2HtSoGpgLPmdlG4GxgsS4Yi4j0rsN6Z7G7V4VH5xd2o/hyYJKZTTSzHODjwOK0ae1z91J3n+DuE4AXgTnuXt755EREJBOO5OX13eLuSeBG4BmC200XufsbZnaHmc3J1HxFROTwdOeBsiPm7k8DT3cY9tUuyp6fyVhERKRzGTsjEBGR/kGJQEQkyykRiIhkOSUCEZEsp0QgIpLllAhERLKcEoGISJZTIhARyXJKBCIiWU6JQEQkyykRiIhkOSUCEZEsp0QgIpLllAhERLKcEoGISJZTIhARyXJKBCIiWU6JQEQkyykRiIhkOSUCEZEsp0QgIpLllAhERLKcEoGISJZTIhARyXJKBCIiWU6JQEQkyykRiIhkOSUCEZEsp0QgIpLllAhERLKcEoGISJZTIhARyXJKBCIiWU6JQEQky2U0EZjZRWb2ppmtN7MFnYz/vJmtNrOVZrbEzMZnMh4RETlQxhKBmUWBhcDFwMnAJ8zs5A7FXgFmuvtpwBPAXZmKR0REOpfJM4IzgfXu/o67NwOPAXPTC7j7UnevD3tfBMZmMB4REelEJhPBGGBzWn9FOKwr1wO/72yEmc03s3IzK6+srOzBEEVE5Ji4WGxmVwMzgbs7G+/u97n7THefWVZW1rvBiYgMcLEMTnsLMC6tf2w4bD9m9kHgy8D73b0pg/GIiEgnMnlGsByYZGYTzSwH+DiwOL2AmU0HfgzMcfedGYxFRES6kLFE4O5J4EbgGWANsMjd3zCzO8xsTljsbqAI+KWZvWpmi7uYnIiIZEgmq4Zw96eBpzsM+2pa9z3KHNMAAA8ISURBVAczOX8RETm0jCaC3pJIJKioqKCxsbGvQ5FjRF5eHmPHjiUej/d1KCLHvAGRCCoqKiguLmbChAmYWV+HI33M3dm9ezcVFRVMnDixr8MROeYdE7ePHq3GxkaGDRumJCAAmBnDhg3TGaJINw2IRAAoCch+tD2IdN+ASQQiInJklAh6wN69e/nBD35wRN+95JJL2Lt3bw9HJCLSfUoEPeBgiSCZTB70u08//TRDhgzJRFhHxd1paWnp6zBEpBcMiLuG0n39qTdYvbW6R6d58uhBfO3Dp3Q5fsGCBbz99tucfvrpzJ49m0svvZR/+Zd/oaSkhLVr1/LWW2/xkY98hM2bN9PY2Mgtt9zC/PnzAZgwYQLl5eXU1tZy8cUXc+655/LXv/6VMWPG8Jvf/Ib8/Pz95vXUU09x55130tzczLBhw3j44YcZMWIEtbW13HTTTZSXl2NmfO1rX+Oyyy7jD3/4A1/60pdIpVKUlpayZMkSbr/9doqKivjCF74AwNSpU/ntb38LwD/8wz9w1llnsWLFCp5++mm+9a1vsXz5choaGrj88sv5+te/DsDy5cu55ZZbqKurIzc3lyVLlnDppZdy7733cvrppwNw7rnnsnDhQqZNm9aj60NEetaASwR94Vvf+harVq3i1VdfBeC5557j5ZdfZtWqVW23Lz7wwAMMHTqUhoYGZs2axWWXXcawYcP2m866det49NFHuf/++7nyyiv51a9+xdVXX71fmXPPPZcXX3wRM+MnP/kJd911F//2b//GN77xDQYPHszrr78OQFVVFZWVlXzmM59h2bJlTJw4kT179hxyWdatW8dDDz3E2WefDcA3v/lNhg4dSiqV4sILL2TlypVMnjyZefPm8fjjjzNr1iyqq6vJz8/n+uuv58EHH+See+7hrbfeorGxUUlApB8YcIngYEfuvenMM8/c7x72e++9lyeffBKAzZs3s27dugMSwcSJE9uOpmfMmMHGjRsPmG5FRQXz5s1j27ZtNDc3t83j2Wef5bHHHmsrV1JSwlNPPcV5553XVmbo0KGHjHv8+PFtSQBg0aJF3HfffSSTSbZt28bq1asxM0aNGsWsWbMAGDRoEABXXHEF3/jGN7j77rt54IEHuPbaaw85PxHpe7pGkCGFhYVt3c899xzPPvssL7zwAq+99hrTp0/v9B733Nzctu5oNNrp9YWbbrqJG2+8kddff50f//jHR3SvfCwW26/+P30a6XFv2LCB73znOyxZsoSVK1dy6aWXHnR+BQUFzJ49m9/85jcsWrSIq6666rBjE5Hep0TQA4qLi6mpqely/L59+ygpKaGgoIC1a9fy4osvHvG89u3bx5gxwft9Hnroobbhs2fPZuHChW39VVVVnH322SxbtowNGzYAtFUNTZgwgZdffhmAl19+uW18R9XV1RQWFjJ48GB27NjB738fvDfopJNOYtu2bSxfvhyAmpqatqT16U9/mptvvplZs2ZRUlJyxMspIr1HiaAHDBs2jHPOOYepU6fyxS9+8YDxF110EclkkilTprBgwYL9ql4O1+23384VV1zBjBkzKC0tbRv+la98haqqKqZOncq0adNYunQpZWVl3HfffXzsYx9j2rRpzJs3D4DLLruMPXv2cMopp/D973+fE088sdN5TZs2jenTpzN58mQ++clPcs455wCQk5PD448/zk033cS0adOYPXt225nCjBkzGDRoENddd90RL6OI9C5z976O4bDMnDnTy8vL9xu2Zs0apkyZ0kcRSbqtW7dy/vnns3btWiKRvj3O0HYh0s7MVrj7zM7G6YxAeszPf/5zzjrrLL75zW/2eRIQke4bcHcNSd+55ppruOaaa/o6DBE5TDpsExHJckoEIiJZTolARCTLKRGIiGQ5JYI+UlRUBAS3W15++eWdljn//PPpeKtsR/fccw/19fVt/WrWWkQOlxJBHxs9ejRPPPHEEX+/YyI4Vpu17oqauxbpewPv9tHfL4Dtr/fsNEeeChd/q8vRCxYsYNy4cXzuc58DaGvm+bOf/Sxz586lqqqKRCLBnXfeydy5c/f77saNG/nQhz7EqlWraGho4LrrruO1115j8uTJNDQ0tJW74YYbDmgO+t5772Xr1q1ccMEFlJaWsnTp0rZmrUtLS/nud7/LAw88AARNP9x6661s3LhRzV2LyH4GXiLoA/PmzePWW29tSwSLFi3imWeeIS8vjyeffJJBgwaxa9cuzj77bObMmdPl+3R/+MMfUlBQwJo1a1i5ciVnnHFG27jOmoO++eab+e53v8vSpUv3a24CYMWKFfzsZz/jb3/7G+7OWWedxfvf/35KSkrU3LWI7GfgJYKDHLlnyvTp09m5cydbt26lsrKSkpISxo0bRyKR4Etf+hLLli0jEomwZcsWduzYwciRIzudzrJly7j55psBOO200zjttNPaxnXWHHT6+I7+/Oc/89GPfrStNdGPfexjPP/888yZM0fNXYvIfgZeIugjV1xxBU888QTbt29va9zt4YcfprKykhUrVhCPx5kwYcIRNRvd2hz08uXLKSkp4dprrz2i6bTq2Nx1ehVUq5tuuonPf/7zzJkzh+eee47bb7/9sOdzuM1dd3f5OjZ3vWLFisOOTUTa6WJxD5k3bx6PPfYYTzzxBFdccQUQNBk9fPhw4vE4S5cuZdOmTQedxnnnnccjjzwCwKpVq1i5ciXQdXPQ0HUT2O973/v49a9/TX19PXV1dTz55JO8733v6/byqLlrkeyhRNBDTjnlFGpqahgzZgyjRo0C4KqrrqK8vJxTTz2Vn//850yePPmg07jhhhuora1lypQpfPWrX2XGjBlA181BA8yfP5+LLrqICy64YL9pnXHGGVx77bWceeaZnHXWWXz6059m+vTp3V4eNXctkj3UDLX0S91p7lrbhUg7NUMtA4qauxbpWbpYLP2OmrsW6VkD5nCqv1VxSWZpexDpvgGRCPLy8ti9e7f++QUIksDu3bvJy8vr61BE+oUBUTU0duxYKioqqKys7OtQ5BiRl5fH2LFj+zoMkX5hQCSCeDze9lSriIgcnoxWDZnZRWb2ppmtN7MFnYzPNbPHw/F/M7MJmYxHREQOlLFEYGZRYCFwMXAy8AkzO7lDseuBKnc/Afh34NuZikdERDqXyTOCM4H17v6OuzcDjwFzO5SZC7S2X/AEcKF11TSniIhkRCavEYwBNqf1VwBndVXG3ZNmtg8YBuxKL2Rm84H5YW+tmb15hDGVdpx2lsjG5c7GZYbsXO5sXGY4/OUe39WIfnGx2N3vA+472umYWXlXj1gPZNm43Nm4zJCdy52Nyww9u9yZrBraAoxL6x8bDuu0jJnFgMHA7gzGJCIiHWQyESwHJpnZRDPLAT4OLO5QZjHwj2H35cCfXE+FiYj0qoxVDYV1/jcCzwBR4AF3f8PM7gDK3X0x8FPgF2a2HthDkCwy6airl/qpbFzubFxmyM7lzsZlhh5c7n7XDLWIiPSsAdHWkIiIHDklAhGRLJc1ieBQzV0MBGY2zsyWmtlqM3vDzG4Jhw81sz+a2brw74B7ya+ZRc3sFTP7bdg/MWy2ZH3YjElOX8fY08xsiJk9YWZrzWyNmb03S9b1/wq371Vm9qiZ5Q209W1mD5jZTjNblTas03VrgXvDZV9pZmcc7vyyIhF0s7mLgSAJ3ObuJwNnA58Ll3MBsMTdJwFLwv6B5hZgTVr/t4F/D5svqSJozmSg+Q/gD+4+GZhGsPwDel2b2RjgZmCmu08luBHl4wy89f0gcFGHYV2t24uBSeFnPvDDw51ZViQCutfcRb/n7tvc/eWwu4ZgxzCG/ZvyeAj4SN9EmBlmNha4FPhJ2G/ABwiaLYGBucyDgfMI7rzD3ZvdfS8DfF2HYkB++OxRAbCNAba+3X0ZwZ2U6bpat3OBn3vgRWCImY06nPllSyLorLmLMX0US68IW3KdDvwNGOHu28JR24ERfRRWptwD/G+gJewfBux192TYPxDX90SgEvhZWCX2EzMrZICva3ffAnwHeJcgAewDVjDw1zd0vW6Pev+WLYkgq5hZEfAr4FZ3r04fFz6wN2DuGTazDwE73X1FX8fSy2LAGcAP3X06UEeHaqCBtq4BwnrxuQSJcDRQyIFVKANeT6/bbEkE3WnuYkAwszhBEnjY3f8rHLyj9VQx/Luzr+LLgHOAOWa2kaDK7wMEdedDwqoDGJjruwKocPe/hf1PECSGgbyuAT4IbHD3SndPAP9FsA0M9PUNXa/bo96/ZUsi6E5zF/1eWDf+U2CNu383bVR6Ux7/CPymt2PLFHf/P+4+1t0nEKzXP7n7VcBSgmZLYIAtM4C7bwc2m9lJ4aALgdUM4HUdehc428wKwu29dbkH9PoOdbVuFwPXhHcPnQ3sS6tC6h53z4oPcAnwFvA28OW+jidDy3guweniSuDV8HMJQZ35EmAd8CwwtK9jzdDynw/8Nuw+HngJWA/8Esjt6/gysLynA+Xh+v41UJIN6xr4OrAWWAX8AsgdaOsbeJTgGkiC4Ozv+q7WLWAEd0W+DbxOcEfVYc1PTUyIiGS5bKkaEhGRLigRiIhkOSUCEZEsp0QgIpLllAhERLKcEoFIB2aWMrNX0z491nCbmU1Ib1FS5FiQsVdVivRjDe5+el8HIdJbdEYg0k1mttHM7jKz183sJTM7IRw+wcz+FLYFv8TMjguHjzCzJ83stfDzd+GkomZ2f9im/n+bWX6fLZQISgQincnvUDU0L23cPnc/Ffg+QaunAN8DHnL304CHgXvD4fcC/8/dpxG0A/RGOHwSsNDdTwH2ApdleHlEDkpPFot0YGa17l7UyfCNwAfc/Z2wcb/t7j7MzHYBo9w9EQ7f5u6lZlYJjHX3prRpTAD+6MHLRTCzfwbi7n5n5pdMpHM6IxA5PN5F9+FoSutOoWt10seUCEQOz7y0vy+E3X8laPkU4Crg+bB7CXADtL1TeXBvBSlyOHQkInKgfDN7Na3/D+7eegtpiZmtJDiq/0Q47CaCN4V9keCtYdeFw28B7jOz6wmO/G8gaFFS5JiiawQi3RReI5jp7rv6OhaRnqSqIRGRLKczAhGRLKczAhGRLKdEICKS5ZQIRESynBKBiEiWUyIQEcly/x9mbNkkjDKhXAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFmTSaxYMw-"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "\n",
        "savepath= \"./gdrive/MyDrive/Models/KimVGG/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTzhGWp12XdE"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVh1s2cLCahH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85681295-289a-43a5-87aa-2ee3a8fc25f8"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids = input_ids, \n",
        "        tweet_img = tweet_imgs\n",
        "        )\n",
        "      preds = torch.round(outputs)\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Informative', 'Informative'], digits = 4))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not Informative     0.7718    0.7549    0.7633       457\n",
            "    Informative     0.8835    0.8927    0.8881       951\n",
            "\n",
            "       accuracy                         0.8480      1408\n",
            "      macro avg     0.8276    0.8238    0.8257      1408\n",
            "   weighted avg     0.8472    0.8480    0.8476      1408\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}