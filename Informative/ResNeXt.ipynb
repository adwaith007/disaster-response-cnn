{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNeXt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "ff1344d8-8815-4187-9eb9-5fecb62ba4cf"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzejvNI6kk3A"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3 sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwWQL0acqLuH",
        "outputId": "73a14d43-30df-4e4e-894e-60a0bec3d1c8"
      },
      "source": [
        "!ls gdrive/MyDrive/data_image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "california_wildfires  hurricane_irma   iraq_iran_earthquake  srilanka_floods\n",
            "hurricane_harvey      hurricane_maria  mexico_earthquake\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "4f762df7-8873-4b8e-a7c3-e7ee31e6ec2b"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"informative\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/Models/train.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/Models/val.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/Models/test.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, paths, targets):\n",
        "    self.paths = paths\n",
        "    self.targets = targets\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.paths)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    path = str(self.paths[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    return {\n",
        "      'tweet_image': img,\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    paths=df.image.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.resnext = torchvision.models.resnext50_32x4d(pretrained=True)\n",
        "    # for param in self.resnext.parameters():\n",
        "    #   param.requires_grad = False\n",
        "\n",
        "    self.linear1 = nn.Linear(1000, 256)\n",
        "    self.relu    = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=0.4)\n",
        "    self.linear2 = nn.Linear(256, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, tweet_img):\n",
        "    output = self.resnext(tweet_img)\n",
        "    linear1_output = self.linear1(output)\n",
        "    relu_output = self.relu(linear1_output)\n",
        "    dropout_output = self.dropout(relu_output)\n",
        "    linear2_output = self.linear2(dropout_output)\n",
        "    probas = self.sigmoid(linear2_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.round(outputs) == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 40\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.BCELoss().to(device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "953671b9-ba60-4c2b-e925-23d7e9d44f42"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/Models/ResNeXt/checkpoint.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "\n",
        "# print(start_epoch)\n",
        "# print(checkpoint['best_accuracy'])\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if 1:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/Models/ResNeXt/checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4191951837563357 accuracy 0.8098114779710447\n",
            "Val   loss 0.359236336350441 accuracy 0.8493324856961221\n",
            "\n",
            "Epoch 2/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1921736423345591 accuracy 0.9289657327361733\n",
            "Val   loss 0.45066380500793457 accuracy 0.8359821996185632\n",
            "\n",
            "Epoch 3/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0682694675117139 accuracy 0.9825018227267993\n",
            "Val   loss 0.6416543436050415 accuracy 0.8264462809917356\n",
            "\n",
            "Epoch 4/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.037138835813415344 accuracy 0.9938548067909593\n",
            "Val   loss 0.7340599846839905 accuracy 0.82453909726637\n",
            "\n",
            "Epoch 5/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.027923815255554468 accuracy 0.9975002603895428\n",
            "Val   loss 0.871400101184845 accuracy 0.8397965670692944\n",
            "\n",
            "Epoch 6/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.014577438340309178 accuracy 0.9988542860118738\n",
            "Val   loss 0.9097359067201615 accuracy 0.8423394787031151\n",
            "\n",
            "Epoch 7/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.011197188455063699 accuracy 0.9989584418289761\n",
            "Val   loss 1.0152589547634125 accuracy 0.8385251112523839\n",
            "\n",
            "Epoch 8/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.010867652897484021 accuracy 0.9997916883657952\n",
            "Val   loss 0.9237770628929138 accuracy 0.8334392879847425\n",
            "\n",
            "Epoch 9/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0029685811565672193 accuracy 0.9998958441828976\n",
            "Val   loss 1.081264226436615 accuracy 0.8353464717101081\n",
            "\n",
            "Epoch 10/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0016101765123789395 accuracy 0.9996875325486928\n",
            "Val   loss 1.1530718433856963 accuracy 0.8366179275270185\n",
            "\n",
            "Epoch 11/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0016155607859730402 accuracy 0.9996875325486928\n",
            "Val   loss 1.1515252208709716 accuracy 0.8328035600762873\n",
            "\n",
            "Epoch 12/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0008486726875698336 accuracy 1.0\n",
            "Val   loss 1.322331472635269 accuracy 0.8353464717101081\n",
            "\n",
            "Epoch 13/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.00019171831731518556 accuracy 1.0\n",
            "Val   loss 1.2253942358493806 accuracy 0.8359821996185632\n",
            "\n",
            "Epoch 14/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0001558633830216615 accuracy 1.0\n",
            "Val   loss 1.2222156250476837 accuracy 0.8340750158931978\n",
            "\n",
            "Epoch 15/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0002412728363257179 accuracy 1.0\n",
            "Val   loss 1.4408791196346282 accuracy 0.8321678321678322\n",
            "\n",
            "Epoch 16/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0004105487722887685 accuracy 0.9997916883657952\n",
            "Val   loss 1.4543666851520538 accuracy 0.8302606484424666\n",
            "\n",
            "Epoch 17/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.00034220091248358427 accuracy 0.9998958441828976\n",
            "Val   loss 1.6246296095848083 accuracy 0.8353464717101081\n",
            "\n",
            "Epoch 18/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0005097452160091944 accuracy 0.9998958441828976\n",
            "Val   loss 1.6995680546760559 accuracy 0.8308963763509218\n",
            "\n",
            "Epoch 19/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0003192464777902875 accuracy 0.9998958441828976\n",
            "Val   loss 1.652351529598236 accuracy 0.8340750158931978\n",
            "\n",
            "Epoch 20/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.00021645112837183317 accuracy 1.0\n",
            "Val   loss 1.8534871554374694 accuracy 0.8340750158931978\n",
            "\n",
            "Epoch 21/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.00037781898668945603 accuracy 1.0\n",
            "Val   loss 1.9257119250297547 accuracy 0.8347107438016529\n",
            "\n",
            "Epoch 22/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.000894738485661019 accuracy 0.9998958441828976\n",
            "Val   loss 1.5858740949630736 accuracy 0.8328035600762873\n",
            "\n",
            "Epoch 23/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.00033340536051492255 accuracy 0.9997916883657952\n",
            "Val   loss 1.6883894443511962 accuracy 0.8296249205340115\n",
            "\n",
            "Epoch 24/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.002545353688413867 accuracy 0.9997916883657952\n",
            "Val   loss 1.6300993585586547 accuracy 0.8302606484424666\n",
            "\n",
            "Epoch 25/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.392917354544933e-05 accuracy 1.0\n",
            "Val   loss 1.6351622939109802 accuracy 0.8308963763509218\n",
            "\n",
            "Epoch 26/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.371419522462586e-05 accuracy 1.0\n",
            "Val   loss 1.7874820816516876 accuracy 0.8302606484424666\n",
            "\n",
            "Epoch 27/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.6428357604964303e-05 accuracy 1.0\n",
            "Val   loss 1.8466280317306518 accuracy 0.8302606484424666\n",
            "\n",
            "Epoch 28/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 9.765402140731006e-06 accuracy 1.0\n",
            "Val   loss 1.8529631471633912 accuracy 0.8308963763509218\n",
            "\n",
            "Epoch 29/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.5127550953728988e-05 accuracy 1.0\n",
            "Val   loss 1.8590006375312804 accuracy 0.8321678321678322\n",
            "\n",
            "Epoch 30/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.559964730118218e-05 accuracy 1.0\n",
            "Val   loss 1.8673259246349334 accuracy 0.8321678321678322\n",
            "\n",
            "Epoch 31/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.3022846449241223e-05 accuracy 1.0\n",
            "Val   loss 1.873494449853897 accuracy 0.8328035600762873\n",
            "\n",
            "Epoch 32/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 7.588559036029263e-06 accuracy 1.0\n",
            "Val   loss 1.8778550255298614 accuracy 0.8328035600762873\n",
            "\n",
            "Epoch 33/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 8.51094330300839e-06 accuracy 1.0\n",
            "Val   loss 1.8811913847923278 accuracy 0.8328035600762873\n",
            "\n",
            "Epoch 34/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 8.676504003745244e-06 accuracy 1.0\n",
            "Val   loss 1.9372424674034119 accuracy 0.8328035600762873\n",
            "\n",
            "Epoch 35/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 9.579629710250274e-06 accuracy 1.0\n",
            "Val   loss 1.9402770948410035 accuracy 0.8328035600762873\n",
            "\n",
            "Epoch 36/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.248575978663645e-05 accuracy 1.0\n",
            "Val   loss 1.9434295392036438 accuracy 0.8328035600762873\n",
            "\n",
            "Epoch 37/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.5149511530732664e-05 accuracy 1.0\n",
            "Val   loss 1.9467819154262542 accuracy 0.8328035600762873\n",
            "\n",
            "Epoch 38/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 9.382020926368087e-06 accuracy 1.0\n",
            "Val   loss 1.949891300201416 accuracy 0.8321678321678322\n",
            "\n",
            "Epoch 39/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 5.536652491968355e-06 accuracy 1.0\n",
            "Val   loss 1.9513762080669403 accuracy 0.8321678321678322\n",
            "\n",
            "Epoch 40/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 8.539467151404217e-06 accuracy 1.0\n",
            "Val   loss 1.9516688251495362 accuracy 0.8321678321678322\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Taz58ppcm8n8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "859a36b4-cce4-4338-e4ab-3a9b655b63c1"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/ResNeXt/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0f4fc0e07a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m state = {\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m }\n",
            "\u001b[0;31mNameError\u001b[0m: name 'start_epoch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScOj15BovCww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "058d6bb5-48dd-43f0-fe50-0f5600cbec7b"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTghsXN8vEpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19f3377-ad4e-48ff-bd44-94759ea21120"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      targets = d[\"targets\"].reshape(-1, 1).float()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(tweet_img=tweet_imgs)\n",
        "      preds = torch.round(outputs)\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Informative', 'Informative']))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not Informative       0.79      0.75      0.77       504\n",
            "    Informative       0.88      0.90      0.89      1030\n",
            "\n",
            "       accuracy                           0.85      1534\n",
            "      macro avg       0.83      0.82      0.83      1534\n",
            "   weighted avg       0.85      0.85      0.85      1534\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}