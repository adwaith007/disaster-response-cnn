{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLNet_and_Resnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9bb261a3213d4956b9d4f4d2557cd681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7171a2e6676549f0a2a54303111184b9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8286416d685d47e1a4eaaa874103e4f6",
              "IPY_MODEL_e9a672b43fb64337ac7bddb831496689"
            ]
          }
        },
        "7171a2e6676549f0a2a54303111184b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8286416d685d47e1a4eaaa874103e4f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_95d7a43ae21f4bfe83a0dd3f86709927",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 798011,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 798011,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_68fdec7935a2488aa744eaff07ab8ebe"
          }
        },
        "e9a672b43fb64337ac7bddb831496689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e353b5675c0b4719815cff69f4cd34ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 798k/798k [00:01&lt;00:00, 579kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2b780b12424f4be08a7b47867f8daed0"
          }
        },
        "95d7a43ae21f4bfe83a0dd3f86709927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "68fdec7935a2488aa744eaff07ab8ebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e353b5675c0b4719815cff69f4cd34ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2b780b12424f4be08a7b47867f8daed0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3edf6f912418413493fe486dff7d28aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1c261d76f94b40198161733c8bdb068a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5bf753d775844f1e97ab52ff941e7c84",
              "IPY_MODEL_32fa887a322f40eba21a632ba9f21ed9"
            ]
          }
        },
        "1c261d76f94b40198161733c8bdb068a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5bf753d775844f1e97ab52ff941e7c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7ea2d5afe50a440eaa9e35c3f0eb6800",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1382015,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1382015,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d408550094a4f08b2492dea29aa40a5"
          }
        },
        "32fa887a322f40eba21a632ba9f21ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b50e98f88d234d0a89d7035b8e21135a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.38M/1.38M [00:10&lt;00:00, 136kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0426cbe96fa14d59b007f1b2df42b22a"
          }
        },
        "7ea2d5afe50a440eaa9e35c3f0eb6800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d408550094a4f08b2492dea29aa40a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b50e98f88d234d0a89d7035b8e21135a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0426cbe96fa14d59b007f1b2df42b22a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2fcbe8b502ce4714916d37a1f6e355bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1b0d331d0887479daaeff5b3cb27052c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_60dd15a8bea2431baf6274e192ab9898",
              "IPY_MODEL_b474940262ac4230bdfd2c29759a40c4"
            ]
          }
        },
        "1b0d331d0887479daaeff5b3cb27052c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60dd15a8bea2431baf6274e192ab9898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1d62b05b55ec4e5192406c9a731fc952",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 760,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 760,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b0ae25635f0147c38815093991139689"
          }
        },
        "b474940262ac4230bdfd2c29759a40c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a09aafc1b7544d8e8f34d2476892101a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 760/760 [00:09&lt;00:00, 81.5B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b5ef4620575f4707b3fbc7488f7feb77"
          }
        },
        "1d62b05b55ec4e5192406c9a731fc952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b0ae25635f0147c38815093991139689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a09aafc1b7544d8e8f34d2476892101a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b5ef4620575f4707b3fbc7488f7feb77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5cb6e82a6dad4794aa3d0d2c9cab9ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ff062d0185314d6aab7bbd5fdf2c991b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f554b03394e4402391477f21379d27bc",
              "IPY_MODEL_8675a2ad36b44f329022e0fe64ed2592"
            ]
          }
        },
        "ff062d0185314d6aab7bbd5fdf2c991b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f554b03394e4402391477f21379d27bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_01cc0454692e412d84145c9a403b8643",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 467042463,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 467042463,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_719a56ff5887483990870b7c808fd486"
          }
        },
        "8675a2ad36b44f329022e0fe64ed2592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f01ad31945084a18acd27791531895ac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 467M/467M [00:08&lt;00:00, 54.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eeb63e8f72654eebbf5877b45396464a"
          }
        },
        "01cc0454692e412d84145c9a403b8643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "719a56ff5887483990870b7c808fd486": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f01ad31945084a18acd27791531895ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eeb63e8f72654eebbf5877b45396464a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ffaa6e04530f45239a894384c65218b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7fc585da15584bd5b64aac3c3f74f568",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a25de76674f049cf8639b9c3846b9cea",
              "IPY_MODEL_4d80c8b6836f4a33af43523cf3da43b3"
            ]
          }
        },
        "7fc585da15584bd5b64aac3c3f74f568": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a25de76674f049cf8639b9c3846b9cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bcb49a85144947a28cc5404d00045ad7",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d04a0483918471ca445d261cf5b7dba"
          }
        },
        "4d80c8b6836f4a33af43523cf3da43b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_79af394a14ca45e48d976fb15a06f80e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [26:11&lt;00:00, 29.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed0c481d1f684aefbd43a70c200f43c4"
          }
        },
        "bcb49a85144947a28cc5404d00045ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d04a0483918471ca445d261cf5b7dba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "79af394a14ca45e48d976fb15a06f80e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed0c481d1f684aefbd43a70c200f43c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "7e7b3b6f-94bc-4776-dada-e8bc64f2061a"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzejvNI6kk3A"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3 sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "4cd4b363-c59d-41fc-ba48-31b75b7ca21b"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"not_humanitarian\":\n",
        "    return 0\n",
        "  elif text == \"infrastructure_and_utility_damage\":\n",
        "    return 1\n",
        "  elif text == \"other_relevant_information\":\n",
        "    return 2\n",
        "  elif text == \"rescue_volunteering_or_donation_effort\":\n",
        "    return 3\n",
        "  elif text == \"affected_individuals\":\n",
        "    return 4\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/Models/train.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'tweet_text', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['tweet_text'] = df_train['tweet_text'].apply(clean_text)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/Models/val.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'tweet_text', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['tweet_text'] = df_val['tweet_text'].apply(clean_text)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/Models/test.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'tweet_text', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(clean_text)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, targets, paths, tokenizer, max_len):\n",
        "    self.tweets = tweets\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.paths = paths\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    tweet = str(self.tweets[item])\n",
        "    target = self.targets[item]\n",
        "    path = str(self.paths[item])\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "      truncation = True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'tweet_text': tweet,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long),\n",
        "      'tweet_image': img\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    tweets=df.tweet_text.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "    paths=df.image.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.xlnet = XLNetModel.from_pretrained(\"xlnet-base-cased\")\n",
        "    for param in self.xlnet.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.resnet = torchvision.models.resnet18(pretrained=True)\n",
        "    for param in self.resnet.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(self.xlnet.config.hidden_size*MAX_LEN + 1000)\n",
        "\n",
        "    self.linear1 = nn.Linear(self.xlnet.config.hidden_size*MAX_LEN + 1000, 1000)\n",
        "    self.relu1    = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(p=0.4)\n",
        "\n",
        "    self.linear2 = nn.Linear(1000, 500)\n",
        "    self.relu2    = nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "    self.linear3 = nn.Linear(500, 250)\n",
        "    self.relu3    = nn.ReLU()\n",
        "    self.dropout3 = nn.Dropout(p=0.1)\n",
        "\n",
        "    self.linear4 = nn.Linear(250, 125)\n",
        "    self.relu4    = nn.ReLU()\n",
        "    self.dropout4 = nn.Dropout(p=0.02)\n",
        "\n",
        "    self.linear5 = nn.Linear(125, 5)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask, tweet_img):\n",
        "    text_output = self.xlnet(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False\n",
        "    )\n",
        "\n",
        "    text_output = text_output[0]\n",
        "    text_output = torch.reshape(text_output, (-1, MAX_LEN*self.xlnet.config.hidden_size))\n",
        "\n",
        "    image_output = self.resnet(tweet_img)\n",
        "    merged_output = torch.cat((text_output, image_output), dim=1)\n",
        "    bn_output = self.bn(merged_output)\n",
        "\n",
        "    linear1_output = self.linear1(bn_output)\n",
        "    relu1_output = self.relu1(linear1_output)\n",
        "    dropout1_output = self.dropout1(relu1_output)\n",
        "\n",
        "    linear2_output = self.linear2(dropout1_output)\n",
        "    relu2_output = self.relu2(linear2_output)\n",
        "    dropout2_output = self.dropout2(relu2_output)\n",
        "\n",
        "    linear3_output = self.linear3(dropout2_output)\n",
        "    relu3_output = self.relu3(linear3_output)\n",
        "    dropout3_output = self.dropout3(relu3_output)\n",
        "\n",
        "    linear4_output = self.linear4(dropout3_output)\n",
        "    relu4_output = self.relu4(linear4_output)\n",
        "    dropout4_output = self.dropout4(relu4_output)\n",
        "\n",
        "    linear5_output = self.linear5(dropout4_output)\n",
        "\n",
        "\n",
        "    probas = self.softmax(linear5_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].long()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.max(outputs, dim=1).indices == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].long()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.max(outputs, dim=1).indices == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279,
          "referenced_widgets": [
            "9bb261a3213d4956b9d4f4d2557cd681",
            "7171a2e6676549f0a2a54303111184b9",
            "8286416d685d47e1a4eaaa874103e4f6",
            "e9a672b43fb64337ac7bddb831496689",
            "95d7a43ae21f4bfe83a0dd3f86709927",
            "68fdec7935a2488aa744eaff07ab8ebe",
            "e353b5675c0b4719815cff69f4cd34ce",
            "2b780b12424f4be08a7b47867f8daed0",
            "3edf6f912418413493fe486dff7d28aa",
            "1c261d76f94b40198161733c8bdb068a",
            "5bf753d775844f1e97ab52ff941e7c84",
            "32fa887a322f40eba21a632ba9f21ed9",
            "7ea2d5afe50a440eaa9e35c3f0eb6800",
            "5d408550094a4f08b2492dea29aa40a5",
            "b50e98f88d234d0a89d7035b8e21135a",
            "0426cbe96fa14d59b007f1b2df42b22a",
            "2fcbe8b502ce4714916d37a1f6e355bc",
            "1b0d331d0887479daaeff5b3cb27052c",
            "60dd15a8bea2431baf6274e192ab9898",
            "b474940262ac4230bdfd2c29759a40c4",
            "1d62b05b55ec4e5192406c9a731fc952",
            "b0ae25635f0147c38815093991139689",
            "a09aafc1b7544d8e8f34d2476892101a",
            "b5ef4620575f4707b3fbc7488f7feb77",
            "5cb6e82a6dad4794aa3d0d2c9cab9ef0",
            "ff062d0185314d6aab7bbd5fdf2c991b",
            "f554b03394e4402391477f21379d27bc",
            "8675a2ad36b44f329022e0fe64ed2592",
            "01cc0454692e412d84145c9a403b8643",
            "719a56ff5887483990870b7c808fd486",
            "f01ad31945084a18acd27791531895ac",
            "eeb63e8f72654eebbf5877b45396464a",
            "ffaa6e04530f45239a894384c65218b1",
            "7fc585da15584bd5b64aac3c3f74f568",
            "a25de76674f049cf8639b9c3846b9cea",
            "4d80c8b6836f4a33af43523cf3da43b3",
            "bcb49a85144947a28cc5404d00045ad7",
            "3d04a0483918471ca445d261cf5b7dba",
            "79af394a14ca45e48d976fb15a06f80e",
            "ed0c481d1f684aefbd43a70c200f43c4"
          ]
        },
        "outputId": "27d9f7aa-9c8b-42e6-81bd-d2bb78a27fb5"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "MAX_LEN = 150\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'xlnet-base-cased'\n",
        "tokenizer = XLNetTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 70\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-2)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bb261a3213d4956b9d4f4d2557cd681",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3edf6f912418413493fe486dff7d28aa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1382015.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fcbe8b502ce4714916d37a1f6e355bc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cb6e82a6dad4794aa3d0d2c9cab9ef0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=467042463.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffaa6e04530f45239a894384c65218b1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "f1526a20-cce0-45de-cb80-f5b199b02e1f"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/Models/XLNetResNet/checkpoint-50.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "# best_accuracy = checkpoint['best_accuracy']\n",
        "\n",
        "# print(start_epoch)\n",
        "# print(best_accuracy)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {start_epoch + epoch + 1}/{start_epoch + EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/Models/XLNetResNet/checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc\n",
        "\n",
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/XLNetResNet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)\n",
        "\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 21.551883469025295 accuracy 0.4268690825987594\n",
            "Val   loss 1.4180526733398438 accuracy 0.39378757515030055\n",
            "\n",
            "Epoch 2/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.017608384291331 accuracy 0.6118184786157362\n",
            "Val   loss 0.9727209508419037 accuracy 0.625250501002004\n",
            "\n",
            "Epoch 3/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8691708594560623 accuracy 0.677930133855697\n",
            "Val   loss 1.1655213832855225 accuracy 0.5661322645290581\n",
            "\n",
            "Epoch 4/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.7855197141567866 accuracy 0.7060071825008162\n",
            "Val   loss 0.8750589191913605 accuracy 0.6803607214428857\n",
            "\n",
            "Epoch 5/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.7325494339068731 accuracy 0.7370225269343781\n",
            "Val   loss 0.7741808593273163 accuracy 0.7314629258517034\n",
            "\n",
            "Epoch 6/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.7020055651664734 accuracy 0.7491021873979758\n",
            "Val   loss 0.767487496137619 accuracy 0.6933867735470941\n",
            "\n",
            "Epoch 7/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6486158867677053 accuracy 0.7704864511916422\n",
            "Val   loss 0.7613328993320465 accuracy 0.7374749498997996\n",
            "\n",
            "Epoch 8/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.630839983622233 accuracy 0.7832190662748939\n",
            "Val   loss 0.6917639672756195 accuracy 0.7204408817635269\n",
            "\n",
            "Epoch 9/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6196202586094538 accuracy 0.7908912830558276\n",
            "Val   loss 0.6644608378410339 accuracy 0.7575150300601202\n",
            "\n",
            "Epoch 10/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5941303571065267 accuracy 0.7966046359777995\n",
            "Val   loss 0.6980237066745758 accuracy 0.7565130260521041\n",
            "\n",
            "Epoch 11/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5700908253590266 accuracy 0.8031341821743389\n",
            "Val   loss 0.672845870256424 accuracy 0.7595190380761523\n",
            "\n",
            "Epoch 12/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5895549108584722 accuracy 0.7998694090760692\n",
            "Val   loss 0.7488413155078888 accuracy 0.7414829659318637\n",
            "\n",
            "Epoch 13/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5878963793317477 accuracy 0.8005223636957232\n",
            "Val   loss 0.6684592664241791 accuracy 0.7695390781563125\n",
            "\n",
            "Epoch 14/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5172934010624886 accuracy 0.8197845249755141\n",
            "Val   loss 0.6857520341873169 accuracy 0.7725450901803607\n",
            "\n",
            "Epoch 15/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.536335734029611 accuracy 0.8194580476656872\n",
            "Val   loss 0.7233397364616394 accuracy 0.7615230460921844\n",
            "\n",
            "Epoch 16/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5154489229122797 accuracy 0.8199477636304277\n",
            "Val   loss 0.6984838843345642 accuracy 0.7755511022044087\n",
            "\n",
            "Epoch 17/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5042222738265991 accuracy 0.8276199804113614\n",
            "Val   loss 0.6964529752731323 accuracy 0.7865731462925851\n",
            "\n",
            "Epoch 18/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5049057801564535 accuracy 0.8307215148547176\n",
            "Val   loss 0.7701282501220703 accuracy 0.7725450901803607\n",
            "\n",
            "Epoch 19/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.477946013212204 accuracy 0.8321906627489389\n",
            "Val   loss 0.6458852887153625 accuracy 0.7845691382765531\n",
            "\n",
            "Epoch 20/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5001148929198583 accuracy 0.8277832190662748\n",
            "Val   loss 0.6271138191223145 accuracy 0.7955911823647294\n",
            "\n",
            "Epoch 21/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.47600535303354263 accuracy 0.8421482206986615\n",
            "Val   loss 0.7002688348293304 accuracy 0.7895791583166332\n",
            "\n",
            "Epoch 22/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.46755149712165195 accuracy 0.8447600391772772\n",
            "Val   loss 0.6833544075489044 accuracy 0.7815631262525049\n",
            "\n",
            "Epoch 23/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4488702267408371 accuracy 0.8455762324518445\n",
            "Val   loss 0.7303542494773865 accuracy 0.783567134268537\n",
            "\n",
            "Epoch 24/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4512825558582942 accuracy 0.8454129937969311\n",
            "Val   loss 0.6252974569797516 accuracy 0.7925851703406813\n",
            "\n",
            "Epoch 25/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4594723880290985 accuracy 0.8405158341495266\n",
            "Val   loss 0.6712229549884796 accuracy 0.7775551102204408\n",
            "\n",
            "Epoch 26/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4629196524620056 accuracy 0.8426379366634019\n",
            "Val   loss 0.7081781327724457 accuracy 0.8066132264529058\n",
            "\n",
            "Epoch 27/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.44759701192379 accuracy 0.846718903036239\n",
            "Val   loss 0.6800163686275482 accuracy 0.8056112224448897\n",
            "\n",
            "Epoch 28/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4280763690670331 accuracy 0.8576558929154423\n",
            "Val   loss 0.6437831223011017 accuracy 0.7945891783567134\n",
            "\n",
            "Epoch 29/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.42198316752910614 accuracy 0.85145282402873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4162631829579671 accuracy 0.8625530525628469\n",
            "Val   loss 0.6666645407676697 accuracy 0.7955911823647294\n",
            "\n",
            "Epoch 31/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4280366450548172 accuracy 0.8480248122755468\n",
            "Val   loss 0.809023916721344 accuracy 0.7805611222444889\n",
            "\n",
            "Epoch 32/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.43627028663953143 accuracy 0.8490042442050277\n",
            "Val   loss 0.7686569094657898 accuracy 0.785571142284569\n",
            "\n",
            "Epoch 33/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4253321836392085 accuracy 0.8610839046686255\n",
            "Val   loss 0.6375093460083008 accuracy 0.7945891783567134\n",
            "\n",
            "Epoch 34/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3891970341404279 accuracy 0.8676134508651648\n",
            "Val   loss 0.6519068777561188 accuracy 0.8076152304609218\n",
            "\n",
            "Epoch 35/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.37574126571416855 accuracy 0.870388507998694\n",
            "Val   loss 0.7146957218647003 accuracy 0.812625250501002\n",
            "\n",
            "Epoch 36/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.38961397111415863 accuracy 0.866960496245511\n",
            "Val   loss 0.7464669048786163 accuracy 0.8106212424849699\n",
            "\n",
            "Epoch 37/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3985596646865209 accuracy 0.860920666013712\n",
            "Val   loss 0.7124713361263275 accuracy 0.7895791583166332\n",
            "\n",
            "Epoch 38/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.36350558201471966 accuracy 0.8733268037871368\n",
            "Val   loss 0.7174020409584045 accuracy 0.8006012024048096\n",
            "\n",
            "Epoch 39/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.34983881562948227 accuracy 0.8762650995755795\n",
            "Val   loss 0.8474795520305634 accuracy 0.7875751503006011\n",
            "\n",
            "Epoch 40/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3780964861313502 accuracy 0.8676134508651648\n",
            "Val   loss 0.7827459573745728 accuracy 0.8036072144288576\n",
            "\n",
            "Epoch 41/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3807651648918788 accuracy 0.8788769180541952\n",
            "Val   loss 0.7177786231040955 accuracy 0.8096192384769538\n",
            "\n",
            "Epoch 42/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.35818877071142197 accuracy 0.881162259222984\n",
            "Val   loss 0.809315413236618 accuracy 0.7985971943887775\n",
            "\n",
            "Epoch 43/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.36974500864744186 accuracy 0.8765915768854065\n",
            "Val   loss 0.6891208589076996 accuracy 0.7985971943887775\n",
            "\n",
            "Epoch 44/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.33043884485960007 accuracy 0.8862226575253019\n",
            "Val   loss 0.6700053513050079 accuracy 0.8016032064128256\n",
            "\n",
            "Epoch 45/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.323943759004275 accuracy 0.8901403852432256\n",
            "Val   loss 0.7491597831249237 accuracy 0.813627254509018\n",
            "\n",
            "Epoch 46/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.32282355676094693 accuracy 0.8867123734900424\n",
            "Val   loss 0.751847505569458 accuracy 0.8006012024048096\n",
            "\n",
            "Epoch 47/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3196876496076584 accuracy 0.890956578517793\n",
            "Val   loss 0.8167333006858826 accuracy 0.8066132264529058\n",
            "\n",
            "Epoch 48/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.30379785348971683 accuracy 0.8924257264120143\n",
            "Val   loss 0.7480177879333496 accuracy 0.812625250501002\n",
            "\n",
            "Epoch 49/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.30712612221638363 accuracy 0.8938948743062357\n",
            "Val   loss 0.6922239363193512 accuracy 0.8066132264529058\n",
            "\n",
            "Epoch 50/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2819897457957268 accuracy 0.900750897812602\n",
            "Val   loss 0.8565674722194672 accuracy 0.8036072144288576\n",
            "\n",
            "Epoch 51/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.29175323247909546 accuracy 0.9000979431929481\n",
            "Val   loss 0.720057874917984 accuracy 0.8056112224448897\n",
            "\n",
            "Epoch 52/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.31318243344624835 accuracy 0.8932419196865817\n",
            "Val   loss 0.7178564071655273 accuracy 0.8016032064128256\n",
            "\n",
            "Epoch 53/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2930869037906329 accuracy 0.8920992491021874\n",
            "Val   loss 0.71998330950737 accuracy 0.8046092184368737\n",
            "\n",
            "Epoch 54/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.28210676833987236 accuracy 0.8997714658831211\n",
            "Val   loss 0.7617705464363098 accuracy 0.7945891783567134\n",
            "\n",
            "Epoch 55/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2801639127234618 accuracy 0.8997714658831211\n",
            "Val   loss 0.7400650084018707 accuracy 0.8076152304609218\n",
            "\n",
            "Epoch 56/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2827470041811466 accuracy 0.90581129611492\n",
            "Val   loss 0.7112817764282227 accuracy 0.8066132264529058\n",
            "\n",
            "Epoch 57/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.26702215522527695 accuracy 0.9079333986287953\n",
            "Val   loss 0.7720407545566559 accuracy 0.7975951903807614\n",
            "\n",
            "Epoch 58/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2662176949282487 accuracy 0.9100555011426705\n",
            "Val   loss 0.7128517627716064 accuracy 0.8006012024048096\n",
            "\n",
            "Epoch 59/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.263430184374253 accuracy 0.915932092719556\n",
            "Val   loss 0.730347067117691 accuracy 0.81563126252505\n",
            "\n",
            "Epoch 60/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2577272007862727 accuracy 0.9067907280444009\n",
            "Val   loss 0.7540527880191803 accuracy 0.8116232464929859\n",
            "\n",
            "Epoch 61/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2567705685893695 accuracy 0.910218739797584\n",
            "Val   loss 0.7079009115695953 accuracy 0.8096192384769538\n",
            "\n",
            "Epoch 62/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.26308051993449527 accuracy 0.9089128305582762\n",
            "Val   loss 0.6956637501716614 accuracy 0.8116232464929859\n",
            "\n",
            "Epoch 63/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23513328159848848 accuracy 0.9216454456415278\n",
            "Val   loss 0.7784313559532166 accuracy 0.8096192384769538\n",
            "\n",
            "Epoch 64/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24593995635708174 accuracy 0.9146261834802482\n",
            "Val   loss 0.7809551358222961 accuracy 0.8106212424849699\n",
            "\n",
            "Epoch 65/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2338273674249649 accuracy 0.9170747633039503\n",
            "Val   loss 0.745591402053833 accuracy 0.812625250501002\n",
            "\n",
            "Epoch 66/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23012768601377806 accuracy 0.9174012406137774\n",
            "Val   loss 0.7390166521072388 accuracy 0.8116232464929859\n",
            "\n",
            "Epoch 67/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2364634871482849 accuracy 0.9214822069866144\n",
            "Val   loss 0.7437699735164642 accuracy 0.8086172344689379\n",
            "\n",
            "Epoch 68/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.22734871009985605 accuracy 0.9211557296767875\n",
            "Val   loss 0.755601167678833 accuracy 0.8116232464929859\n",
            "\n",
            "Epoch 69/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23158777132630348 accuracy 0.9195233431276526\n",
            "Val   loss 0.7662545442581177 accuracy 0.8106212424849699\n",
            "\n",
            "Epoch 70/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24576334406932196 accuracy 0.9167482859941234\n",
            "Val   loss 0.7624479234218597 accuracy 0.8086172344689379\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFmTSaxYMw-"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/XLNetResNet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScOj15BovCww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "96f2dcd3-2554-4157-8847-17ba65146aaa"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTghsXN8vEpo",
        "outputId": "f84b4e42-5ed0-4468-96db-0c25756a6ed0"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].long()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "      preds = torch.max(outputs, dim=1).indices\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['not_humanitarian', 'infrastructure_and_utility_damage', 'other_relevant_information', 'rescue_volunteering_or_donation_effort', 'affected_individuals'], digits = 4))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                        precision    recall  f1-score   support\n",
            "\n",
            "                      not_humanitarian     0.8474    0.8591    0.8532       504\n",
            "     infrastructure_and_utility_damage     0.7674    0.8148    0.7904        81\n",
            "            other_relevant_information     0.8182    0.8043    0.8112       235\n",
            "rescue_volunteering_or_donation_effort     0.6880    0.6825    0.6853       126\n",
            "                  affected_individuals     0.0000    0.0000    0.0000         9\n",
            "\n",
            "                              accuracy                         0.8105       955\n",
            "                             macro avg     0.6242    0.6321    0.6280       955\n",
            "                          weighted avg     0.8044    0.8105    0.8073       955\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}