{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Roberta_and_Densenet(h).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "40ca1d70-d5f2-408e-a90c-80e2aac9309c"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzejvNI6kk3A"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "22d76d8b-0c5f-4f6c-9e9e-c919e012faeb"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"not_humanitarian\":\n",
        "    return 0\n",
        "  elif text == \"infrastructure_and_utility_damage\":\n",
        "    return 1\n",
        "  elif text == \"other_relevant_information\":\n",
        "    return 2\n",
        "  elif text == \"rescue_volunteering_or_donation_effort\":\n",
        "    return 3\n",
        "  elif text == \"affected_individuals\":\n",
        "    return 4\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/Models/train_hum.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'tweet_text', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['tweet_text'] = df_train['tweet_text'].apply(clean_text)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/Models/val_hum.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'tweet_text', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['tweet_text'] = df_val['tweet_text'].apply(clean_text)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/Models/test_hum.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'tweet_text', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(clean_text)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, targets, paths, tokenizer, max_len):\n",
        "    self.tweets = tweets\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.paths = paths\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    tweet = str(self.tweets[item])\n",
        "    target = self.targets[item]\n",
        "    path = str(self.paths[item])\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "      truncation = True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'tweet_text': tweet,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long),\n",
        "      'tweet_image': img\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    tweets=df.tweet_text.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "    paths=df.image.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "    for param in self.roberta.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.densenet = torchvision.models.densenet161(pretrained=True)\n",
        "    for param in self.densenet.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(self.roberta.config.hidden_size + 1000)\n",
        "\n",
        "    self.linear1 = nn.Linear(self.roberta.config.hidden_size + 1000, 1000)\n",
        "    self.relu1    = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(p=0.4)\n",
        "\n",
        "    self.linear2 = nn.Linear(1000, 500)\n",
        "    self.relu2    = nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "    self.linear3 = nn.Linear(500, 250)\n",
        "    self.relu3    = nn.ReLU()\n",
        "    self.dropout3 = nn.Dropout(p=0.1)\n",
        "\n",
        "    self.linear4 = nn.Linear(250, 125)\n",
        "    self.relu4    = nn.ReLU()\n",
        "    self.dropout4 = nn.Dropout(p=0.02)\n",
        "\n",
        "    self.linear5 = nn.Linear(125, 5)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask, tweet_img):\n",
        "    output = self.roberta(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False\n",
        "    )\n",
        "    output = output[0]\n",
        "    text_output = torch.mean(output, 1)\n",
        "    \n",
        "    image_output = self.densenet(tweet_img)\n",
        "    merged_output = torch.cat((text_output, image_output), dim=1)\n",
        "    bn_output = self.bn(merged_output)\n",
        "\n",
        "    linear1_output = self.linear1(bn_output)\n",
        "    relu1_output = self.relu1(linear1_output)\n",
        "    dropout1_output = self.dropout1(relu1_output)\n",
        "\n",
        "    linear2_output = self.linear2(dropout1_output)\n",
        "    relu2_output = self.relu2(linear2_output)\n",
        "    dropout2_output = self.dropout2(relu2_output)\n",
        "\n",
        "    linear3_output = self.linear3(dropout2_output)\n",
        "    relu3_output = self.relu3(linear3_output)\n",
        "    dropout3_output = self.dropout3(relu3_output)\n",
        "\n",
        "    linear4_output = self.linear4(dropout3_output)\n",
        "    relu4_output = self.relu4(linear4_output)\n",
        "    dropout4_output = self.dropout4(relu4_output)\n",
        "\n",
        "    linear5_output = self.linear5(dropout4_output)\n",
        "\n",
        "\n",
        "    probas = self.softmax(linear5_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].long()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.max(outputs, dim=1).indices == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].long()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.max(outputs, dim=1).indices == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "MAX_LEN = 150\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 70\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-2)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "d0c04b89-4d8a-4dbb-e211-3b5cf77c6f70"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/Models/RobertaDensenet/checkpoint.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "# best_accuracy = checkpoint['best_accuracy']\n",
        "\n",
        "# print(start_epoch)\n",
        "# print(best_accuracy)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {start_epoch + epoch + 1}/{start_epoch + EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/Models/RobertaDensenet/checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc\n",
        "\n",
        "\n",
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/RobertaDensenet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)\n",
        "\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.4722683106859524 accuracy 0.6382631407117205\n",
            "Val   loss 0.7365804314613342 accuracy 0.7505010020040079\n",
            "Epoch 2/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5047752459843954 accuracy 0.8352921971922951\n",
            "Val   loss 0.5641736909747124 accuracy 0.8236472945891783\n",
            "Epoch 3/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.43556948378682137 accuracy 0.8566764609859615\n",
            "Val   loss 0.5601342469453812 accuracy 0.8186372745490982\n",
            "Epoch 4/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3915226434667905 accuracy 0.8764283382304929\n",
            "Val   loss 0.5752390548586845 accuracy 0.8186372745490982\n",
            "Epoch 5/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.34259583180149394 accuracy 0.886549134835129\n",
            "Val   loss 0.629909336566925 accuracy 0.8266533066132263\n",
            "Epoch 6/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3304044461498658 accuracy 0.890630101207966\n",
            "Val   loss 0.5950838550925255 accuracy 0.8206412825651302\n",
            "Epoch 7/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3080995163569848 accuracy 0.9064642507345739\n",
            "Val   loss 0.6522315815091133 accuracy 0.8236472945891783\n",
            "Epoch 8/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2853320377568404 accuracy 0.9094025465230167\n",
            "Val   loss 0.6349860727787018 accuracy 0.8326653306613225\n",
            "Epoch 9/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.30355064757168293 accuracy 0.9027097616715638\n",
            "Val   loss 0.5720858871936798 accuracy 0.8176352705410821\n",
            "Epoch 10/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3056657947599888 accuracy 0.9046686255305256\n",
            "Val   loss 0.7062581032514572 accuracy 0.8346693386773546\n",
            "Epoch 11/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2239679725219806 accuracy 0.9293176624224616\n",
            "Val   loss 0.6177921295166016 accuracy 0.8256513026052104\n",
            "Epoch 12/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23388887817660967 accuracy 0.925726412014365\n",
            "Val   loss 0.7740401104092598 accuracy 0.8256513026052104\n",
            "Epoch 13/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.21271423871318498 accuracy 0.935194253999347\n",
            "Val   loss 0.7405407279729843 accuracy 0.812625250501002\n",
            "Epoch 14/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19548777056237063 accuracy 0.9413973228860594\n",
            "Val   loss 0.711725577712059 accuracy 0.8276553106212424\n",
            "Epoch 15/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.18000271171331406 accuracy 0.9440091413646752\n",
            "Val   loss 0.7106346040964127 accuracy 0.8276553106212424\n",
            "Epoch 16/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1675234536329905 accuracy 0.9533137446947437\n",
            "Val   loss 0.868138000369072 accuracy 0.8286573146292584\n",
            "Epoch 17/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.16836739735056958 accuracy 0.9487430623571661\n",
            "Val   loss 1.0010429322719574 accuracy 0.8326653306613225\n",
            "Epoch 18/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.15469354360053936 accuracy 0.9564152791380999\n",
            "Val   loss 0.8045683205127716 accuracy 0.8326653306613225\n",
            "Epoch 19/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13600298513968787 accuracy 0.960822722820764\n",
            "Val   loss 0.9047361016273499 accuracy 0.8376753507014028\n",
            "Epoch 20/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13336970719198385 accuracy 0.9590270976167156\n",
            "Val   loss 0.9374901950359344 accuracy 0.8196392785571142\n",
            "Epoch 21/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14020733116194606 accuracy 0.9591903362716291\n",
            "Val   loss 0.8674332797527313 accuracy 0.8196392785571142\n",
            "Epoch 22/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.15420957189053297 accuracy 0.9572314724126673\n",
            "Val   loss 0.9444703310728073 accuracy 0.8226452905811622\n",
            "Epoch 23/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12498552026227117 accuracy 0.9640874959190336\n",
            "Val   loss 1.3695925027132034 accuracy 0.8286573146292584\n",
            "Epoch 24/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1320779180775086 accuracy 0.9666993143976493\n",
            "Val   loss 0.8232276290655136 accuracy 0.8296593186372745\n",
            "Epoch 25/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10982996132224798 accuracy 0.9696376101860921\n",
            "Val   loss 1.0409023761749268 accuracy 0.8256513026052104\n",
            "Epoch 26/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13818583668520054 accuracy 0.9640874959190336\n",
            "Val   loss 0.8111663907766342 accuracy 0.8236472945891783\n",
            "Epoch 27/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11286747983346383 accuracy 0.9663728370878224\n",
            "Val   loss 1.1864441633224487 accuracy 0.8266533066132263\n",
            "Epoch 28/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09530917756880324 accuracy 0.9719229513548808\n",
            "Val   loss 0.9904370605945587 accuracy 0.8446893787575149\n",
            "Epoch 29/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09392983683695395 accuracy 0.975350963108064\n",
            "Val   loss 1.4030073285102844 accuracy 0.8346693386773546\n",
            "Epoch 30/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09166101707766454 accuracy 0.975024485798237\n",
            "Val   loss 1.205020010471344 accuracy 0.8286573146292584\n",
            "Epoch 31/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09483553236350417 accuracy 0.975024485798237\n",
            "Val   loss 1.5700877606868744 accuracy 0.8316633266533066\n",
            "Epoch 32/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09012116678059101 accuracy 0.975350963108064\n",
            "Val   loss 1.2673976719379425 accuracy 0.8226452905811622\n",
            "Epoch 33/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.07659026456531137 accuracy 0.9789422135161606\n",
            "Val   loss 1.2762246876955032 accuracy 0.8176352705410821\n",
            "Epoch 34/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.09254151613761981 accuracy 0.9760039177277179\n",
            "Val   loss 1.2187446355819702 accuracy 0.8236472945891783\n",
            "Epoch 35/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.07620067615061998 accuracy 0.9810643160300359\n",
            "Val   loss 1.474701628088951 accuracy 0.8186372745490982\n",
            "Epoch 36/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.06805594482769568 accuracy 0.9835128958537381\n",
            "Val   loss 1.74241703748703 accuracy 0.8206412825651302\n",
            "Epoch 37/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.06818931867989401 accuracy 0.9822069866144303\n",
            "Val   loss 1.478518009185791 accuracy 0.814629258517034\n",
            "Epoch 38/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.06776172659980755 accuracy 0.9822069866144303\n",
            "Val   loss 1.771267831325531 accuracy 0.8166332665330661\n",
            "Epoch 39/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.06173802919996282 accuracy 0.9846555664381326\n",
            "Val   loss 1.483622133731842 accuracy 0.812625250501002\n",
            "Epoch 40/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.048933325917460024 accuracy 0.9871041462618347\n",
            "Val   loss 1.7229830175638199 accuracy 0.8236472945891783\n",
            "Epoch 41/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0687029369873926 accuracy 0.9841658504733921\n",
            "Val   loss 1.6170586049556732 accuracy 0.8176352705410821\n",
            "Epoch 42/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.037901615452331804 accuracy 0.9888997714658831\n",
            "Val   loss 1.6262465417385101 accuracy 0.8366733466933867\n",
            "Epoch 43/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.045069352723658085 accuracy 0.9882468168462292\n",
            "Val   loss 1.3031482696533203 accuracy 0.8246492985971943\n",
            "Epoch 44/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.040311962560129665 accuracy 0.9885732941560561\n",
            "Val   loss 1.388019174337387 accuracy 0.8336673346693386\n",
            "Epoch 45/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.03835375391645357 accuracy 0.9913483512895853\n",
            "Val   loss 1.2705264538526535 accuracy 0.8426853707414829\n",
            "Epoch 46/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.03010326138852785 accuracy 0.9916748285994124\n",
            "Val   loss 1.2029732316732407 accuracy 0.8376753507014028\n",
            "Epoch 47/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.03769755258690566 accuracy 0.9897159647404505\n",
            "Val   loss 1.0379088073968887 accuracy 0.8426853707414829\n",
            "Epoch 48/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.028999231251267094 accuracy 0.9906953966699314\n",
            "Val   loss 1.2240234017372131 accuracy 0.8346693386773546\n",
            "Epoch 49/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.033357560789833464 accuracy 0.990205680705191\n",
            "Val   loss 1.301022693514824 accuracy 0.8296593186372745\n",
            "Epoch 50/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.03528994145259882 accuracy 0.9908586353248449\n",
            "Val   loss 1.2721402049064636 accuracy 0.8226452905811622\n",
            "Epoch 51/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0232341783218241 accuracy 0.9936336924583741\n",
            "Val   loss 1.3340107649564743 accuracy 0.8266533066132263\n",
            "Epoch 52/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.02343723232237001 accuracy 0.9936336924583741\n",
            "Val   loss 1.47699673473835 accuracy 0.8306613226452905\n",
            "Epoch 53/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.01609447895316407 accuracy 0.9947763630427685\n",
            "Val   loss 1.6904159188270569 accuracy 0.8206412825651302\n",
            "Epoch 54/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.018157914513722062 accuracy 0.9955925563173359\n",
            "Val   loss 1.7543071806430817 accuracy 0.8306613226452905\n",
            "Epoch 55/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.02626200642650171 accuracy 0.9937969311132876\n",
            "Val   loss 1.5376693904399872 accuracy 0.8206412825651302\n",
            "Epoch 56/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.02078655470783512 accuracy 0.994939601697682\n",
            "Val   loss 1.4098562002182007 accuracy 0.8276553106212424\n",
            "Epoch 57/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.023640428805568565 accuracy 0.994613124387855\n",
            "Val   loss 1.4465405195951462 accuracy 0.8176352705410821\n",
            "Epoch 58/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.012591277714818716 accuracy 0.9968984655566437\n",
            "Val   loss 1.4966415464878082 accuracy 0.8266533066132263\n",
            "Epoch 59/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.014470484195044264 accuracy 0.9955925563173359\n",
            "Val   loss 1.5319577306509018 accuracy 0.8256513026052104\n",
            "Epoch 60/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.01508696816987746 accuracy 0.9954293176624225\n",
            "Val   loss 1.5009932816028595 accuracy 0.8296593186372745\n",
            "Epoch 61/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.009994577490336573 accuracy 0.9968984655566437\n",
            "Val   loss 1.6123195141553879 accuracy 0.8186372745490982\n",
            "Epoch 62/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.01090971219309722 accuracy 0.9970617042115573\n",
            "Val   loss 1.6907392144203186 accuracy 0.8196392785571142\n",
            "Epoch 63/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.01659894360151763 accuracy 0.9965719882468168\n",
            "Val   loss 1.5003566443920135 accuracy 0.8316633266533066\n",
            "Epoch 64/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.010796701858149996 accuracy 0.9972249428664708\n",
            "Val   loss 1.5659416168928146 accuracy 0.8256513026052104\n",
            "Epoch 65/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.011334845420302978 accuracy 0.9972249428664708\n",
            "Val   loss 1.5876272767782211 accuracy 0.8306613226452905\n",
            "Epoch 66/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.012126526979651922 accuracy 0.9972249428664708\n",
            "Val   loss 1.5672466158866882 accuracy 0.8306613226452905\n",
            "Epoch 67/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0076650200735457474 accuracy 0.9973881815213842\n",
            "Val   loss 1.618176281452179 accuracy 0.8276553106212424\n",
            "Epoch 68/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.011190531261187667 accuracy 0.9965719882468168\n",
            "Val   loss 1.6338684409856796 accuracy 0.8276553106212424\n",
            "Epoch 69/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.012202616489958018 accuracy 0.9967352269017303\n",
            "Val   loss 1.6076256036758423 accuracy 0.8296593186372745\n",
            "Epoch 70/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.01343762507470577 accuracy 0.9968984655566437\n",
            "Val   loss 1.5983845591545105 accuracy 0.8296593186372745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFmTSaxYMw-"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/RobertaDensenet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScOj15BovCww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "a57aa35c-830d-4031-e1bc-0611ff2011f8"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTghsXN8vEpo",
        "outputId": "24853592-7715-4a9a-f09f-f8b20a028b64"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].long()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "      preds = torch.max(outputs, dim=1).indices\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['not_humanitarian', 'infrastructure_and_utility_damage', 'other_relevant_information', 'rescue_volunteering_or_donation_effort', 'affected_individuals'], digits = 4))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                        precision    recall  f1-score   support\n",
            "\n",
            "                      not_humanitarian     0.8566    0.9127    0.8838       504\n",
            "     infrastructure_and_utility_damage     0.8961    0.8519    0.8734        81\n",
            "            other_relevant_information     0.8716    0.8085    0.8389       235\n",
            "rescue_volunteering_or_donation_effort     0.8348    0.7619    0.7967       126\n",
            "                  affected_individuals     0.7500    0.6667    0.7059         9\n",
            "\n",
            "                              accuracy                         0.8597       955\n",
            "                             macro avg     0.8418    0.8003    0.8197       955\n",
            "                          weighted avg     0.8598    0.8597    0.8587       955\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}