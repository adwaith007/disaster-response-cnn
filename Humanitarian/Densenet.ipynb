{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Densenet_hum.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de0cf5d2950b4892ad540a1902268059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_21dbad639de54f32ab1b1fa1caf7564b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1bbc3274531a4f1b83bb925189fd8c03",
              "IPY_MODEL_eb818822f01d4b9c8f04e2451c55b9e2"
            ]
          }
        },
        "21dbad639de54f32ab1b1fa1caf7564b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1bbc3274531a4f1b83bb925189fd8c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8218f07559e34392aeb3b8c9c3ebd091",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 115730790,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 115730790,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2265900df91b40c3b7d486ef03afd8ed"
          }
        },
        "eb818822f01d4b9c8f04e2451c55b9e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_897548524d174a37806f5c89fdcfdde7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 110M/110M [00:01&lt;00:00, 67.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9e31e499d3a4b73bc08d2c441b3f752"
          }
        },
        "8218f07559e34392aeb3b8c9c3ebd091": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2265900df91b40c3b7d486ef03afd8ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "897548524d174a37806f5c89fdcfdde7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9e31e499d3a4b73bc08d2c441b3f752": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "3da92556-e8cf-4ae5-9c0d-8a28457cf3ce"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzejvNI6kk3A"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3 sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwWQL0acqLuH",
        "outputId": "30350610-9e91-4fc5-ab15-8c8654aa8d10"
      },
      "source": [
        "!ls gdrive/MyDrive/data_image"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "california_wildfires  hurricane_irma   iraq_iran_earthquake  srilanka_floods\n",
            "hurricane_harvey      hurricane_maria  mexico_earthquake\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "4d7ff622-c47b-49a0-d0fe-cb3aaddd552e"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"not_humanitarian\":\n",
        "    return 0\n",
        "  elif text == \"infrastructure_and_utility_damage\":\n",
        "    return 1\n",
        "  elif text == \"other_relevant_information\":\n",
        "    return 2\n",
        "  elif text == \"rescue_volunteering_or_donation_effort\":\n",
        "    return 3\n",
        "  elif text == \"affected_individuals\":\n",
        "    return 4\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/Models/train_hum.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/Models/val_hum.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/Models/test_hum.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, paths, targets):\n",
        "    self.paths = paths\n",
        "    self.targets = targets\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.paths)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    path = str(self.paths[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    return {\n",
        "      'tweet_image': img,\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    paths=df.image.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.densenet = torchvision.models.densenet161(pretrained=True)\n",
        "    # for param in self.resnet.parameters():\n",
        "    #   param.requires_grad = False\n",
        "\n",
        "    self.bn = nn.BatchNorm1d(1000)\n",
        "    self.linear1 = nn.Linear(1000, 256)\n",
        "    self.relu    = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=0.4)\n",
        "    self.linear2 = nn.Linear(256, 5)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "  \n",
        "  def forward(self, tweet_img):\n",
        "    output = self.densenet(tweet_img)\n",
        "    bn_output = self.bn(output)\n",
        "    linear1_output = self.linear1(bn_output)\n",
        "    relu_output = self.relu(linear1_output)\n",
        "    dropout_output = self.dropout(relu_output)\n",
        "    linear2_output = self.linear2(dropout_output)\n",
        "    probas = self.softmax(linear2_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    targets = d[\"targets\"].long()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.max(outputs, dim=1).indices == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      targets = d[\"targets\"].long()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.max(outputs, dim=1).indices == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "de0cf5d2950b4892ad540a1902268059",
            "21dbad639de54f32ab1b1fa1caf7564b",
            "1bbc3274531a4f1b83bb925189fd8c03",
            "eb818822f01d4b9c8f04e2451c55b9e2",
            "8218f07559e34392aeb3b8c9c3ebd091",
            "2265900df91b40c3b7d486ef03afd8ed",
            "897548524d174a37806f5c89fdcfdde7",
            "f9e31e499d3a4b73bc08d2c441b3f752"
          ]
        },
        "outputId": "996654e1-1024-4697-c3ea-79b761c61e69"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/hub/checkpoints/densenet161-8d451a50.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de0cf5d2950b4892ad540a1902268059",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=115730790.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "c2b4bc07-abba-46ad-8229-abe41f89f782"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/Models/Densenet/checkpoint.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "\n",
        "# print(start_epoch)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "  # scheduler.step(val_acc)\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/Models/Densenet/checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc\n",
        "\n",
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/Densenet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)\n",
        "\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.0240911953151226 accuracy 0.6642180868429644\n",
            "Val   loss 0.761169626377523 accuracy 0.751503006012024\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5347684690107902 accuracy 0.8299053215801502\n",
            "Val   loss 0.6819627396762371 accuracy 0.7755511022044087\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.28044792970952886 accuracy 0.9125040809663728\n",
            "Val   loss 0.7527611320838332 accuracy 0.7765531062124248\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12557377727353014 accuracy 0.9647404505386875\n",
            "Val   loss 0.8799826148897409 accuracy 0.7725450901803607\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.06340775347416638 accuracy 0.9843290891283055\n",
            "Val   loss 0.989763667806983 accuracy 0.7625250501002003\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.028643612475813523 accuracy 0.9937969311132876\n",
            "Val   loss 1.0653068413957953 accuracy 0.7595190380761523\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.013207099514602305 accuracy 0.9970617042115573\n",
            "Val   loss 1.0940589727833867 accuracy 0.7725450901803607\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.005958382645985694 accuracy 0.9986940907606922\n",
            "Val   loss 1.158824567683041 accuracy 0.7775551102204408\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0031908284584763655 accuracy 0.9991838067254326\n",
            "Val   loss 1.2633356973528862 accuracy 0.7705410821643286\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0019246281503531766 accuracy 0.9998367613450865\n",
            "Val   loss 1.3993392353877425 accuracy 0.7675350701402806\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.002555340652368917 accuracy 0.999673522690173\n",
            "Val   loss 1.2978723933920264 accuracy 0.7755511022044087\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0008014482732316234 accuracy 1.0\n",
            "Val   loss 1.37250008713454 accuracy 0.7805611222444889\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0006479432619433586 accuracy 1.0\n",
            "Val   loss 1.397733485326171 accuracy 0.7875751503006011\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0013528766833132977 accuracy 0.9998367613450865\n",
            "Val   loss 1.4046453582122922 accuracy 0.782565130260521\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0005067771498753851 accuracy 1.0\n",
            "Val   loss 1.4201752273365855 accuracy 0.782565130260521\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0003266648165739146 accuracy 1.0\n",
            "Val   loss 1.4360601222142577 accuracy 0.7875751503006011\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0003534937022019828 accuracy 1.0\n",
            "Val   loss 1.5027145445346832 accuracy 0.7845691382765531\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0012136652268187238 accuracy 0.9998367613450865\n",
            "Val   loss 1.4826001077890396 accuracy 0.7815631262525049\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0012182339387247036 accuracy 0.9998367613450865\n",
            "Val   loss 1.4922570660710335 accuracy 0.7845691382765531\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0029035663043922946 accuracy 0.999347045380346\n",
            "Val   loss 1.5315450970083475 accuracy 0.7695390781563125\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0006882871324099445 accuracy 0.9998367613450865\n",
            "Val   loss 1.5922201201319695 accuracy 0.7685370741482965\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.00532981885000557 accuracy 0.9983676134508651\n",
            "Val   loss 1.683178273960948 accuracy 0.7785571142284569\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0027842378287440774 accuracy 0.9985308521057786\n",
            "Val   loss 1.599228410050273 accuracy 0.7745490981963927\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0023474496176163484 accuracy 0.9995102840352595\n",
            "Val   loss 1.701269831508398 accuracy 0.7695390781563125\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0006437280344139632 accuracy 0.999673522690173\n",
            "Val   loss 1.7317383736371994 accuracy 0.7745490981963927\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.001040259953261587 accuracy 0.9998367613450865\n",
            "Val   loss 1.7404184695333242 accuracy 0.7705410821643286\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0007400830942809004 accuracy 0.9998367613450865\n",
            "Val   loss 1.7522573871538043 accuracy 0.7735470941883766\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.00017688159587455252 accuracy 1.0\n",
            "Val   loss 1.7668568044900894 accuracy 0.7675350701402806\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 9.210636134090085e-05 accuracy 1.0\n",
            "Val   loss 1.7718808129429817 accuracy 0.7695390781563125\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0006710681682378095 accuracy 0.9998367613450865\n",
            "Val   loss 1.8003798201680183 accuracy 0.7635270541082164\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 9.645594727677083e-05 accuracy 1.0\n",
            "Val   loss 1.804888192564249 accuracy 0.7665330661322645\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 6.480769740827934e-05 accuracy 1.0\n",
            "Val   loss 1.7950432756915689 accuracy 0.7655310621242485\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 4.767867733571999e-05 accuracy 1.0\n",
            "Val   loss 1.8082892512902617 accuracy 0.7645290581162324\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 3.925449980772081e-05 accuracy 1.0\n",
            "Val   loss 1.8230013949796557 accuracy 0.7645290581162324\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 3.3048853136818934e-05 accuracy 1.0\n",
            "Val   loss 1.8282103762030602 accuracy 0.7645290581162324\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 3.278984659615958e-05 accuracy 1.0\n",
            "Val   loss 1.8372007394209504 accuracy 0.7655310621242485\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.8703810606600655e-05 accuracy 1.0\n",
            "Val   loss 1.848627625964582 accuracy 0.7685370741482965\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.8272212511855816e-05 accuracy 1.0\n",
            "Val   loss 1.8536718981340528 accuracy 0.7695390781563125\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.8454163163142237e-05 accuracy 1.0\n",
            "Val   loss 1.8586093978956342 accuracy 0.7685370741482965\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.305411071882683e-05 accuracy 1.0\n",
            "Val   loss 1.866903162561357 accuracy 0.7695390781563125\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.401179415301158e-05 accuracy 1.0\n",
            "Val   loss 1.8745385594666004 accuracy 0.7715430861723446\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.2418596248741096e-05 accuracy 1.0\n",
            "Val   loss 1.8829266466200352 accuracy 0.7705410821643286\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.049380117992901e-05 accuracy 1.0\n",
            "Val   loss 1.8869111761450768 accuracy 0.7725450901803607\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.080846321206309e-05 accuracy 1.0\n",
            "Val   loss 1.8929759478196502 accuracy 0.7715430861723446\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.8206978469474205e-05 accuracy 1.0\n",
            "Val   loss 1.8993515660986304 accuracy 0.7725450901803607\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.0663503506786658e-05 accuracy 1.0\n",
            "Val   loss 1.9069372890517116 accuracy 0.7735470941883766\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.9030452654315393e-05 accuracy 1.0\n",
            "Val   loss 1.9134348686784506 accuracy 0.7735470941883766\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.850902566324919e-05 accuracy 1.0\n",
            "Val   loss 1.9143757419660687 accuracy 0.7735470941883766\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.783896419560449e-05 accuracy 1.0\n",
            "Val   loss 1.9165347376838326 accuracy 0.7735470941883766\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.9539679620095285e-05 accuracy 1.0\n",
            "Val   loss 1.917069185525179 accuracy 0.7735470941883766\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Taz58ppcm8n8"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "\n",
        "\n",
        "savepath= \"./gdrive/MyDrive/Models/Densenet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ScOj15BovCww",
        "outputId": "ba388127-9da4-43fa-f9a9-1efeccd1ab61"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTghsXN8vEpo",
        "outputId": "8e02e48a-729c-42bf-82b8-1c36619ebe1b"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      targets = d[\"targets\"].long()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(tweet_img=tweet_imgs)\n",
        "      preds = torch.max(outputs, dim=1).indices\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['not_humanitarian', 'infrastructure_and_utility_damage', 'other_relevant_information', 'rescue_volunteering_or_donation_effort', 'affected_individuals'], digits = 4))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                        precision    recall  f1-score   support\n",
            "\n",
            "                      not_humanitarian     0.8305    0.8849    0.8569       504\n",
            "     infrastructure_and_utility_damage     0.7727    0.8395    0.8047        81\n",
            "            other_relevant_information     0.8519    0.7830    0.8160       235\n",
            "rescue_volunteering_or_donation_effort     0.6697    0.5794    0.6213       126\n",
            "                  affected_individuals     0.4000    0.2222    0.2857         9\n",
            "\n",
            "                              accuracy                         0.8094       955\n",
            "                             macro avg     0.7050    0.6618    0.6769       955\n",
            "                          weighted avg     0.8056    0.8094    0.8059       955\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}