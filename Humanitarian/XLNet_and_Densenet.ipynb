{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLNet and Densenet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "d2a6e1b1-d3bd-4e62-c675-cd29b2f8373c"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzejvNI6kk3A"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3 sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "68227e2a-877a-4942-fa1e-d71db6a99103"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"not_humanitarian\":\n",
        "    return 0\n",
        "  elif text == \"infrastructure_and_utility_damage\":\n",
        "    return 1\n",
        "  elif text == \"other_relevant_information\":\n",
        "    return 2\n",
        "  elif text == \"rescue_volunteering_or_donation_effort\":\n",
        "    return 3\n",
        "  elif text == \"affected_individuals\":\n",
        "    return 4\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/Models/train.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'tweet_text', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['tweet_text'] = df_train['tweet_text'].apply(clean_text)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/Models/val.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'tweet_text', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['tweet_text'] = df_val['tweet_text'].apply(clean_text)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/Models/test.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'tweet_text', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(clean_text)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, targets, paths, tokenizer, max_len):\n",
        "    self.tweets = tweets\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.paths = paths\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    tweet = str(self.tweets[item])\n",
        "    target = self.targets[item]\n",
        "    path = str(self.paths[item])\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "      truncation = True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'tweet_text': tweet,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long),\n",
        "      'tweet_image': img\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    tweets=df.tweet_text.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "    paths=df.image.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.xlnet = XLNetModel.from_pretrained(\"xlnet-base-cased\")\n",
        "    for param in self.xlnet.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.resnet = torchvision.models.densenet161(pretrained=True)\n",
        "    for param in self.resnet.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(self.xlnet.config.hidden_size*MAX_LEN + 1000)\n",
        "\n",
        "    self.linear1 = nn.Linear(self.xlnet.config.hidden_size*MAX_LEN + 1000, 1000)\n",
        "    self.relu1    = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(p=0.4)\n",
        "\n",
        "    self.linear2 = nn.Linear(1000, 500)\n",
        "    self.relu2    = nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "    self.linear3 = nn.Linear(500, 250)\n",
        "    self.relu3    = nn.ReLU()\n",
        "    self.dropout3 = nn.Dropout(p=0.1)\n",
        "\n",
        "    self.linear4 = nn.Linear(250, 125)\n",
        "    self.relu4    = nn.ReLU()\n",
        "    self.dropout4 = nn.Dropout(p=0.02)\n",
        "\n",
        "    self.linear5 = nn.Linear(125, 5)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask, tweet_img):\n",
        "    text_output = self.xlnet(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False\n",
        "    )\n",
        "\n",
        "    text_output = text_output[0]\n",
        "    text_output = torch.reshape(text_output, (-1, MAX_LEN*self.xlnet.config.hidden_size))\n",
        "\n",
        "    image_output = self.resnet(tweet_img)\n",
        "    merged_output = torch.cat((text_output, image_output), dim=1)\n",
        "    bn_output = self.bn(merged_output)\n",
        "\n",
        "    linear1_output = self.linear1(bn_output)\n",
        "    relu1_output = self.relu1(linear1_output)\n",
        "    dropout1_output = self.dropout1(relu1_output)\n",
        "\n",
        "    linear2_output = self.linear2(dropout1_output)\n",
        "    relu2_output = self.relu2(linear2_output)\n",
        "    dropout2_output = self.dropout2(relu2_output)\n",
        "\n",
        "    linear3_output = self.linear3(dropout2_output)\n",
        "    relu3_output = self.relu3(linear3_output)\n",
        "    dropout3_output = self.dropout3(relu3_output)\n",
        "\n",
        "    linear4_output = self.linear4(dropout3_output)\n",
        "    relu4_output = self.relu4(linear4_output)\n",
        "    dropout4_output = self.dropout4(relu4_output)\n",
        "\n",
        "    linear5_output = self.linear5(dropout4_output)\n",
        "\n",
        "\n",
        "    probas = self.softmax(linear5_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].long()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.max(outputs, dim=1).indices == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].long()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.max(outputs, dim=1).indices == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "MAX_LEN = 150\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'xlnet-base-cased'\n",
        "tokenizer = XLNetTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 70\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-2)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "205bee76-0eb8-4c99-e520-8d9fda639fc1"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/Models/XLNetDenseNet/checkpoint-50.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "# best_accuracy = checkpoint['best_accuracy']\n",
        "\n",
        "# print(start_epoch)\n",
        "# print(best_accuracy)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {start_epoch + epoch + 1}/{start_epoch + EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/Models/XLNetDenseNet/checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc\n",
        "\n",
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/XLNetDenseNet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)\n",
        "\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 16.095362275838852 accuracy 0.4758406790728044\n",
            "Val   loss 1.0959906578063965 accuracy 0.5751503006012023\n",
            "\n",
            "Epoch 2/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.998305469751358 accuracy 0.609369898792034\n",
            "Val   loss 1.0598974227905273 accuracy 0.5901803607214429\n",
            "\n",
            "Epoch 3/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8978646894296011 accuracy 0.6598106431603004\n",
            "Val   loss 1.0130132138729095 accuracy 0.6122244488977956\n",
            "\n",
            "Epoch 4/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.7898718118667603 accuracy 0.7094351942539994\n",
            "Val   loss 0.7800608277320862 accuracy 0.7034068136272544\n",
            "\n",
            "Epoch 5/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.7309588839610418 accuracy 0.743062357166177\n",
            "Val   loss 0.7615537345409393 accuracy 0.7314629258517034\n",
            "\n",
            "Epoch 6/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6736914664506912 accuracy 0.7678746327130265\n",
            "Val   loss 0.7027043402194977 accuracy 0.7424849699398797\n",
            "\n",
            "Epoch 7/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6513941586017609 accuracy 0.7747306562193927\n",
            "Val   loss 0.6854044795036316 accuracy 0.7505010020040079\n",
            "\n",
            "Epoch 8/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6226572841405869 accuracy 0.7856676460985962\n",
            "Val   loss 0.6550227403640747 accuracy 0.7414829659318637\n",
            "\n",
            "Epoch 9/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6050893813371658 accuracy 0.796767874632713\n",
            "Val   loss 0.6862105131149292 accuracy 0.7434869739478958\n",
            "\n",
            "Epoch 10/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5862286984920502 accuracy 0.7939928174991838\n",
            "Val   loss 0.6872184574604034 accuracy 0.7484969939879759\n",
            "\n",
            "Epoch 11/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6029312709967295 accuracy 0.796767874632713\n",
            "Val   loss 0.6720218360424042 accuracy 0.7655310621242485\n",
            "\n",
            "Epoch 12/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5978629241387049 accuracy 0.7944825334639243\n",
            "Val   loss 0.6320130825042725 accuracy 0.7755511022044087\n",
            "\n",
            "Epoch 13/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5666006778677305 accuracy 0.8032974208292524\n",
            "Val   loss 0.631738156080246 accuracy 0.7695390781563125\n",
            "\n",
            "Epoch 14/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5727634703119596 accuracy 0.806562193927522\n",
            "Val   loss 0.5992042422294617 accuracy 0.8006012024048096\n",
            "\n",
            "Epoch 15/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5461769898732504 accuracy 0.8161932745674175\n",
            "Val   loss 0.6542269885540009 accuracy 0.7845691382765531\n",
            "\n",
            "Epoch 16/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.555935355524222 accuracy 0.8143976493633692\n",
            "Val   loss 0.6401316523551941 accuracy 0.7875751503006011\n",
            "\n",
            "Epoch 17/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.536753219862779 accuracy 0.8178256611165524\n",
            "Val   loss 0.6375563740730286 accuracy 0.7735470941883766\n",
            "\n",
            "Epoch 18/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.53028022001187 accuracy 0.8161932745674175\n",
            "Val   loss 0.6316483616828918 accuracy 0.7755511022044087\n",
            "\n",
            "Epoch 19/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5185661191741625 accuracy 0.8232125367286973\n",
            "Val   loss 0.6071386635303497 accuracy 0.783567134268537\n",
            "\n",
            "Epoch 20/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5508807525038719 accuracy 0.8130917401240614\n",
            "Val   loss 0.6205909550189972 accuracy 0.8056112224448897\n",
            "\n",
            "Epoch 21/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.513358990351359 accuracy 0.8263140711720536\n",
            "Val   loss 0.6032519936561584 accuracy 0.7865731462925851\n",
            "\n",
            "Epoch 22/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5269987707336744 accuracy 0.8237022526934378\n",
            "Val   loss 0.553626537322998 accuracy 0.8016032064128256\n",
            "\n",
            "Epoch 23/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.511080282429854 accuracy 0.8325171400587659\n",
            "Val   loss 0.6047639846801758 accuracy 0.8066132264529058\n",
            "\n",
            "Epoch 24/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5231240342060725 accuracy 0.8147241266731962\n",
            "Val   loss 0.555357813835144 accuracy 0.7985971943887775\n",
            "\n",
            "Epoch 25/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5222064107656479 accuracy 0.8254978778974861\n",
            "Val   loss 0.5824943780899048 accuracy 0.7865731462925851\n",
            "\n",
            "Epoch 26/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.529152882595857 accuracy 0.8186418543911198\n",
            "Val   loss 0.6379209458827972 accuracy 0.7845691382765531\n",
            "\n",
            "Epoch 27/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5279064004619917 accuracy 0.816682990532158\n",
            "Val   loss 0.5766007006168365 accuracy 0.8066132264529058\n",
            "\n",
            "Epoch 28/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4912206580241521 accuracy 0.8344760039177277\n",
            "Val   loss 0.621481329202652 accuracy 0.7975951903807614\n",
            "\n",
            "Epoch 29/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4738275706768036 accuracy 0.8418217433888344\n",
            "Val   loss 0.6276759207248688 accuracy 0.7935871743486973\n",
            "\n",
            "Epoch 30/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4757806236545245 accuracy 0.8437806072477962\n",
            "Val   loss 0.5560398995876312 accuracy 0.8106212424849699\n",
            "\n",
            "Epoch 31/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4420171504219373 accuracy 0.851126346718903\n",
            "Val   loss 0.5662273466587067 accuracy 0.7995991983967935\n",
            "\n",
            "Epoch 32/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4509273444612821 accuracy 0.846392425726412\n",
            "Val   loss 0.6504605412483215 accuracy 0.8026052104208417\n",
            "\n",
            "Epoch 33/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4695311312874158 accuracy 0.8468821416911524\n",
            "Val   loss 0.624247819185257 accuracy 0.7945891783567134\n",
            "\n",
            "Epoch 34/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.48918022215366364 accuracy 0.8341495266079008\n",
            "Val   loss 0.6368062496185303 accuracy 0.7915831663326652\n",
            "\n",
            "Epoch 35/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.47835176189740497 accuracy 0.8406790728044401\n",
            "Val   loss 0.5975756645202637 accuracy 0.8046092184368737\n",
            "\n",
            "Epoch 36/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4614313319325447 accuracy 0.8480248122755468\n",
            "Val   loss 0.6347133219242096 accuracy 0.7995991983967935\n",
            "\n",
            "Epoch 37/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.43604529400666553 accuracy 0.8525954946131243\n",
            "Val   loss 0.5452873259782791 accuracy 0.8196392785571142\n",
            "\n",
            "Epoch 38/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4276805867751439 accuracy 0.855860267711394\n",
            "Val   loss 0.5671797394752502 accuracy 0.8046092184368737\n",
            "\n",
            "Epoch 39/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.41266898810863495 accuracy 0.8584720861900098\n",
            "Val   loss 0.5691429823637009 accuracy 0.812625250501002\n",
            "\n",
            "Epoch 40/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4084843769669533 accuracy 0.861247143323539\n",
            "Val   loss 0.6729993224143982 accuracy 0.813627254509018\n",
            "\n",
            "Epoch 41/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.41264216353495914 accuracy 0.8643486777668952\n",
            "Val   loss 0.5887571424245834 accuracy 0.81563126252505\n",
            "\n",
            "Epoch 42/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.41183796028296155 accuracy 0.8584720861900098\n",
            "Val   loss 0.6670005321502686 accuracy 0.8016032064128256\n",
            "\n",
            "Epoch 43/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.400704766313235 accuracy 0.8636957231472413\n",
            "Val   loss 0.6221785843372345 accuracy 0.7965931863727455\n",
            "\n",
            "Epoch 44/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4021836146712303 accuracy 0.8578191315703558\n",
            "Val   loss 0.6823707520961761 accuracy 0.7915831663326652\n",
            "\n",
            "Epoch 45/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4016958251595497 accuracy 0.8641854391119816\n",
            "Val   loss 0.7103741466999054 accuracy 0.7915831663326652\n",
            "\n",
            "Epoch 46/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.39393962919712067 accuracy 0.8708782239634345\n",
            "Val   loss 0.6546735167503357 accuracy 0.8076152304609218\n",
            "\n",
            "Epoch 47/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.40745091438293457 accuracy 0.8602677113940581\n",
            "Val   loss 0.5812140852212906 accuracy 0.8096192384769538\n",
            "\n",
            "Epoch 48/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.37989870955546695 accuracy 0.871367939928175\n",
            "Val   loss 0.5665813684463501 accuracy 0.812625250501002\n",
            "\n",
            "Epoch 49/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3727673913041751 accuracy 0.8694090760692131\n",
            "Val   loss 0.6647258698940277 accuracy 0.813627254509018\n",
            "\n",
            "Epoch 50/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3702843164404233 accuracy 0.8757753836108391\n",
            "Val   loss 0.6433506608009338 accuracy 0.8076152304609218\n",
            "\n",
            "Epoch 51/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.36447852353254956 accuracy 0.8738165197518772\n",
            "Val   loss 0.6626713275909424 accuracy 0.8226452905811622\n",
            "\n",
            "Epoch 52/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.34826763222614926 accuracy 0.880835781913157\n",
            "Val   loss 0.6360414028167725 accuracy 0.81563126252505\n",
            "\n",
            "Epoch 53/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3443470473090808 accuracy 0.8826314071172053\n",
            "Val   loss 0.6464691460132599 accuracy 0.814629258517034\n",
            "\n",
            "Epoch 54/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.33112278083960217 accuracy 0.8901403852432256\n",
            "Val   loss 0.6488319933414459 accuracy 0.8186372745490982\n",
            "\n",
            "Epoch 55/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.33482473095258075 accuracy 0.8857329415605615\n",
            "Val   loss 0.6218578219413757 accuracy 0.812625250501002\n",
            "\n",
            "Epoch 56/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.33218637357155484 accuracy 0.8870388507998694\n",
            "Val   loss 0.6095705777406693 accuracy 0.8186372745490982\n",
            "\n",
            "Epoch 57/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3297879671057065 accuracy 0.8872020894547828\n",
            "Val   loss 0.6193739622831345 accuracy 0.8246492985971943\n",
            "\n",
            "Epoch 58/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.32142097502946854 accuracy 0.8896506692784851\n",
            "Val   loss 0.6328603327274323 accuracy 0.814629258517034\n",
            "\n",
            "Epoch 59/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.33532371123631793 accuracy 0.8839373163565132\n",
            "Val   loss 0.6188057661056519 accuracy 0.8226452905811622\n",
            "\n",
            "Epoch 60/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.32543163498242694 accuracy 0.8878550440744368\n",
            "Val   loss 0.5945329666137695 accuracy 0.8216432865731462\n",
            "\n",
            "Epoch 61/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3183116738994916 accuracy 0.8889977146588312\n",
            "Val   loss 0.5868211835622787 accuracy 0.8186372745490982\n",
            "\n",
            "Epoch 62/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.30786988884210587 accuracy 0.8919360104472739\n",
            "Val   loss 0.5998644232749939 accuracy 0.8246492985971943\n",
            "\n",
            "Epoch 63/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.30051857481400174 accuracy 0.8966699314397649\n",
            "Val   loss 0.5890126675367355 accuracy 0.8246492985971943\n",
            "\n",
            "Epoch 64/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3000335320830345 accuracy 0.8955272608553705\n",
            "Val   loss 0.6116822957992554 accuracy 0.8316633266533066\n",
            "\n",
            "Epoch 65/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2979928143322468 accuracy 0.8983023179888997\n",
            "Val   loss 0.6041268557310104 accuracy 0.8246492985971943\n",
            "\n",
            "Epoch 66/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.30082424978415173 accuracy 0.895690499510284\n",
            "Val   loss 0.6025266498327255 accuracy 0.8296593186372745\n",
            "\n",
            "Epoch 67/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2954259092609088 accuracy 0.8978126020241592\n",
            "Val   loss 0.6210277676582336 accuracy 0.8286573146292584\n",
            "\n",
            "Epoch 68/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2776419868071874 accuracy 0.9072804440091413\n",
            "Val   loss 0.6349500417709351 accuracy 0.8296593186372745\n",
            "\n",
            "Epoch 69/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2848784439265728 accuracy 0.9038524322559582\n",
            "Val   loss 0.62519671022892 accuracy 0.8296593186372745\n",
            "\n",
            "Epoch 70/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2842854435245196 accuracy 0.9015670910871694\n",
            "Val   loss 0.6238894760608673 accuracy 0.8316633266533066\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFmTSaxYMw-"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/XLNetResNet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScOj15BovCww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "96f2dcd3-2554-4157-8847-17ba65146aaa"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTghsXN8vEpo"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].long()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "      preds = torch.max(outputs, dim=1).indices\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['not_humanitarian', 'infrastructure_and_utility_damage', 'other_relevant_information', 'rescue_volunteering_or_donation_effort', 'affected_individuals'], digits = 4))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}