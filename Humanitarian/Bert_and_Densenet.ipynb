{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bert_and_Densenet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "0c6ffe6b-9285-457f-c83e-2b3c817d9f98"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzejvNI6kk3A"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "0aa8216d-b5d1-4f99-9722-f3d9887e766e"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"not_humanitarian\":\n",
        "    return 0\n",
        "  elif text == \"infrastructure_and_utility_damage\":\n",
        "    return 1\n",
        "  elif text == \"other_relevant_information\":\n",
        "    return 2\n",
        "  elif text == \"rescue_volunteering_or_donation_effort\":\n",
        "    return 3\n",
        "  elif text == \"affected_individuals\":\n",
        "    return 4\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/Models/train.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'tweet_text', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['tweet_text'] = df_train['tweet_text'].apply(clean_text)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/Models/val.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'tweet_text', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['tweet_text'] = df_val['tweet_text'].apply(clean_text)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/Models/test.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'tweet_text', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(clean_text)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, targets, paths, tokenizer, max_len):\n",
        "    self.tweets = tweets\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.paths = paths\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    tweet = str(self.tweets[item])\n",
        "    target = self.targets[item]\n",
        "    path = str(self.paths[item])\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "      truncation = True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'tweet_text': tweet,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long),\n",
        "      'tweet_image': img\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    tweets=df.tweet_text.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "    paths=df.image.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    for param in self.bert.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.densenet = torchvision.models.densenet161(pretrained=True)\n",
        "    for param in self.densenet.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(self.bert.config.hidden_size + 1000)\n",
        "\n",
        "    self.linear1 = nn.Linear(self.bert.config.hidden_size + 1000, 1000)\n",
        "    self.relu1    = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(p=0.4)\n",
        "\n",
        "    self.linear2 = nn.Linear(1000, 500)\n",
        "    self.relu2    = nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "    self.linear3 = nn.Linear(500, 250)\n",
        "    self.relu3    = nn.ReLU()\n",
        "    self.dropout3 = nn.Dropout(p=0.1)\n",
        "\n",
        "    self.linear4 = nn.Linear(250, 125)\n",
        "    self.relu4    = nn.ReLU()\n",
        "    self.dropout4 = nn.Dropout(p=0.02)\n",
        "\n",
        "    self.linear5 = nn.Linear(125, 5)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask, tweet_img):\n",
        "    _, text_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False\n",
        "    )\n",
        "    image_output = self.densenet(tweet_img)\n",
        "    merged_output = torch.cat((text_output, image_output), dim=1)\n",
        "    bn_output = self.bn(merged_output)\n",
        "\n",
        "    linear1_output = self.linear1(bn_output)\n",
        "    relu1_output = self.relu1(linear1_output)\n",
        "    dropout1_output = self.dropout1(relu1_output)\n",
        "\n",
        "    linear2_output = self.linear2(dropout1_output)\n",
        "    relu2_output = self.relu2(linear2_output)\n",
        "    dropout2_output = self.dropout2(relu2_output)\n",
        "\n",
        "    linear3_output = self.linear3(dropout2_output)\n",
        "    relu3_output = self.relu3(linear3_output)\n",
        "    dropout3_output = self.dropout3(relu3_output)\n",
        "\n",
        "    linear4_output = self.linear4(dropout3_output)\n",
        "    relu4_output = self.relu4(linear4_output)\n",
        "    dropout4_output = self.dropout4(relu4_output)\n",
        "\n",
        "    linear5_output = self.linear5(dropout4_output)\n",
        "\n",
        "\n",
        "    probas = self.softmax(linear5_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].long()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.max(outputs, dim=1).indices == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].long()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.max(outputs, dim=1).indices == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "MAX_LEN = 150\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 70\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-2)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "986bd7b1-a858-4456-d9b7-b939a50bc12d"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/Models/BertDenseNet/checkpoint.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "# best_accuracy = checkpoint['best_accuracy']\n",
        "\n",
        "# print(start_epoch)\n",
        "# print(best_accuracy)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {start_epoch + epoch + 1}/{start_epoch + EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/Models/BertDenseNet/checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc\n",
        "\n",
        "\n",
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/BertDenseNet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)\n",
        "\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.8672852367162704 accuracy 0.5226901730329742\n",
            "Val   loss 0.8006501793861389 accuracy 0.6492985971943888\n",
            "Epoch 2/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6754341175158819 accuracy 0.7615083251714005\n",
            "Val   loss 0.6905445158481598 accuracy 0.75250501002004\n",
            "Epoch 3/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5589615032076836 accuracy 0.8090107737512243\n",
            "Val   loss 0.6341380476951599 accuracy 0.7765531062124248\n",
            "Epoch 4/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5166925887266794 accuracy 0.8246816846229187\n",
            "Val   loss 0.6752055585384369 accuracy 0.7675350701402806\n",
            "Epoch 5/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.48635520537694293 accuracy 0.8325171400587659\n",
            "Val   loss 0.5804504752159119 accuracy 0.8106212424849699\n",
            "Epoch 6/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4269205455978711 accuracy 0.8545543584720862\n",
            "Val   loss 0.6048908531665802 accuracy 0.8096192384769538\n",
            "Epoch 7/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.40629177043835324 accuracy 0.8583088475350963\n",
            "Val   loss 0.6380390524864197 accuracy 0.7865731462925851\n",
            "Epoch 8/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3835893049836159 accuracy 0.8601044727391446\n",
            "Val   loss 0.6783261597156525 accuracy 0.7985971943887775\n",
            "Epoch 9/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.365551916261514 accuracy 0.8620633365981064\n",
            "Val   loss 0.7098752558231354 accuracy 0.7955911823647294\n",
            "Epoch 10/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3633745089173317 accuracy 0.8684296441397322\n",
            "Val   loss 0.6976792514324188 accuracy 0.7985971943887775\n",
            "Epoch 11/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3191033328572909 accuracy 0.8875285667646099\n",
            "Val   loss 0.664669007062912 accuracy 0.812625250501002\n",
            "Epoch 12/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3140089785059293 accuracy 0.8886712373490042\n",
            "Val   loss 0.7387233674526215 accuracy 0.8106212424849699\n",
            "Epoch 13/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2852904337147872 accuracy 0.8934051583414953\n",
            "Val   loss 0.7160923480987549 accuracy 0.7995991983967935\n",
            "Epoch 14/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.26851017276446026 accuracy 0.9043421482206987\n",
            "Val   loss 0.7823622226715088 accuracy 0.7935871743486973\n",
            "Epoch 15/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.25853365411361057 accuracy 0.8996082272282077\n",
            "Val   loss 0.7583682537078857 accuracy 0.8026052104208417\n",
            "Epoch 16/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.25815602019429207 accuracy 0.9121776036565459\n",
            "Val   loss 0.7465124130249023 accuracy 0.7955911823647294\n",
            "Epoch 17/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24984108159939447 accuracy 0.9097290238328436\n",
            "Val   loss 0.8041724562644958 accuracy 0.8096192384769538\n",
            "Epoch 18/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23522243152062097 accuracy 0.9175644792686908\n",
            "Val   loss 0.832811027765274 accuracy 0.8026052104208417\n",
            "Epoch 19/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.23632078617811203 accuracy 0.9165850473392099\n",
            "Val   loss 1.0061123371124268 accuracy 0.8106212424849699\n",
            "Epoch 20/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.22955837349096933 accuracy 0.9149526607900751\n",
            "Val   loss 0.929828554391861 accuracy 0.8056112224448897\n",
            "Epoch 21/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2589319398005803 accuracy 0.9074436826640548\n",
            "Val   loss 0.9895966649055481 accuracy 0.7945891783567134\n",
            "Epoch 22/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.22790200263261795 accuracy 0.9193601044727391\n",
            "Val   loss 1.0014630556106567 accuracy 0.7985971943887775\n",
            "Epoch 23/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.20226068422198296 accuracy 0.9278485145282402\n",
            "Val   loss 1.0627877116203308 accuracy 0.7775551102204408\n",
            "Epoch 24/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.18999618912736574 accuracy 0.9327456741756448\n",
            "Val   loss 1.2503862977027893 accuracy 0.8036072144288576\n",
            "Epoch 25/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19456618651747704 accuracy 0.9382957884427032\n",
            "Val   loss 1.165956735610962 accuracy 0.7905811623246493\n",
            "Epoch 26/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.18634729521969953 accuracy 0.9276852758733268\n",
            "Val   loss 1.1063226461410522 accuracy 0.8166332665330661\n",
            "Epoch 27/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.16816776804625988 accuracy 0.9409076069213189\n",
            "Val   loss 1.1576799750328064 accuracy 0.7965931863727455\n",
            "Epoch 28/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.152553278952837 accuracy 0.9474371531178583\n",
            "Val   loss 1.238933801651001 accuracy 0.8016032064128256\n",
            "Epoch 29/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1309941733876864 accuracy 0.9521710741103493\n",
            "Val   loss 1.5276848077774048 accuracy 0.8036072144288576\n",
            "Epoch 30/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11743441286186378 accuracy 0.9573947110675808\n",
            "Val   loss 1.9042633175849915 accuracy 0.7965931863727455\n",
            "Epoch 31/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.15004768470923105 accuracy 0.9487430623571661\n",
            "Val   loss 2.0692643523216248 accuracy 0.8046092184368737\n",
            "Epoch 32/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1620668601244688 accuracy 0.94564152791381\n",
            "Val   loss 1.5961512923240662 accuracy 0.7905811623246493\n",
            "Epoch 33/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.16985109883050123 accuracy 0.944662095984329\n",
            "Val   loss 1.3761672973632812 accuracy 0.8016032064128256\n",
            "Epoch 34/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11749980474511783 accuracy 0.9606594841658505\n",
            "Val   loss 1.4142783284187317 accuracy 0.7965931863727455\n",
            "Epoch 35/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.08850631987055142 accuracy 0.970617042115573\n",
            "Val   loss 1.417103111743927 accuracy 0.7895791583166332\n",
            "Epoch 36/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.07229707576334476 accuracy 0.9743715311785831\n",
            "Val   loss 1.5573020577430725 accuracy 0.7935871743486973\n",
            "Epoch 37/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.07349393920352061 accuracy 0.9760039177277179\n",
            "Val   loss 1.7956116795539856 accuracy 0.7915831663326652\n",
            "Epoch 38/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.08025396335870028 accuracy 0.9727391446294482\n",
            "Val   loss 1.5444982051849365 accuracy 0.7885771543086172\n",
            "Epoch 39/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.07869984582066536 accuracy 0.9738818152138426\n",
            "Val   loss 1.813061237335205 accuracy 0.8116232464929859\n",
            "Epoch 40/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0785227349648873 accuracy 0.9738818152138426\n",
            "Val   loss 1.813353955745697 accuracy 0.7965931863727455\n",
            "Epoch 41/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.07367223283896844 accuracy 0.9773098269670257\n",
            "Val   loss 1.64379221200943 accuracy 0.7955911823647294\n",
            "Epoch 42/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.06415854239215453 accuracy 0.9799216454456415\n",
            "Val   loss 1.6276476979255676 accuracy 0.785571142284569\n",
            "Epoch 43/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.05837971499810616 accuracy 0.980411361410382\n",
            "Val   loss 1.754154920578003 accuracy 0.7935871743486973\n",
            "Epoch 44/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.05487192949901024 accuracy 0.9853085210577864\n",
            "Val   loss 1.89372718334198 accuracy 0.7955911823647294\n",
            "Epoch 45/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.04956022851789991 accuracy 0.984818805093046\n",
            "Val   loss 1.9898123145103455 accuracy 0.7865731462925851\n",
            "Epoch 46/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.04596554161980748 accuracy 0.9853085210577864\n",
            "Val   loss 2.014064371585846 accuracy 0.7795591182364728\n",
            "Epoch 47/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0361294544612368 accuracy 0.9872673849167483\n",
            "Val   loss 2.3451168537139893 accuracy 0.7895791583166332\n",
            "Epoch 48/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.04313994183515509 accuracy 0.9864511916421809\n",
            "Val   loss 2.3162784576416016 accuracy 0.7965931863727455\n",
            "Epoch 49/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.03690096829086542 accuracy 0.9874306235716618\n",
            "Val   loss 2.0156842470169067 accuracy 0.7905811623246493\n",
            "Epoch 50/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.045888405914107956 accuracy 0.9856349983676135\n",
            "Val   loss 2.0589759349823 accuracy 0.7865731462925851\n",
            "Epoch 51/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.037250043358653784 accuracy 0.9892262487757101\n",
            "Val   loss 1.8702839016914368 accuracy 0.7915831663326652\n",
            "Epoch 52/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.031701467698439956 accuracy 0.9903689193601044\n",
            "Val   loss 2.2157593965530396 accuracy 0.8016032064128256\n",
            "Epoch 53/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.024497965816408396 accuracy 0.9918380672543258\n",
            "Val   loss 2.4210453033447266 accuracy 0.7925851703406813\n",
            "Epoch 54/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.029025787642846506 accuracy 0.9913483512895853\n",
            "Val   loss 2.4534419775009155 accuracy 0.8076152304609218\n",
            "Epoch 55/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.02510311547666788 accuracy 0.9920013059092393\n",
            "Val   loss 2.4022237062454224 accuracy 0.7955911823647294\n",
            "Epoch 56/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.028203072492033243 accuracy 0.9908586353248449\n",
            "Val   loss 2.3994640111923218 accuracy 0.7945891783567134\n",
            "Epoch 57/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.026225565078978736 accuracy 0.9920013059092393\n",
            "Val   loss 2.3480031490325928 accuracy 0.7975951903807614\n",
            "Epoch 58/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.018601426233847935 accuracy 0.9941234084231145\n",
            "Val   loss 2.3853765726089478 accuracy 0.7935871743486973\n",
            "Epoch 59/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.01654119440354407 accuracy 0.9944498857329416\n",
            "Val   loss 2.577750086784363 accuracy 0.8016032064128256\n",
            "Epoch 60/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.015365026270349821 accuracy 0.9955925563173359\n",
            "Val   loss 2.68966805934906 accuracy 0.8036072144288576\n",
            "Epoch 61/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.01401583336216087 accuracy 0.9941234084231145\n",
            "Val   loss 2.699357748031616 accuracy 0.7965931863727455\n",
            "Epoch 62/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.013227635257256528 accuracy 0.9951028403525954\n",
            "Val   loss 2.768269181251526 accuracy 0.8046092184368737\n",
            "Epoch 63/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.015101493491480747 accuracy 0.9955925563173359\n",
            "Val   loss 2.7202727794647217 accuracy 0.7985971943887775\n",
            "Epoch 64/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.017507767925659817 accuracy 0.9947763630427685\n",
            "Val   loss 2.73159122467041 accuracy 0.8006012024048096\n",
            "Epoch 65/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.019298016947383683 accuracy 0.995266079007509\n",
            "Val   loss 2.6779403686523438 accuracy 0.8036072144288576\n",
            "Epoch 66/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.01591914057886849 accuracy 0.9947763630427685\n",
            "Val   loss 2.5877134799957275 accuracy 0.7965931863727455\n",
            "Epoch 67/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0101121526871187 accuracy 0.9968984655566437\n",
            "Val   loss 2.5956685543060303 accuracy 0.7985971943887775\n",
            "Epoch 68/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.013248983188532293 accuracy 0.995266079007509\n",
            "Val   loss 2.6150827407836914 accuracy 0.8016032064128256\n",
            "Epoch 69/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.01589465808744232 accuracy 0.994939601697682\n",
            "Val   loss 2.608080744743347 accuracy 0.8036072144288576\n",
            "Epoch 70/70\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.011688737858397266 accuracy 0.9965719882468168\n",
            "Val   loss 2.6091039180755615 accuracy 0.8046092184368737\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFmTSaxYMw-"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/Models/BertDenseNet/checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ScOj15BovCww",
        "outputId": "96f2dcd3-2554-4157-8847-17ba65146aaa"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pTghsXN8vEpo",
        "outputId": "25b0cf0b-6251-458c-a9e3-d3ada036d1f7"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].long()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "      preds = torch.max(outputs, dim=1).indices\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['not_humanitarian', 'infrastructure_and_utility_damage', 'other_relevant_information', 'rescue_volunteering_or_donation_effort', 'affected_individuals'], digits = 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                        precision    recall  f1-score   support\n",
            "\n",
            "                      not_humanitarian     0.8623    0.9067    0.8839       504\n",
            "     infrastructure_and_utility_damage     0.8193    0.8395    0.8293        81\n",
            "            other_relevant_information     0.8591    0.8043    0.8308       235\n",
            "rescue_volunteering_or_donation_effort     0.8288    0.7302    0.7764       126\n",
            "                  affected_individuals     0.5455    0.6667    0.6000         9\n",
            "\n",
            "                              accuracy                         0.8503       955\n",
            "                             macro avg     0.7830    0.7895    0.7841       955\n",
            "                          weighted avg     0.8504    0.8503    0.8494       955\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}