{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNeXt-HUMAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc_rI6d-khY2",
        "outputId": "60f5ab40-be69-4279-d724-7950c79b9556"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzejvNI6kk3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93addd14-de6e-4fc7-8046-c45ac5b269fa"
      },
      "source": [
        "!pip3 install torch torchvision pandas transformers scikit-learn tensorflow numpy seaborn matplotlib textwrap3 sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 15.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Collecting textwrap3\n",
            "  Downloading https://files.pythonhosted.org/packages/77/9c/a53e561d496ee5866bbeea4d3a850b3b545ed854f8a21007c1e0d872e94d/textwrap3-0.9.2-py2.py3-none-any.whl\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 53.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (56.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, textwrap3, sentencepiece\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.95 textwrap3-0.9.2 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwWQL0acqLuH",
        "outputId": "e71cad38-79fc-4200-84f3-2964bd03b25c"
      },
      "source": [
        "!ls gdrive/MyDrive/data_image"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'gdrive/MyDrive/data_image': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBErHTyknRL",
        "outputId": "4c6a6570-12b4-40a2-f7e8-bca71d77bc3b"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9plONFGko6I"
      },
      "source": [
        "import transformers\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-z.,!?'0-9]\", ' ', text)\n",
        "    text = re.sub('\\t', ' ',  text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    return text\n",
        "\n",
        "def label_to_target(text):\n",
        "  if text == \"not_humanitarian\":\n",
        "    return 0\n",
        "  elif text == \"infrastructure_and_utility_damage\":\n",
        "    return 1\n",
        "  elif text == \"other_relevant_information\":\n",
        "    return 2\n",
        "  elif text == \"rescue_volunteering_or_donation_effort\":\n",
        "    return 3\n",
        "  elif text == \"affected_individuals\":\n",
        "    return 4\n",
        "\n",
        "df_train = pd.read_csv(\"./gdrive/MyDrive/FYP/task_humanitarian_text_img_agreed_lab_train.tsv\", sep='\\t')\n",
        "df_train = df_train[['image', 'label_text']]\n",
        "df_train = df_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_train['label_text'] = df_train['label_text'].apply(label_to_target)\n",
        "\n",
        "df_val = pd.read_csv(\"./gdrive/MyDrive/FYP/task_humanitarian_text_img_agreed_lab_dev.tsv\", sep='\\t')\n",
        "df_val = df_val[['image', 'label_text']]\n",
        "df_val = df_val.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_val['label_text'] = df_val['label_text'].apply(label_to_target)\n",
        "\n",
        "df_test = pd.read_csv(\"./gdrive/MyDrive/FYP/task_humanitarian_text_img_agreed_lab_test.tsv\", sep='\\t')\n",
        "df_test = df_test[['image', 'label_text']]\n",
        "df_test = df_test.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "df_test['label_text'] = df_test['label_text'].apply(label_to_target)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjM9tz8ksnJ"
      },
      "source": [
        "data_dir = \"./gdrive/MyDrive/FYP/\"\n",
        "class DisasterTweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, paths, targets):\n",
        "    self.paths = paths\n",
        "    self.targets = targets\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.paths)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    path = str(self.paths[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    img = Image.open(data_dir+self.paths[item]).convert('RGB')\n",
        "    img = self.transform(img)  \n",
        "\n",
        "    return {\n",
        "      'tweet_image': img,\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, batch_size):\n",
        "  ds = DisasterTweetDataset(\n",
        "    paths=df.image.to_numpy(),\n",
        "    targets=df.label_text.to_numpy(),\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TweetClassifier, self).__init__()\n",
        "    self.resnet = torchvision.models.resnext50_32x4d(pretrained=True)\n",
        "    # for param in self.resnet.parameters():\n",
        "    #   param.requires_grad = False\n",
        "\n",
        "    self.bn = nn.BatchNorm1d(1000)\n",
        "    self.linear1 = nn.Linear(1000, 256)\n",
        "    self.relu    = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=0.4)\n",
        "    self.linear2 = nn.Linear(256, 5)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "  \n",
        "  def forward(self, tweet_img):\n",
        "    output = self.resnet(tweet_img)\n",
        "    bn_output = self.bn(output)\n",
        "    linear1_output = self.linear1(bn_output)\n",
        "    relu_output = self.relu(linear1_output)\n",
        "    dropout_output = self.dropout(relu_output)\n",
        "    linear2_output = self.linear2(dropout_output)\n",
        "    probas = self.softmax(linear2_output)\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "    targets = d[\"targets\"].long()\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      tweet_img = tweet_imgs\n",
        "    )\n",
        "\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(torch.max(outputs, dim=1).indices == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      targets = d[\"targets\"].long()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        tweet_img = tweet_imgs\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(torch.max(outputs, dim=1).indices == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2k2wgLkyo2"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = TweetClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CK6j-pnDk1d6",
        "outputId": "80c387ef-feaa-4cf3-b56d-8a1bd5eaad4d"
      },
      "source": [
        "history = defaultdict(list)\n",
        "start_epoch = 0\n",
        "best_accuracy = -1\n",
        "\n",
        "# checkpoint = torch.load(\"./gdrive/MyDrive/Models/ResNet/checkpoint.t7\")\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "\n",
        "# print(start_epoch)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "  # scheduler.step(val_acc)\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    state = {\n",
        "            'best_accuracy': val_acc,\n",
        "            'epoch': start_epoch+epoch+1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    savepath= \"./gdrive/MyDrive/FYP/resnext-human-checkpoint.t7\"\n",
        "    torch.save(state,savepath)\n",
        "    best_accuracy = val_acc\n",
        "\n",
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/FYP/resnext-human-checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)\n",
        "\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8894290389497971 accuracy 0.7042115572967679\n",
            "Val   loss 0.6653659937400667 accuracy 0.7695390781563125\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3810233363672585 accuracy 0.8793666340189357\n",
            "Val   loss 0.71717864511505 accuracy 0.783567134268537\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.130015743087841 accuracy 0.9585373816519752\n",
            "Val   loss 0.9265443292402086 accuracy 0.7765531062124248\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.06050980065547431 accuracy 0.9833496571988246\n",
            "Val   loss 1.0347749291667863 accuracy 0.7815631262525049\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.029863281357260154 accuracy 0.9915115899444988\n",
            "Val   loss 1.1812043675472812 accuracy 0.7865731462925851\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.011790585201023895 accuracy 0.9967352269017303\n",
            "Val   loss 1.257784733755721 accuracy 0.7805611222444889\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.009875024170350527 accuracy 0.9973881815213842\n",
            "Val   loss 1.3952235747128725 accuracy 0.7715430861723446\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.007752894165862003 accuracy 0.9972249428664708\n",
            "Val   loss 1.4078245712592015 accuracy 0.7905811623246493\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.00877340655982606 accuracy 0.9970617042115573\n",
            "Val   loss 1.3989419638814906 accuracy 0.7845691382765531\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.004735379656363643 accuracy 0.9991838067254326\n",
            "Val   loss 1.47445289666454 accuracy 0.7965931863727455\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.009183942876992252 accuracy 0.9975514201762977\n",
            "Val   loss 1.5076903382700586 accuracy 0.7865731462925851\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.007445216454421664 accuracy 0.9972249428664708\n",
            "Val   loss 1.576818145929821 accuracy 0.7895791583166332\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.003404378007515063 accuracy 0.9986940907606922\n",
            "Val   loss 1.5893413092172335 accuracy 0.7795591182364728\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.003551975653272047 accuracy 0.9991838067254326\n",
            "Val   loss 1.6247822563829166 accuracy 0.7935871743486973\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.002632777751338507 accuracy 0.999347045380346\n",
            "Val   loss 1.6485783872859818 accuracy 0.7765531062124248\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0013021841249929386 accuracy 0.9995102840352595\n",
            "Val   loss 1.6556607546905677 accuracy 0.7885771543086172\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.003257635821586009 accuracy 0.9991838067254326\n",
            "Val   loss 1.7088883826435204 accuracy 0.7875751503006011\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0004266139747433555 accuracy 1.0\n",
            "Val   loss 1.799800166978486 accuracy 0.7805611222444889\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.002170230346851012 accuracy 0.9991838067254326\n",
            "Val   loss 1.8755515694825187 accuracy 0.782565130260521\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0017122329923672035 accuracy 0.9995102840352595\n",
            "Val   loss 1.7796467786122645 accuracy 0.7795591182364728\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.003376278743476427 accuracy 0.9990205680705191\n",
            "Val   loss 1.9142689521291427 accuracy 0.7795591182364728\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.01016283705558087 accuracy 0.9983676134508651\n",
            "Val   loss 2.0269886967799966 accuracy 0.7755511022044087\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0030330763284923123 accuracy 0.9988573294156056\n",
            "Val   loss 1.984617655948987 accuracy 0.7795591182364728\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.000183219544621919 accuracy 1.0\n",
            "Val   loss 1.8868401484593513 accuracy 0.7875751503006011\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0023971737570483054 accuracy 0.999673522690173\n",
            "Val   loss 2.0181730884674285 accuracy 0.7965931863727455\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.001351963390552875 accuracy 0.9998367613450865\n",
            "Val   loss 2.0184272338769267 accuracy 0.7915831663326652\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 7.351922789995158e-05 accuracy 1.0\n",
            "Val   loss 2.0800973958971483 accuracy 0.782565130260521\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 3.772721931978371e-05 accuracy 1.0\n",
            "Val   loss 2.133041134473705 accuracy 0.7845691382765531\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0001392431695534813 accuracy 1.0\n",
            "Val   loss 2.06333834496105 accuracy 0.7865731462925851\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.001373305849292449 accuracy 0.9995102840352595\n",
            "Val   loss 2.204373640281754 accuracy 0.785571142284569\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.004904845029131913 accuracy 0.999673522690173\n",
            "Val   loss 2.262405142080896 accuracy 0.7815631262525049\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.003065759818286724 accuracy 0.999673522690173\n",
            "Val   loss 2.322066293171947 accuracy 0.7895791583166332\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0015070383832601825 accuracy 0.9998367613450865\n",
            "Val   loss 2.3325888617999024 accuracy 0.782565130260521\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0026876355891157647 accuracy 0.9998367613450865\n",
            "Val   loss 2.298012062907219 accuracy 0.7745490981963927\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.000210727957042929 accuracy 0.9998367613450865\n",
            "Val   loss 2.267698332194298 accuracy 0.7815631262525049\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 3.050257722804835e-05 accuracy 1.0\n",
            "Val   loss 2.3291203985138544 accuracy 0.7935871743486973\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.7637355854363958e-05 accuracy 1.0\n",
            "Val   loss 2.3316979488683125 accuracy 0.7895791583166332\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 8.265372591152495e-06 accuracy 1.0\n",
            "Val   loss 2.332685951202635 accuracy 0.7865731462925851\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 6.588344842080191e-06 accuracy 1.0\n",
            "Val   loss 2.332284392108993 accuracy 0.7875751503006011\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 4.942404701079094e-06 accuracy 1.0\n",
            "Val   loss 2.335513276003656 accuracy 0.7865731462925851\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 4.95376787473032e-06 accuracy 1.0\n",
            "Val   loss 2.3435501061261648 accuracy 0.785571142284569\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 5.017567167414102e-06 accuracy 1.0\n",
            "Val   loss 2.3430893195290414 accuracy 0.7845691382765531\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 4.428116099245865e-06 accuracy 1.0\n",
            "Val   loss 2.345512232846684 accuracy 0.7865731462925851\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 4.221819683559136e-06 accuracy 1.0\n",
            "Val   loss 2.3452114505427226 accuracy 0.7845691382765531\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 3.633461501317709e-06 accuracy 1.0\n",
            "Val   loss 2.3539413589806784 accuracy 0.785571142284569\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 3.3957853068798987e-06 accuracy 1.0\n",
            "Val   loss 2.3552422409965876 accuracy 0.7845691382765531\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 3.387580685446242e-06 accuracy 1.0\n",
            "Val   loss 2.3593926344599043 accuracy 0.7845691382765531\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.8369839495297423e-06 accuracy 1.0\n",
            "Val   loss 2.364676720093167 accuracy 0.7845691382765531\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 3.203656545158171e-06 accuracy 1.0\n",
            "Val   loss 2.3659597261557503 accuracy 0.785571142284569\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 2.2803515270123076e-06 accuracy 1.0\n",
            "Val   loss 2.367423197343236 accuracy 0.785571142284569\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZyVdd3/8ddnNgaGfTN2MBd2ZCcXwpBCM0yN0DLTn0u3ld3+6rabu7tfkeXv161mZll3WG7dphKGormUBqEmCrggiwjqIMM2w86wzXI+vz++1wyH4czMYTkzzFzv58PjOddyrvO5Dmeu97V+L3N3REQkvrIauwAREWlcCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYE0a2b2rJl99XiPe4Q1TDCzojqG/7eZ/Z/j/bki6TJdRyAnGjMrTepsBRwAKqPur7n7ww1f1dEzswnA/7h7z2OcTiFwrbu/cDzqEqmS09gFiNTk7q2rXte18DOzHHevaMjamip9V1IX7RqSJqNqF4uZ/buZbQLuN7MOZva0mZWY2fbodc+k98w3s2uj11eZ2ctmdkc07odmdv5RjtvPzBaY2W4ze8HM7jGz/6mn/u+YWbGZbTSzq5P6P2BmP4led47mYYeZbTOzl8wsy8z+APQGnjKzUjP7bjT+FDNbHo0/38wGJE23MPqulgJ7zOxmM3u8Rk13m9kvjubfQ5oPBYE0NR8DOgJ9gOsJv+H7o+7ewD7gV3W8fyywCugM3Ab83szsKMb9I/A60AmYAXwljbrbAT2Aa4B7zKxDivG+AxQBXYCTgO8B7u5fAT4CPufurd39NjM7DXgEuCka/xlCUOQlTe9y4LNAe+B/gMlm1h7CVgJwGfBQPbVLM6cgkKYmAfzQ3Q+4+z533+ruj7v7XnffDdwKfLKO969193vdvRJ4EOhGWOCmPa6Z9QZGAz9w9zJ3fxmYW0/d5cAt7l7u7s8ApcDptYzXDegTjfuS134gbxrwF3f/m7uXA3cALYEzk8a5293XRd/VRmABMDUaNhnY4u5L6qldmjkFgTQ1Je6+v6rDzFqZ2W/NbK2Z7SIs6NqbWXYt799U9cLd90YvWx/huN2BbUn9ANbVU/fWGvvo99byubcDa4C/mtkHZja9jml2B9Ym1ZiI6uhRR10PAldEr68A/lBP3RIDCgJpamquHX+HsGY91t3bAuOj/rXt7jkeNgIdzaxVUr9ex2PC7r7b3b/j7icDU4Bvm9nEqsE1Rt9A2CUGQLTbqhewPnmSNd7zBDDUzAYDFwJN6gwsyQwFgTR1bQjHBXaYWUfgh5n+QHdfCywGZphZnpl9Avjc8Zi2mV1oZqdEC/WdhNNmE9HgzcDJSaPPAj5rZhPNLJcQigeAf9ZR+35gNtExDnf/6HjULU2bgkCaursI+8W3AAuB5xroc78MfALYCvwEeIywED5WpwIvEI4hvAr82t3nRcP+H/D96Ayhf3P3VYTdO78kzP/nCAeTy+r5jAeBIWi3kER0QZnIcWBmjwHvunvGt0iOVXSw+13gY+6+q7HrkcanLQKRo2Bmo83s49E5/pOBiwj7309oZpYFfBt4VCEgVTIWBGZ2X3TxzLJahlt0McsaM1tqZiMyVYtIBnwMmE/YhXM3cIO7v9moFdXDzAqAXcAkGuBYijQdGds1ZGbjCX8kD7n74BTDLwBuBC4gXLjzC3cfm5FiRESkVhnbInD3BcC2Oka5iBAS7u4LCed+d8tUPSIiklpjNjrXg0MvdimK+m2sOaKZXU9oToCCgoKR/fv3b5ACj4UDB8or2V9eyf7yBAcqErg7DriD4+G5aoPMDnmqltz4gUX/T7hTmXAqEk6iji06A7KzjCyz6ukYRvQfAAl3Eg6V9UxLRBpfj/Yt6ViQV/+IKSxZsmSLu3dJNaxJtD7q7jOBmQCjRo3yxYsXN3JFh9u4cx/zV5WwuHA7KzfuYk1xKWWV4fTvVjlZDOzUivzcbHKzs8jNtug5i5yssEhOOBCFQ9XCOYSGVwdHIhGeW+Rk06FVLu1b5dGxIK/6dYucLEoPVFB6oILd+6se5ew5UEGlQyIRAqQyCpKEO63ysinIy6GgRQ6tW1Q9Z9MiJ5vsLCMn28jJyoqeD4bKIc8A0evwCAFW1SyPu1OZOBg2VTXkZmXRIjeLvOws8nKyaJETnt3hQEWCsooEByoqKatIUFaZoCLhZJtVh1t2VniE788pr0yEgKwMIVlRmaA84ZRXJCivrHqE8areXz1/0bRa5GbRMjc7PPKiR/Tvllx/coAe/C7CF5EVzXuWheC1pO/DODTc05GI/u2q5696HhOH/DsAh4R+Ksn/ZhaNW/X6SB2v+ZP0tc3PpaDF0S22zWxtbcMaMwjWc+jVmD059IrIE1pZRYLFa7fxj1UlzF9VwqrNuwHo3LoFA7u35ZzTOjOwW1sGdGtLv84F5GbrBC0ROTE1ZhDMBb5pZo8SDhbvjBrFOqEdqKjkJ0+vZM6b6yk9UEFutjG6b0e+N7I/E07vyqldWx/V2pWISGPJWBCY2SPABKCzhdv0/RDIBXD3/yY0mXsBoYGtvcDVqad04ti5t5zr/7CY1z7cxhdG9mTSwJM465TOtD7KTTURkRNBxpZg7n55PcMd+EamPv94W7dtL1fd/zrrtu3jF5edwUVn9Kj/TSIZVl5eTlFREfv3769/ZImF/Px8evbsSW5ubtrv0apsGt5at4NrH1xEeaXzh2vGMPbkTo1dkggARUVFtGnThr59+2qXpODubN26laKiIvr165f2+3QEsx7PLdvEZTNfpWVeNn/++pkKATmh7N+/n06dOikEBAhnbnXq1OmItxC1RVCHB175kB89vYJhPdvzu6+OonPrFo1dkshhFAKS7Gh+DwqCWny0dS8znlrBeQNO4peXD6dlXm03vBIRadq0a6gWT7wVLmm45aJBCgGRWuzYsYNf//rXR/XeCy64gB07dhzniuRoKAhScHeeeHM9407uSPf2LRu7HJETVl1BUFFRkbJ/lWeeeYb27dtnoqxj4u4kEon6R2xGFAQpvF20kw+27OGS4T0buxSRE9r06dN5//33OeOMM7j55puZP38+55xzDlOmTGHgwIEAfP7zn2fkyJEMGjSImTNnVr+3b9++bNmyhcLCQgYMGMB1113HoEGD+PSnP82+ffsO+6ynnnqKsWPHMnz4cM477zw2b94MQGlpKVdffTVDhgxh6NChPP744wA899xzjBgxgmHDhjFxYrjt84wZM7jjjjuqpzl48GAKCwspLCzk9NNP58orr2Tw4MGsW7eOG264gVGjRjFo0CB++MODrXYvWrSIM888k2HDhjFmzBh2797N+PHjeeutt6rHOfvss3n77beP4zedWTpGkMITb64nLyeLyUM+1tiliKTtR08tZ8WG43uvmYHd2/LDzw2qdfhPf/pTli1bVr0QnD9/Pm+88QbLli2rPn3xvvvuo2PHjuzbt4/Ro0dz6aWX0qnToWffrV69mkceeYR7772XL37xizz++ONcccUVh4xz9tlns3DhQsyM3/3ud9x222387Gc/48c//jHt2rXjnXfeAWD79u2UlJRw3XXXsWDBAvr168e2bXU1hHywhgcffJBx48YBcOutt9KxY0cqKyuZOHEiS5cupX///kybNo3HHnuM0aNHs2vXLlq2bMk111zDAw88wF133cV7773H/v37GTZsWPpfdCNTENRQXpngqbc3MGnASbTNT/+CDBEJxowZc8g57HfffTdz5swBYN26daxevfqwIOjXrx9nnHEGACNHjqSwsPCw6RYVFTFt2jQ2btxIWVlZ9We88MILPProo9XjdejQgaeeeorx48dXj9OxY8d66+7Tp091CADMmjWLmTNnUlFRwcaNG1mxYgVmRrdu3Rg9ejQAbdu2BWDq1Kn8+Mc/5vbbb+e+++7jqquuqvfzTiQKghpeWl3C1j1lXDxcVw5L01LXmntDKigoqH49f/58XnjhBV599VVatWrFhAkTUp7j3qLFwVOzs7OzU+4auvHGG/n2t7/NlClTmD9/PjNmzDji2nJycg7Z/59cS3LdH374IXfccQeLFi2iQ4cOXHXVVXWem9+qVSsmTZrEk08+yaxZs1iyZMkR19aYdIyghjlvbqBDq1zGn5ay2W4RSdKmTRt2795d6/CdO3fSoUMHWrVqxbvvvsvChQuP+rN27txJjx5hBe3BBx+s7j9p0iTuueee6u7t27czbtw4FixYwIcffghQvWuob9++vPHGGwC88cYb1cNr2rVrFwUFBbRr147Nmzfz7LPPAnD66aezceNGFi1aBMDu3burD4pfe+21fOtb32L06NF06NDhqOezMSgIkuzeX85fl2/iwqHdycvRVyNSn06dOnHWWWcxePBgbr755sOGT548mYqKCgYMGMD06dMP2fVypGbMmMHUqVMZOXIknTt3ru7//e9/n+3btzN48GCGDRvGvHnz6NKlCzNnzuSSSy5h2LBhTJs2DYBLL72Ubdu2MWjQIH71q19x2mmnpfysYcOGMXz4cPr378+XvvQlzjrrLADy8vJ47LHHuPHGGxk2bBiTJk2q3lIYOXIkbdu25eqrT/j2Mw+TsXsWZ0omb0zzp8XruHn2Uv789TMZ0btpJbrE08qVKxkwYEBjlyHAhg0bmDBhAu+++y5ZWY27Ipnqd2FmS9x9VKrxtdqb5Im31tOnUyuG9zrxzm0WkRPXQw89xNixY7n11lsbPQSOhg4WRzbu3Mc/39/Ktz51qtpuEZEjcuWVV3LllVc2dhlHrelFV4bMfWsD7uhsIRGJHQVBZM6b6xneuz19OxfUP7KISDOiIABWbtzFu5t2a2tARGJJQUBoUiIny7hwaPfGLkVEpMHFPggqE86Tb21gwuld6FiQ19jliDR7rVu3BsLpll/4whdSjjNhwgTqO038rrvuYu/evdXdatb66MU+CN5at51Nu/YzRTejF2lQ3bt3Z/bs2Uf9/ppBcKI2a12bE6m569gHwcqN4fL4kX10AZnIkZo+ffohzTtUNfNcWlrKxIkTGTFiBEOGDOHJJ5887L2FhYUMHjwYgH379nHZZZcxYMAALr744kPaGkrVHPTdd9/Nhg0bOPfcczn33HOBg81aA9x5550MHjyYwYMHc9ddd1V/npq7Ti321xGsKS6lIC+b7u3yG7sUkWPz7HTY9M7xnebHhsD5P6118LRp07jpppv4xje+AYQWO59//nny8/OZM2cObdu2ZcuWLYwbN44pU6bUeo3Ob37zG1q1asXKlStZunQpI0aMqB6Wqjnob33rW9x5553MmzfvkOYmAJYsWcL999/Pa6+9hrszduxYPvnJT9KhQwc1d12L2G8RrCku5ZSurXURmchRGD58OMXFxWzYsIG3336bDh060KtXL9yd733vewwdOpTzzjuP9evXV69Zp7JgwYLqBfLQoUMZOnRo9bBZs2YxYsQIhg8fzvLly1mxYkWdNb388stcfPHFFBQU0Lp1ay655BJeeuklIP3mrj/zmc8wZMgQbr/9dpYvXw6E5q6rAg9Cc9cLFy48Ls1d15y/VatWHdbcdU5ODlOnTuXpp5+mvLz8uDZ3HfstgtXFuzn7FLU0Ks1AHWvumTR16lRmz57Npk2bqht3e/jhhykpKWHJkiXk5ubSt2/fOptxrs2RNgddHzV3nVqstwh27S9n864DnNK1dWOXItJkTZs2jUcffZTZs2czdepUIDQZ3bVrV3Jzc5k3bx5r166tcxrjx4/nj3/8IwDLli1j6dKlQO3NQUPtTWCfc845PPHEE+zdu5c9e/YwZ84czjnnnLTnJ47NXcc6CNYUlwJwqoJA5KgNGjSI3bt306NHD7p16wbAl7/8ZRYvXsyQIUN46KGH6N+/f53TuOGGGygtLWXAgAH84Ac/YOTIkUDtzUEDXH/99UyePLn6YHGVESNGcNVVVzFmzBjGjh3Ltddey/Dhw9Oenzg2dx3rZqhnLVrHdx9fyj9unkCfTmpaQpoeNUMdP+k0d61mqI/A6uLdtMjJomeHVo1diohIvTLV3HWsDxavLi7l5C6tyc7SGUMicuLLVHPXsd4iWFNcquMD0uQ1td27kllH83uIbRDsLaugaPs+BYE0afn5+WzdulVhIEAIga1bt5Kff2QXyMZ219D7xXsAOPUkBYE0XT179qSoqIiSkpLGLkVOEPn5+fTs2fOI3hPbIFhdHM4/1jUE0pTl5uZWX9UqcrRiu2toTXEpOVmm00ZFJPYyGgRmNtnMVpnZGjObnmJ4bzObZ2ZvmtlSM7sgk/UkW11cSr/OBeRmxzYLRUSADAaBmWUD9wDnAwOBy81sYI3Rvg/McvfhwGXArzNVT01rikt1fEBEhMxuEYwB1rj7B+5eBjwKXFRjHAfaRq/bARsyWE+1/eWVrN26h1O6NKMgqCiDvfU3gSvHUaISdqwLzyJNWCYPFvcA1iV1FwFja4wzA/irmd0IFADnpZqQmV0PXA/Qu3fvYy6scOseEg6nnNTmmKfV6HauhyX3w5IHYM8W6HMWDPkCDLwIWtXfJG7aEgl4L2rw6/QLIK7Ndpfvhw//ASufglXPwt4tkJMPXU6HrgPD46SBcNJgaPOxxq62Ye3eDMv/DDktoOPJ4dG2B2RlN3ZlUo/GPmvocuABd/+ZmX0C+IOZDXb3Q+7f5u4zgZkQ2ho61g9dvfkEb2xu0zL42/8BDHqMjB4joHXXMNwdCl+G12fCu38BT8Dp54eFz4on4Omb4Jmb4ZSJMGQqnPppKCuFXRth94aDz3u3Qq9xMOBCyG+XupZEJSyfAwtuh5J3Q7/en4ALbg83LanLro2w+q8hNHJaQm7+wefcVmHhmdcEDtbv2wFrXggL/zUvhO+yRdvwvfYeB9sLYfNyeP/v8PYjB9/XdVD4bvtfGL6rxgrPsr3h9/LBfGjRBvqeBT1HQ27LY5+2O6x7DV6/F1Y8CYnyQ4dn50GHviEU2vUK4di2O7TpdvC5RRuo2A/l+w59TlSEkM1pcejvJzs3visiGZLJIFgP9Erq7hn1S3YNMBnA3V81s3ygM1CcwbpYXVxKlkG/zgVhDe+jf0Ln06DdEZx7W3EASlZB8UooXg6bV0DxitC/+xkHF+DdR0DrNO93UFkOL90JC26Dlh2g9Unw0h1hQQ/Qrjf0GA4l70HJyjDOmd+EUf8r/LEBnPu9cJeqd/4Eyx6H955L/VlZOZDXGt54CJ7+33DaZw6GRm4+VFaEabx0B2xdA10GwBfug7I98MIM+O14GH1d+LyWSfeJdYePFoaQWjk3/DHXJisHug8PWzF9z4ZeYyG/7eHjJSph/87wumWHzC8EEgnY+BaseTEs+IsWgVdCQdewtdX/c9DvnLCAqmnP1vA72PBm2GL4x23wj/+C9r3D+/p/Flp1gn3bw2P/joOvaz72bgshVLEvWiDmHxqmea2hfZ9o7bvfwbXw/Hbht7nmhfBY+0+oPADZLcKC+h8JyMoNv8++Z4Xvv/e4Iwvlsr2wbHb4d970DrRoB2OuC7/F3Jaw7YPo8eHB1x8tDPN7rCz78O+i+jk/fH7N58ryFN/xDqgsC7/flh2SHu0hr034zsr3h++/6rmiDLJzUnxuyyPf8klUHjrt5M+ozZk3hpWL4yxjrY+aWQ7wHjCREACLgC+5+/KkcZ4FHnP3B8xsAPAi0MPrKOp4tD769YeX8P76Ep4f/yG8fBeUbgoD2vcJC6Q+Z4U/kPZ9woJtx9rwx715xcGF/tY1YeEA4Y+qatdATh6sfzMsqJMX4L1Gw2nnw2mfTr32vekdeOKG8DxkKpx/W9i1U7YHNi6F9UvCY8Mb0LIjjL4GBl9a91pdIgEfvRoWBK06Hrom1qpzWKAWLQ5/0Msehz0lYU33tMlQ9HpY0z1pCHzy5rAQq2rkau82mPd/YfHvQy2TfgQDPx8tGO6FzcvCPA7/Cgy/Iiywaq7xHdgd5mftK7D+jbCAsizoNixMc9922LctWljuPDhPOflhrbJNt4PzUtA5vLcmyw7hXrWgbJFiV2CiEnYWwfYPYev7YWH1/othawlCUJ1yHpwyCXqOOvI/9tKSsEtt5dPwwbyw4EnFsiC/5gIpeuS2DCsYNRcaB3bB9rVh6y5Zbisoj27q3qV/VP9E6H1mWLh9tDB874WvhMDyyvAb7vOJaNzzwm85OXATifDvuvafsPZl+HBB+HfpOigEwNAvphckZXth98bw2BU9l5WmXoBn5aReGJfvT70FUevz/vDv1qrj4d9tVk6Yj6pgqAqJA7tD0FfXFC3sc1qEUElVz5EeK7KsWkKsBVDLys64G8LW/1Goq/XRjDZDHZ0OeheQDdzn7rea2S3AYnefG51FdC/QmnDg+Lvu/te6pnnMQVC2h5l3/ifTyp+gXeV26HM2jP1aWBisfSU89m0P47bpFn4QZaUH39++D5w06OC+4K6DoNPHw+ZqsgOlsClpAV74CuwpDn9w/c4Juwuq1g5f+lnY9dKyI1z484wkfr0qK6BwAbwzOyy0Op0M478bfnS1rYFvfDvsglr3WpivRHkIjjHXhTDLS7NV17K9IXgKXwnBVb4v9QLREzUWItFurorD7zKVUkGXEArt+4Q//m0fhJBPXjgXdIGPTwwLzpPPTX9rLh0HdofdM5Vlh89bXpuDQXukyvaG0K5a895ZFH6jp0ysfyv3QGn49/tgftgCKo7W09p0C99Dp5Nh3aKw1VwVyO17Q9/xMPzLYTehdtM0CY0WBJlw1EFwYDe8fi/+6q+wvVspbDeGvhfPCGv+yRKJsDZf+ErYJdCyw8EFftf+qdcq05FIwPrFYT/zu0+HP1gsLHj2FB+6FdCUJBKw9LHwXQ2ZGnYxNOSCwT0EByl+x5XlsHNd0m6KaFfF9sKw9p28O6Xq0abb0S+Qm4NdGw7uEvtgXlj4d/x4tAvpbOhzJrTvVf905ISjIAD4+09gwe3s6f0pvrJ6PFdMncolI46sPY7jxj0cW3j3L2EBOuLKxtkKEKlLZUXY/dTUVk4kpbqCoLHPGmo4Y/8FTj+fl7Z354333mBGY54xZBadYljz+jqRE0h2jkIgJuITBAWdoaAza1atBuDjzeliMhGRYxC7naGri0vp0b4lBS3ik4EiInWJXxBsVhtDIiLJYhUElQnn/ZLS5tXGkIjIMYpVEKzfvo8DFQltEYiIJIlVEBy8K1kzaGxOROQ4iVkQhCuEdXtKEZGD4hUEm0vp2qYF7Vrm1j+yiEhMxCoI1pTojCERkZpiEwTuzprNuzlVxwdERA4RmyDYuHM/e8oqdXxARKSG2ASBDhSLiKQWmyBYU3yC355SRKSRxCYIxp3ckf+8YACdWqe4vaCISIzFpuW1Qd3bMah7LTdoFxGJsdhsEYiISGoKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScxkNAjObbGarzGyNmU2vZZwvmtkKM1tuZn/MZD0iInK4jN2q0syygXuASUARsMjM5rr7iqRxTgX+AzjL3bebWddM1SMiIqllcotgDLDG3T9w9zLgUeCiGuNcB9zj7tsB3L04g/WIiEgKmQyCHsC6pO6iqF+y04DTzOwVM1toZpNTTcjMrjezxWa2uKSkJEPliojEU2MfLM4BTgUmAJcD95pZ+5ojuftMdx/l7qO6dOnSwCWKiDRv9QaBmX3OzI4mMNYDvZK6e0b9khUBc9293N0/BN4jBIOIiDSQdBbw04DVZnabmfU/gmkvAk41s35mlgdcBsytMc4ThK0BzKwzYVfRB0fwGSIicozqDQJ3vwIYDrwPPGBmr0b77NvU874K4JvA88BKYJa7LzezW8xsSjTa88BWM1sBzANudvetxzA/IiJyhMzd0xvRrBPwFeAmwoL9FOBud/9l5so73KhRo3zx4sUN+ZEiIk2emS1x91GphqVzjGCKmc0B5gO5wBh3Px8YBnzneBYqIiINL50Lyi4Ffu7uC5J7uvteM7smM2WJiEhDSScIZgAbqzrMrCVwkrsXuvuLmSpMREQaRjpnDf0JSCR1V0b9RESkGUgnCHKiJiIAiF7nZa4kERFpSOkEQUnS6Z6Y2UXAlsyVJCIiDSmdYwT/AjxsZr8CjNB+0JUZrUpERBpMvUHg7u8D48ysddRdmvGqRESkwaR1PwIz+ywwCMg3MwDc/ZYM1iUiIg0knQvK/pvQ3tCNhF1DU4E+Ga5LREQaSDoHi8909yuB7e7+I+AThMbhRESkGUgnCPZHz3vNrDtQDnTLXEkiItKQ0jlG8FR0s5jbgTcAB+7NaFUiItJg6gyC6IY0L7r7DuBxM3sayHf3nQ1SnYiIZFydu4bcPQHck9R9QCEgItK8pHOM4EUzu9SqzhsVEZFmJZ0g+BqhkbkDZrbLzHab2a4M1yUiIg0knSuL67wlpYiING31BoGZjU/Vv+aNakREpGlK5/TRm5Ne5wNjgCXApzJSkYiINKh0dg19LrnbzHoBd2WsIhERaVDpHCyuqQgYcLwLERGRxpHOMYJfEq4mhhAcZxCuMBYRkWYgnWMEi5NeVwCPuPsrGapHREQaWDpBMBvY7+6VAGaWbWat3H1vZksTEZGGkNaVxUDLpO6WwAuZKUdERBpaOkGQn3x7yuh1q8yVJCIiDSmdINhjZiOqOsxsJLAvcyWJiEhDSucYwU3An8xsA+FWlR8j3LpSRESagXQuKFtkZv2B06Neq9y9PLNliYhIQ0nn5vXfAArcfZm7LwNam9nXM1+aiIg0hHSOEVwX3aEMAHffDlyXuZJERKQhpRME2ck3pTGzbCAvcyWJiEhDSudg8XPAY2b226j7a8CzmStJREQaUjpB8O/A9cC/RN1LCWcOiYhIM1DvrqHoBvavAYWEexF8CliZzsTNbLKZrTKzNWY2vY7xLjUzN7NR6ZUtIiLHS61bBGZ2GnB59NgCPAbg7uemM+HoWMI9wCRC09WLzGyuu6+oMV4b4F8JYSMiIg2sri2Cdwlr/xe6+9nu/kug8gimPQZY4+4fuHsZ8ChwUYrxfgz8FyNmcXIAAAlaSURBVLD/CKYtIiLHSV1BcAmwEZhnZvea2UTClcXp6gGsS+ouivpVi5qu6OXuf6lrQmZ2vZktNrPFJSUlR1CCiIjUp9YgcPcn3P0yoD8wj9DURFcz+42ZffpYP9jMsoA7ge/UN667z3T3Ue4+qkuXLsf60SIikiSdg8V73P2P0b2LewJvEs4kqs96oFdSd8+oX5U2wGBgvpkVAuOAuTpgLCLSsI7onsXuvj1aO5+YxuiLgFPNrJ+Z5QGXAXOTprXT3Tu7e1937wssBKa4++LUkxMRkUw4mpvXp8XdK4BvAs8TTjed5e7LzewWM5uSqc8VEZEjk84FZUfN3Z8BnqnR7we1jDshk7WIiEhqGdsiEBGRpkFBICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMZDQIzm2xmq8xsjZlNTzH822a2wsyWmtmLZtYnk/WIiMjhMhYEZpYN3AOcDwwELjezgTVGexMY5e5DgdnAbZmqR0REUsvkFsEYYI27f+DuZcCjwEXJI7j7PHffG3UuBHpmsB4REUkhk0HQA1iX1F0U9avNNcCzqQaY2fVmttjMFpeUlBzHEkVE5IQ4WGxmVwCjgNtTDXf3me4+yt1HdenSpWGLExFp5nIyOO31QK+k7p5Rv0OY2XnAfwKfdPcDGaxHRERSyOQWwSLgVDPrZ2Z5wGXA3OQRzGw48FtgirsXZ7AWERGpRcaCwN0rgG8CzwMrgVnuvtzMbjGzKdFotwOtgT+Z2VtmNreWyYmISIZkctcQ7v4M8EyNfj9Ien1eJj9fRETqd0IcLBYRkcajIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMxlNAjMbLKZrTKzNWY2PcXwFmb2WDT8NTPrm8l6RETkcBkLAjPLBu4BzgcGApeb2cAao10DbHf3U4CfA/+VqXpERCS1TG4RjAHWuPsH7l4GPApcVGOci4AHo9ezgYlmZhmsSUREasjJ4LR7AOuSuouAsbWN4+4VZrYT6ARsSR7JzK4Hro86S81s1VHW1LnmtGMirvMN8Z13zXe8pDPffWobkMkgOG7cfSYw81inY2aL3X3UcSipSYnrfEN8513zHS/HOt+Z3DW0HuiV1N0z6pdyHDPLAdoBWzNYk4iI1JDJIFgEnGpm/cwsD7gMmFtjnLnAV6PXXwD+7u6ewZpERKSGjO0aivb5fxN4HsgG7nP35WZ2C7DY3ecCvwf+YGZrgG2EsMikY9691ETFdb4hvvOu+Y6XY5pv0wq4iEi86cpiEZGYUxCIiMRcbIKgvuYumgszu8/Mis1sWVK/jmb2NzNbHT13aMwaM8HMepnZPDNbYWbLzexfo/7Net7NLN/MXjezt6P5/lHUv1/UbMuaqBmXvMauNRPMLNvM3jSzp6PuZj/fZlZoZu+Y2Vtmtjjqd0y/81gEQZrNXTQXDwCTa/SbDrzo7qcCL0bdzU0F8B13HwiMA74R/Rs393k/AHzK3YcBZwCTzWwcobmWn0fNt2wnNOfSHP0rsDKpOy7zfa67n5F07cAx/c5jEQSk19xFs+DuCwhnYCVLbsrjQeDzDVpUA3D3je7+RvR6N2Hh0INmPu8elEadudHDgU8Rmm2BZjjfAGbWE/gs8Luo24jBfNfimH7ncQmCVM1d9GikWhrDSe6+MXq9CTipMYvJtKgV2+HAa8Rg3qPdI28BxcDfgPeBHe5eEY3SXH/vdwHfBRJRdyfiMd8O/NXMlkTN78Ax/s6bRBMTcvy4u5tZsz1n2MxaA48DN7n7ruQ2DJvrvLt7JXCGmbUH5gD9G7mkjDOzC4Fid19iZhMau54Gdra7rzezrsDfzOzd5IFH8zuPyxZBOs1dNGebzawbQPRc3Mj1ZISZ5RJC4GF3/3PUOxbzDuDuO4B5wCeA9lGzLdA8f+9nAVPMrJCwq/dTwC9o/vONu6+PnosJwT+GY/ydxyUI0mnuojlLbsrjq8CTjVhLRkT7h38PrHT3O5MGNet5N7Mu0ZYAZtYSmEQ4PjKP0GwLNMP5dvf/cPee7t6X8Pf8d3f/Ms18vs2swMzaVL0GPg0s4xh/57G5stjMLiDsU6xq7uLWRi4pI8zsEWACoVnazcAPgSeAWUBvYC3wRXeveUC5STOzs4GXgHc4uM/4e4TjBM123s1sKOHgYDZhxW6Wu99iZicT1pQ7Am8CV7j7gcarNHOiXUP/5u4XNvf5juZvTtSZA/zR3W81s04cw+88NkEgIiKpxWXXkIiI1EJBICIScwoCEZGYUxCIiMScgkBEJOYUBCI1mFll1LJj1eO4NVRnZn2TW4YVORGoiQmRw+1z9zMauwiRhqItApE0Re3A3xa1Bf+6mZ0S9e9rZn83s6Vm9qKZ9Y76n2Rmc6J7BbxtZmdGk8o2s3uj+wf8NboiWKTRKAhEDteyxq6haUnDdrr7EOBXhCvVAX4JPOjuQ4GHgbuj/ncD/4juFTACWB71PxW4x90HATuASzM8PyJ10pXFIjWYWam7t07Rv5BwE5gPogbuNrl7JzPbAnRz9/Ko/0Z372xmJUDP5CYOoiay/xbdQAQz+3cg191/kvk5E0lNWwQiR8ZreX0kktu+qUTH6qSRKQhEjsy0pOdXo9f/JLSACfBlQuN3EG4ZeANU3zymXUMVKXIktCYicriW0R2/qjzn7lWnkHYws6WEtfrLo343Aveb2c1ACXB11P9fgZlmdg1hzf8GYCMiJxgdIxBJU3SMYJS7b2nsWkSOJ+0aEhGJOW0RiIjEnLYIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5v4/802bKEWBUP4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Taz58ppcm8n8"
      },
      "source": [
        "state = {\n",
        "        'epoch': start_epoch + EPOCHS,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "savepath= \"./gdrive/MyDrive/FYP/resnext-human-checkpoint-{}.t7\".format(start_epoch + EPOCHS)\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ScOj15BovCww",
        "outputId": "865f6d26-f053-40e5-83ff-35edee70e9a8"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T314gl9wz453",
        "outputId": "a42f0e16-9724-4721-aacf-26033e399d5c"
      },
      "source": [
        "checkpoint = torch.load(\"./gdrive/MyDrive/FYP/resnext-human-checkpoint.t7\")\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "start_epoch = checkpoint['epoch']\n",
        "\n",
        "print(start_epoch)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTghsXN8vEpo",
        "outputId": "a11d4a72-64d8-47b4-d942-973ca85f7f3a"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      tweet_imgs = d[\"tweet_image\"].to(device)\n",
        "      targets = d[\"targets\"].long()\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(tweet_img=tweet_imgs)\n",
        "      preds = torch.max(outputs, dim=1).indices\n",
        "\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['not_humanitarian', 'infrastructure_and_utility_damage', 'other_relevant_information', 'rescue_volunteering_or_donation_effort', 'affected_individuals'], digits = 4))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                        precision    recall  f1-score   support\n",
            "\n",
            "                      not_humanitarian     0.8173    0.8790    0.8470       504\n",
            "     infrastructure_and_utility_damage     0.7701    0.8272    0.7976        81\n",
            "            other_relevant_information     0.8800    0.7489    0.8092       235\n",
            "rescue_volunteering_or_donation_effort     0.6230    0.6032    0.6129       126\n",
            "                  affected_individuals     0.5000    0.2222    0.3077         9\n",
            "\n",
            "                              accuracy                         0.8000       955\n",
            "                             macro avg     0.7181    0.6561    0.6749       955\n",
            "                          weighted avg     0.8001    0.8000    0.7976       955\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}